{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Judy-Choi/Tensorflow_Certificate/blob/main/12_sarcasm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal Metric\n",
        "- val_loss : 0.3626\n",
        "- val_acc : 0.8372"
      ],
      "metadata": {
        "id": "wfAcuHvD6Qim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Goal Metric (val_loss : 0.3626)\n",
        "- val_loss : 0.3626\n",
        "- val_acc : 0.8372"
      ],
      "metadata": {
        "id": "Obn4mUaXARLG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bd8pKKWnV5N"
      },
      "source": [
        "## sarcasm 은 Huffpost의 헤드라인 중에서 비꼬는 타이틀과 그렇지 않은 타이틀을 구분하는데 있다.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKMbpxX8Ro-h"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Bidirectional, LSTM, Dense, Input, Dropout, BatchNormalization\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNQz1hnBnyF6"
      },
      "source": [
        "## 1. 데이터 준비하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FLOv_TrRo-m",
        "outputId": "93cd779c-f060-4335-b9ce-18072c9bc92a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('sarcasm.json', <http.client.HTTPMessage at 0x7f0857c6ac40>)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "urllib.request.urlretrieve(url, 'sarcasm.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXNFkRYERo-u"
      },
      "source": [
        "### json 파일 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBg4ZmnKRo-v"
      },
      "outputs": [],
      "source": [
        "with open('sarcasm.json') as f:\n",
        "    data = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM1pXNPTn_Y1"
      },
      "source": [
        "### json 으로 로드한 데이터는 array of dictionaries "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ylqxc8fcRo-0",
        "outputId": "75bb3af7-39e1-46ca-d26d-9f687677f834"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'article_link': 'https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5',\n",
              "  'headline': \"former versace store clerk sues over secret 'black code' for minority shoppers\",\n",
              "  'is_sarcastic': 0},\n",
              " {'article_link': 'https://www.huffingtonpost.com/entry/roseanne-revival-review_us_5ab3a497e4b054d118e04365',\n",
              "  'headline': \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\",\n",
              "  'is_sarcastic': 0},\n",
              " {'article_link': 'https://local.theonion.com/mom-starting-to-fear-son-s-web-series-closest-thing-she-1819576697',\n",
              "  'headline': \"mom starting to fear son's web series closest thing she will have to grandchild\",\n",
              "  'is_sarcastic': 1},\n",
              " {'article_link': 'https://politics.theonion.com/boehner-just-wants-wife-to-listen-not-come-up-with-alt-1819574302',\n",
              "  'headline': 'boehner just wants wife to listen, not come up with alternative debt-reduction ideas',\n",
              "  'is_sarcastic': 1},\n",
              " {'article_link': 'https://www.huffingtonpost.com/entry/jk-rowling-wishes-snape-happy-birthday_us_569117c4e4b0cad15e64fdcb',\n",
              "  'headline': 'j.k. rowling wishes snape happy birthday in the most magical way',\n",
              "  'is_sarcastic': 0}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2JA-GrVRo-4"
      },
      "source": [
        "### 각 헤드라인의 문장(sentences)과 sarcasm 인지 아닌지를 나타내는 답(labels)으로 나누기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2VgV9N8BRo-5",
        "outputId": "6a55c449-6812-40ec-ed6d-1db717adfb55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'boehner just wants wife to listen, not come up with alternative debt-reduction ideas'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data[3]['headline']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbjDiTABRo--",
        "outputId": "342da410-d19c-4d1a-8877-144aca479bf6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data[3]['is_sarcastic']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vn7jar8gWsn",
        "outputId": "58e7e7cb-2422-49d4-ef51-50eede1da7af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26709"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# 전체 sentense의 수\n",
        "len(data)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EqCaF9cpGKL"
      },
      "source": [
        "### sentenses 와 labels 로 나누려면, list 를 사용해서, 하나씩 list.append() 한다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evOur3s7Ro_G"
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "labels = []\n",
        "for d in data:\n",
        "    sentences.append(d['headline'])\n",
        "    labels.append(d['is_sarcastic'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxkCHeN-Ro_J",
        "outputId": "39fd5f45-c39b-422e-fe8f-f54cca7ac602"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"former versace store clerk sues over secret 'black code' for minority shoppers\",\n",
              " \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\",\n",
              " \"mom starting to fear son's web series closest thing she will have to grandchild\",\n",
              " 'boehner just wants wife to listen, not come up with alternative debt-reduction ideas',\n",
              " 'j.k. rowling wishes snape happy birthday in the most magical way']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "sentences[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmDN_DHqjO9v",
        "outputId": "ed6808b2-d161-4dab-cc72-16871bf6f37e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26709"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AFuC-5eRo_N",
        "outputId": "5f0b789e-81d0-4264-9f00-c5152d959c7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 1, 1, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "labels[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV-gws0ZIDT5"
      },
      "source": [
        "### train / test 세트 나누기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP_9RTOFIDT7"
      },
      "outputs": [],
      "source": [
        "# 주어지는 값. 전체 데이터셋을 앞에서 20000개는 training으로 나머지는 test로 사용하기 위해서 나눈다\n",
        "training_size = 20000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbKerNkAIDUC"
      },
      "outputs": [],
      "source": [
        "train_sentences = sentences[:training_size]\n",
        "train_labels = labels[:training_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUv12RFJIDUJ"
      },
      "outputs": [],
      "source": [
        "test_sentences = sentences[training_size:]\n",
        "test_labels = labels[training_size:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEXyUrNHRo_S"
      },
      "source": [
        "### 토큰화의 첫단계: Tokenizer 정의\n",
        "\n",
        "* `num_words`: 단어 max 사이즈를 지정합니다. 가장 빈도수가 높은 단어부터 저장합니다.\n",
        "* `oov_token`: 단어 토큰에 없는 단어를 어떻게 표기할 것인지 지정해줍니다.\n",
        "* `maxlen`: 최대 문장 길이를 정의합니다. 최대 문장길이보다 길면, 잘라냅니다.\n",
        "* `truncating`: 문장의 길이가 `maxlen`보다 길 때 앞을 자를지 뒤를 자를지 정의합니다.\n",
        "* `padding`: 문장의 길이가 `maxlen`보다 짧을 때 채워줄 값을 앞을 채울지, 뒤를 채울지 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ALdg-x5Ro_T"
      },
      "outputs": [],
      "source": [
        "# vocab_size 는 전체 unique한 단어의 수에 맞추는 것이 좋다\n",
        "vocab_size = 1000  # 25637\n",
        "# oov = out of vacabulary\n",
        "oov_tok = \"<oov>\"  \n",
        "\n",
        "# max_length 헤드라인에 들어가는 최대 단어수를 정한다.\n",
        "max_length = 120\n",
        "\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "\n",
        "embedding_dim = 16   # word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bgjblALRo_c"
      },
      "source": [
        "단어의 토큰화를 진행합니다.\n",
        "\n",
        "* `num_words`: 단어 max 사이즈를 지정합니다. 가장 빈도수가 높은 단어부터 저장합니다.\n",
        "* `oov_token`: 단어 토큰에 없는 단어를 어떻게 표기할 것인지 지정해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORwBwbVIRo_d"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyW8MlZ-NQkf"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c6Nqj8YRo_g"
      },
      "source": [
        "## Tokenizer로 학습시킬 문장에 대한 토큰화 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWZFZUFNRo_h"
      },
      "outputs": [],
      "source": [
        "tokenizer.fit_on_texts(train_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2ZPQndyRo_k",
        "outputId": "7a1e7e25-51a6-4322-8587-2c78b5f7efd4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<oov>': 1,\n",
              " 'to': 2,\n",
              " 'of': 3,\n",
              " 'the': 4,\n",
              " 'in': 5,\n",
              " 'for': 6,\n",
              " 'a': 7,\n",
              " 'on': 8,\n",
              " 'and': 9,\n",
              " 'with': 10,\n",
              " 'is': 11,\n",
              " 'new': 12,\n",
              " 'trump': 13,\n",
              " 'man': 14,\n",
              " 'from': 15,\n",
              " 'at': 16,\n",
              " 'about': 17,\n",
              " 'you': 18,\n",
              " 'by': 19,\n",
              " 'this': 20,\n",
              " 'after': 21,\n",
              " 'be': 22,\n",
              " 'up': 23,\n",
              " 'out': 24,\n",
              " 'that': 25,\n",
              " 'how': 26,\n",
              " 'as': 27,\n",
              " 'it': 28,\n",
              " 'not': 29,\n",
              " 'are': 30,\n",
              " 'your': 31,\n",
              " 'what': 32,\n",
              " 'his': 33,\n",
              " 'all': 34,\n",
              " 'he': 35,\n",
              " 'will': 36,\n",
              " 'who': 37,\n",
              " 'just': 38,\n",
              " 'has': 39,\n",
              " 'more': 40,\n",
              " 'one': 41,\n",
              " 'year': 42,\n",
              " 'into': 43,\n",
              " 'report': 44,\n",
              " 'have': 45,\n",
              " 'why': 46,\n",
              " 'over': 47,\n",
              " 'area': 48,\n",
              " 'u': 49,\n",
              " 'donald': 50,\n",
              " 'says': 51,\n",
              " 'day': 52,\n",
              " 'can': 53,\n",
              " 's': 54,\n",
              " 'first': 55,\n",
              " 'woman': 56,\n",
              " 'time': 57,\n",
              " 'like': 58,\n",
              " 'get': 59,\n",
              " 'old': 60,\n",
              " 'no': 61,\n",
              " 'her': 62,\n",
              " \"trump's\": 63,\n",
              " 'off': 64,\n",
              " 'now': 65,\n",
              " 'an': 66,\n",
              " 'obama': 67,\n",
              " 'life': 68,\n",
              " 'people': 69,\n",
              " 'women': 70,\n",
              " 'house': 71,\n",
              " \"'\": 72,\n",
              " 'was': 73,\n",
              " 'still': 74,\n",
              " 'white': 75,\n",
              " 'back': 76,\n",
              " 'make': 77,\n",
              " 'than': 78,\n",
              " 'clinton': 79,\n",
              " 'down': 80,\n",
              " 'when': 81,\n",
              " 'my': 82,\n",
              " '5': 83,\n",
              " 'could': 84,\n",
              " 'world': 85,\n",
              " 'americans': 86,\n",
              " 'if': 87,\n",
              " 'i': 88,\n",
              " 'we': 89,\n",
              " 'way': 90,\n",
              " 'their': 91,\n",
              " 'most': 92,\n",
              " 'study': 93,\n",
              " 'they': 94,\n",
              " 'before': 95,\n",
              " 'family': 96,\n",
              " 'do': 97,\n",
              " 'gop': 98,\n",
              " 'best': 99,\n",
              " 'black': 100,\n",
              " \"it's\": 101,\n",
              " 'bill': 102,\n",
              " 'school': 103,\n",
              " 'but': 104,\n",
              " 'only': 105,\n",
              " 'police': 106,\n",
              " 'him': 107,\n",
              " 'so': 108,\n",
              " 'years': 109,\n",
              " 'president': 110,\n",
              " 'know': 111,\n",
              " '3': 112,\n",
              " 'being': 113,\n",
              " 'watch': 114,\n",
              " 'should': 115,\n",
              " 'would': 116,\n",
              " 'last': 117,\n",
              " 'really': 118,\n",
              " '10': 119,\n",
              " 'video': 120,\n",
              " 'show': 121,\n",
              " \"can't\": 122,\n",
              " 'going': 123,\n",
              " 'death': 124,\n",
              " 'hillary': 125,\n",
              " 'during': 126,\n",
              " 'finds': 127,\n",
              " 'american': 128,\n",
              " 'things': 129,\n",
              " 'good': 130,\n",
              " 'state': 131,\n",
              " 'against': 132,\n",
              " 'she': 133,\n",
              " 'home': 134,\n",
              " 'or': 135,\n",
              " 'may': 136,\n",
              " 'love': 137,\n",
              " 'health': 138,\n",
              " 'say': 139,\n",
              " 'need': 140,\n",
              " 'nation': 141,\n",
              " 'too': 142,\n",
              " '000': 143,\n",
              " 'every': 144,\n",
              " 'right': 145,\n",
              " 'take': 146,\n",
              " 'gets': 147,\n",
              " 'campaign': 148,\n",
              " \"'the\": 149,\n",
              " 'work': 150,\n",
              " 'little': 151,\n",
              " '2': 152,\n",
              " 'mom': 153,\n",
              " 'court': 154,\n",
              " 'some': 155,\n",
              " 'getting': 156,\n",
              " 'party': 157,\n",
              " 'high': 158,\n",
              " 'john': 159,\n",
              " \"here's\": 160,\n",
              " 'our': 161,\n",
              " '7': 162,\n",
              " 'kids': 163,\n",
              " 'calls': 164,\n",
              " 'where': 165,\n",
              " 'takes': 166,\n",
              " 'makes': 167,\n",
              " 'child': 168,\n",
              " 'parents': 169,\n",
              " 'dead': 170,\n",
              " 'big': 171,\n",
              " 'these': 172,\n",
              " 'local': 173,\n",
              " 'while': 174,\n",
              " 'own': 175,\n",
              " 'never': 176,\n",
              " 'election': 177,\n",
              " 'real': 178,\n",
              " 'go': 179,\n",
              " \"doesn't\": 180,\n",
              " 'other': 181,\n",
              " \"he's\": 182,\n",
              " 'news': 183,\n",
              " 'america': 184,\n",
              " 'see': 185,\n",
              " 'self': 186,\n",
              " 'through': 187,\n",
              " '4': 188,\n",
              " 'plan': 189,\n",
              " 'stop': 190,\n",
              " 'want': 191,\n",
              " 'college': 192,\n",
              " 'guy': 193,\n",
              " 'its': 194,\n",
              " 'change': 195,\n",
              " '6': 196,\n",
              " 'look': 197,\n",
              " 'sex': 198,\n",
              " 'gay': 199,\n",
              " 'office': 200,\n",
              " 'again': 201,\n",
              " \"don't\": 202,\n",
              " 'two': 203,\n",
              " 'bush': 204,\n",
              " 'next': 205,\n",
              " \"nation's\": 206,\n",
              " 'around': 207,\n",
              " 'war': 208,\n",
              " 'even': 209,\n",
              " '1': 210,\n",
              " 'got': 211,\n",
              " 'another': 212,\n",
              " 'wants': 213,\n",
              " 'baby': 214,\n",
              " 'million': 215,\n",
              " 'long': 216,\n",
              " \"man's\": 217,\n",
              " 'dog': 218,\n",
              " 'them': 219,\n",
              " 'week': 220,\n",
              " 'thing': 221,\n",
              " 'dad': 222,\n",
              " 'congress': 223,\n",
              " 'made': 224,\n",
              " 'gun': 225,\n",
              " 'north': 226,\n",
              " 'much': 227,\n",
              " 'care': 228,\n",
              " 'under': 229,\n",
              " 'job': 230,\n",
              " 'ever': 231,\n",
              " 'national': 232,\n",
              " 'finally': 233,\n",
              " 'debate': 234,\n",
              " 'actually': 235,\n",
              " 'help': 236,\n",
              " 'ways': 237,\n",
              " '20': 238,\n",
              " 'us': 239,\n",
              " 'been': 240,\n",
              " 'students': 241,\n",
              " '9': 242,\n",
              " 'shows': 243,\n",
              " 'sexual': 244,\n",
              " 'give': 245,\n",
              " \"won't\": 246,\n",
              " 'season': 247,\n",
              " 'live': 248,\n",
              " 'game': 249,\n",
              " 'making': 250,\n",
              " 'better': 251,\n",
              " 'climate': 252,\n",
              " 'senate': 253,\n",
              " 'shooting': 254,\n",
              " 'without': 255,\n",
              " 'anti': 256,\n",
              " 'history': 257,\n",
              " 'media': 258,\n",
              " 'night': 259,\n",
              " 'had': 260,\n",
              " 'star': 261,\n",
              " 'god': 262,\n",
              " 'couple': 263,\n",
              " 'money': 264,\n",
              " 'entire': 265,\n",
              " 'everyone': 266,\n",
              " 'there': 267,\n",
              " 'city': 268,\n",
              " 'free': 269,\n",
              " 'trying': 270,\n",
              " 'any': 271,\n",
              " 'end': 272,\n",
              " 'me': 273,\n",
              " 'children': 274,\n",
              " 'announces': 275,\n",
              " 'tv': 276,\n",
              " 'law': 277,\n",
              " 'top': 278,\n",
              " 'found': 279,\n",
              " 'away': 280,\n",
              " 'paul': 281,\n",
              " 'supreme': 282,\n",
              " 'face': 283,\n",
              " '8': 284,\n",
              " 'men': 285,\n",
              " 'government': 286,\n",
              " 'teen': 287,\n",
              " 'come': 288,\n",
              " 'story': 289,\n",
              " 'tell': 290,\n",
              " 'business': 291,\n",
              " 'food': 292,\n",
              " 'part': 293,\n",
              " 'film': 294,\n",
              " 'bad': 295,\n",
              " 'reveals': 296,\n",
              " 'think': 297,\n",
              " 'deal': 298,\n",
              " 'introduces': 299,\n",
              " 'morning': 300,\n",
              " 'attack': 301,\n",
              " 'find': 302,\n",
              " 'james': 303,\n",
              " 'son': 304,\n",
              " 'car': 305,\n",
              " '11': 306,\n",
              " 'enough': 307,\n",
              " 'fire': 308,\n",
              " 'facebook': 309,\n",
              " 'does': 310,\n",
              " 'power': 311,\n",
              " 'pope': 312,\n",
              " 'movie': 313,\n",
              " 'friend': 314,\n",
              " 'fight': 315,\n",
              " 'york': 316,\n",
              " 'wedding': 317,\n",
              " 'used': 318,\n",
              " 'body': 319,\n",
              " 'book': 320,\n",
              " 'call': 321,\n",
              " 'middle': 322,\n",
              " \"didn't\": 323,\n",
              " 'girl': 324,\n",
              " 'run': 325,\n",
              " 'same': 326,\n",
              " 'use': 327,\n",
              " 'former': 328,\n",
              " 'tax': 329,\n",
              " 'single': 330,\n",
              " 'thinks': 331,\n",
              " 'friends': 332,\n",
              " 'public': 333,\n",
              " 'second': 334,\n",
              " 'email': 335,\n",
              " 'support': 336,\n",
              " 'line': 337,\n",
              " 'photos': 338,\n",
              " 'republican': 339,\n",
              " 'great': 340,\n",
              " 'asks': 341,\n",
              " 'rights': 342,\n",
              " 'company': 343,\n",
              " 'behind': 344,\n",
              " 'sanders': 345,\n",
              " 'marriage': 346,\n",
              " 'speech': 347,\n",
              " 'looking': 348,\n",
              " 'must': 349,\n",
              " 'claims': 350,\n",
              " 'room': 351,\n",
              " 'security': 352,\n",
              " 'already': 353,\n",
              " 'because': 354,\n",
              " 'each': 355,\n",
              " '2016': 356,\n",
              " 'keep': 357,\n",
              " 'judge': 358,\n",
              " 'team': 359,\n",
              " 'republicans': 360,\n",
              " 'christmas': 361,\n",
              " 'goes': 362,\n",
              " 'vote': 363,\n",
              " 'democrats': 364,\n",
              " 'talk': 365,\n",
              " \"world's\": 366,\n",
              " 'presidential': 367,\n",
              " 'violence': 368,\n",
              " 'case': 369,\n",
              " 'inside': 370,\n",
              " 'voters': 371,\n",
              " 'control': 372,\n",
              " 'open': 373,\n",
              " 'ban': 374,\n",
              " 'future': 375,\n",
              " 'name': 376,\n",
              " 'human': 377,\n",
              " 'department': 378,\n",
              " 'between': 379,\n",
              " 'might': 380,\n",
              " 'ad': 381,\n",
              " 'wife': 382,\n",
              " 'coming': 383,\n",
              " 'perfect': 384,\n",
              " 'win': 385,\n",
              " 'having': 386,\n",
              " 'twitter': 387,\n",
              " 'killed': 388,\n",
              " 'secret': 389,\n",
              " 'political': 390,\n",
              " 'fans': 391,\n",
              " 'full': 392,\n",
              " 'sure': 393,\n",
              " 'group': 394,\n",
              " 'very': 395,\n",
              " 'pretty': 396,\n",
              " 'student': 397,\n",
              " 'texas': 398,\n",
              " 'three': 399,\n",
              " 'record': 400,\n",
              " 'once': 401,\n",
              " 'music': 402,\n",
              " '12': 403,\n",
              " 'bernie': 404,\n",
              " '15': 405,\n",
              " 'missing': 406,\n",
              " 'releases': 407,\n",
              " 'poll': 408,\n",
              " 'doing': 409,\n",
              " 'teacher': 410,\n",
              " 'ready': 411,\n",
              " 'living': 412,\n",
              " 'super': 413,\n",
              " 'boy': 414,\n",
              " 'always': 415,\n",
              " 'scientists': 416,\n",
              " 'art': 417,\n",
              " 'here': 418,\n",
              " 'reports': 419,\n",
              " 'unveils': 420,\n",
              " 'save': 421,\n",
              " 'kim': 422,\n",
              " 'until': 423,\n",
              " 'meet': 424,\n",
              " 'photo': 425,\n",
              " 'race': 426,\n",
              " 'something': 427,\n",
              " 'working': 428,\n",
              " 'comes': 429,\n",
              " 'were': 430,\n",
              " 'post': 431,\n",
              " 'obamacare': 432,\n",
              " 'start': 433,\n",
              " 'word': 434,\n",
              " 'running': 435,\n",
              " 'summer': 436,\n",
              " 'ryan': 437,\n",
              " 'lives': 438,\n",
              " 'fucking': 439,\n",
              " 'shot': 440,\n",
              " 'month': 441,\n",
              " 'mike': 442,\n",
              " 'thousands': 443,\n",
              " 'gives': 444,\n",
              " 'days': 445,\n",
              " 'wall': 446,\n",
              " 'leaves': 447,\n",
              " 'drug': 448,\n",
              " 'water': 449,\n",
              " 'california': 450,\n",
              " 'class': 451,\n",
              " 'female': 452,\n",
              " 'age': 453,\n",
              " 'looks': 454,\n",
              " 'social': 455,\n",
              " 'many': 456,\n",
              " 'let': 457,\n",
              " 'country': 458,\n",
              " 'person': 459,\n",
              " 'breaking': 460,\n",
              " 'everything': 461,\n",
              " 'head': 462,\n",
              " 'father': 463,\n",
              " 'plans': 464,\n",
              " 'minutes': 465,\n",
              " 'russia': 466,\n",
              " 'states': 467,\n",
              " 'town': 468,\n",
              " 'candidate': 469,\n",
              " 'forced': 470,\n",
              " 'put': 471,\n",
              " 'cancer': 472,\n",
              " 'yet': 473,\n",
              " 'education': 474,\n",
              " 'left': 475,\n",
              " 'probably': 476,\n",
              " 'korea': 477,\n",
              " 'cruz': 478,\n",
              " \"'i\": 479,\n",
              " 'air': 480,\n",
              " 'tells': 481,\n",
              " 'place': 482,\n",
              " 'past': 483,\n",
              " 'admits': 484,\n",
              " 'employee': 485,\n",
              " 'taking': 486,\n",
              " 'hot': 487,\n",
              " 'believe': 488,\n",
              " 'biden': 489,\n",
              " 'did': 490,\n",
              " 'hard': 491,\n",
              " 'red': 492,\n",
              " 'dream': 493,\n",
              " \"you're\": 494,\n",
              " 'mother': 495,\n",
              " 'half': 496,\n",
              " 'street': 497,\n",
              " 'eating': 498,\n",
              " 'pay': 499,\n",
              " \"i'm\": 500,\n",
              " 'justice': 501,\n",
              " 'secretary': 502,\n",
              " 'beautiful': 503,\n",
              " 'guide': 504,\n",
              " 'romney': 505,\n",
              " 'administration': 506,\n",
              " 'south': 507,\n",
              " 'heart': 508,\n",
              " 'list': 509,\n",
              " 'military': 510,\n",
              " '50': 511,\n",
              " 'isis': 512,\n",
              " 'internet': 513,\n",
              " 'tips': 514,\n",
              " 'personal': 515,\n",
              " 'small': 516,\n",
              " 'young': 517,\n",
              " 'talks': 518,\n",
              " '30': 519,\n",
              " 'needs': 520,\n",
              " 'together': 521,\n",
              " 'fbi': 522,\n",
              " 'rock': 523,\n",
              " 'nuclear': 524,\n",
              " 'lost': 525,\n",
              " 'times': 526,\n",
              " 'wins': 527,\n",
              " 'questions': 528,\n",
              " 'michael': 529,\n",
              " 'fan': 530,\n",
              " 'lot': 531,\n",
              " 'those': 532,\n",
              " 'letter': 533,\n",
              " 'hollywood': 534,\n",
              " 'set': 535,\n",
              " 'crisis': 536,\n",
              " 'officials': 537,\n",
              " 'king': 538,\n",
              " 'owner': 539,\n",
              " 'latest': 540,\n",
              " 'idea': 541,\n",
              " 'ex': 542,\n",
              " 'thought': 543,\n",
              " 'dies': 544,\n",
              " 'march': 545,\n",
              " 'kill': 546,\n",
              " 'excited': 547,\n",
              " 'restaurant': 548,\n",
              " 'iran': 549,\n",
              " 'director': 550,\n",
              " 'warns': 551,\n",
              " 'chris': 552,\n",
              " 'birthday': 553,\n",
              " 'congressman': 554,\n",
              " \"what's\": 555,\n",
              " 'issues': 556,\n",
              " 'ask': 557,\n",
              " 'service': 558,\n",
              " 'system': 559,\n",
              " 'birth': 560,\n",
              " 'federal': 561,\n",
              " 'sleep': 562,\n",
              " 'ceo': 563,\n",
              " 'holiday': 564,\n",
              " 'following': 565,\n",
              " 'daughter': 566,\n",
              " 'celebrates': 567,\n",
              " 'move': 568,\n",
              " 'assault': 569,\n",
              " 'wrong': 570,\n",
              " 'china': 571,\n",
              " 'rise': 572,\n",
              " 'ice': 573,\n",
              " 'someone': 574,\n",
              " 'majority': 575,\n",
              " 'cat': 576,\n",
              " 'muslim': 577,\n",
              " 'giving': 578,\n",
              " 'phone': 579,\n",
              " 'fun': 580,\n",
              " \"women's\": 581,\n",
              " 'problem': 582,\n",
              " 'abortion': 583,\n",
              " 'special': 584,\n",
              " 'mark': 585,\n",
              " 'wearing': 586,\n",
              " 'buy': 587,\n",
              " 'trip': 588,\n",
              " 'third': 589,\n",
              " 'knows': 590,\n",
              " \"isn't\": 591,\n",
              " 'tweets': 592,\n",
              " 'less': 593,\n",
              " 'nothing': 594,\n",
              " 'series': 595,\n",
              " 'earth': 596,\n",
              " 'chief': 597,\n",
              " 'percent': 598,\n",
              " 'favorite': 599,\n",
              " 'washington': 600,\n",
              " 'few': 601,\n",
              " 'cover': 602,\n",
              " 'talking': 603,\n",
              " 'meeting': 604,\n",
              " 'girls': 605,\n",
              " 'online': 606,\n",
              " 'syrian': 607,\n",
              " 'ted': 608,\n",
              " 'bar': 609,\n",
              " 'box': 610,\n",
              " 'hour': 611,\n",
              " 'play': 612,\n",
              " 'hours': 613,\n",
              " 'fashion': 614,\n",
              " 'late': 615,\n",
              " 'outside': 616,\n",
              " 'al': 617,\n",
              " 'community': 618,\n",
              " 'minute': 619,\n",
              " 'using': 620,\n",
              " 'message': 621,\n",
              " 'florida': 622,\n",
              " 'career': 623,\n",
              " 'become': 624,\n",
              " 'kid': 625,\n",
              " 'moment': 626,\n",
              " 'today': 627,\n",
              " 'huffpost': 628,\n",
              " 'watching': 629,\n",
              " 'stephen': 630,\n",
              " 'break': 631,\n",
              " 't': 632,\n",
              " 'un': 633,\n",
              " '2015': 634,\n",
              " 'democratic': 635,\n",
              " 'fox': 636,\n",
              " 'shit': 637,\n",
              " 'offers': 638,\n",
              " 'thinking': 639,\n",
              " 'george': 640,\n",
              " 'adorable': 641,\n",
              " 'response': 642,\n",
              " 'read': 643,\n",
              " 'weekend': 644,\n",
              " 'months': 645,\n",
              " 'visit': 646,\n",
              " 'hair': 647,\n",
              " 'victims': 648,\n",
              " \"america's\": 649,\n",
              " 'told': 650,\n",
              " 'hope': 651,\n",
              " 'least': 652,\n",
              " 'different': 653,\n",
              " 'point': 654,\n",
              " 'travel': 655,\n",
              " 'front': 656,\n",
              " 'billion': 657,\n",
              " 'since': 658,\n",
              " 'rules': 659,\n",
              " 'russian': 660,\n",
              " 'feel': 661,\n",
              " 'happy': 662,\n",
              " 'protest': 663,\n",
              " 'straight': 664,\n",
              " 'gift': 665,\n",
              " 'union': 666,\n",
              " 'bus': 667,\n",
              " 'crash': 668,\n",
              " \"she's\": 669,\n",
              " 'anything': 670,\n",
              " 'dating': 671,\n",
              " 'k': 672,\n",
              " 'scott': 673,\n",
              " 'david': 674,\n",
              " 'accused': 675,\n",
              " \"obama's\": 676,\n",
              " 'taylor': 677,\n",
              " 'trailer': 678,\n",
              " 'kind': 679,\n",
              " 'names': 680,\n",
              " 'well': 681,\n",
              " 'key': 682,\n",
              " 'order': 683,\n",
              " 'true': 684,\n",
              " 'reasons': 685,\n",
              " 'cop': 686,\n",
              " 'pence': 687,\n",
              " 'reason': 688,\n",
              " 'joe': 689,\n",
              " 'dinner': 690,\n",
              " 'oil': 691,\n",
              " 'syria': 692,\n",
              " 'powerful': 693,\n",
              " 'sick': 694,\n",
              " '100': 695,\n",
              " 'brings': 696,\n",
              " 'global': 697,\n",
              " 'date': 698,\n",
              " 'lead': 699,\n",
              " 'policy': 700,\n",
              " 'spends': 701,\n",
              " 'learned': 702,\n",
              " 'millions': 703,\n",
              " 'lessons': 704,\n",
              " 'francis': 705,\n",
              " 'c': 706,\n",
              " 'struggling': 707,\n",
              " 'low': 708,\n",
              " 'discover': 709,\n",
              " 'song': 710,\n",
              " 'opens': 711,\n",
              " 'far': 712,\n",
              " 'himself': 713,\n",
              " 'girlfriend': 714,\n",
              " 'j': 715,\n",
              " 'vows': 716,\n",
              " 'worried': 717,\n",
              " 'leave': 718,\n",
              " 'surprise': 719,\n",
              " 'kills': 720,\n",
              " 'united': 721,\n",
              " 'hate': 722,\n",
              " 'leader': 723,\n",
              " 'nfl': 724,\n",
              " 'oscar': 725,\n",
              " 'style': 726,\n",
              " 'whole': 727,\n",
              " 'anniversary': 728,\n",
              " 'turns': 729,\n",
              " 'conversation': 730,\n",
              " 'hit': 731,\n",
              " 'waiting': 732,\n",
              " 'general': 733,\n",
              " 'fighting': 734,\n",
              " 'schools': 735,\n",
              " 'host': 736,\n",
              " '2017': 737,\n",
              " 'killing': 738,\n",
              " 'apple': 739,\n",
              " '40': 740,\n",
              " 'prison': 741,\n",
              " '2014': 742,\n",
              " 'adds': 743,\n",
              " 'drunk': 744,\n",
              " \"they're\": 745,\n",
              " 'd': 746,\n",
              " 'near': 747,\n",
              " 'rubio': 748,\n",
              " 'reality': 749,\n",
              " 'weird': 750,\n",
              " 'senator': 751,\n",
              " 'feels': 752,\n",
              " 'possible': 753,\n",
              " 'moving': 754,\n",
              " 'stars': 755,\n",
              " 'signs': 756,\n",
              " 'politics': 757,\n",
              " 'sign': 758,\n",
              " 'jr': 759,\n",
              " 'immigration': 760,\n",
              " 'leaders': 761,\n",
              " 'stand': 762,\n",
              " 'called': 763,\n",
              " 'tour': 764,\n",
              " 'returns': 765,\n",
              " 'totally': 766,\n",
              " 'reportedly': 767,\n",
              " 'massive': 768,\n",
              " 'sports': 769,\n",
              " 'major': 770,\n",
              " 'force': 771,\n",
              " 'advice': 772,\n",
              " 'weight': 773,\n",
              " 'breaks': 774,\n",
              " 'number': 775,\n",
              " 'bring': 776,\n",
              " 'transgender': 777,\n",
              " 'west': 778,\n",
              " 'mass': 779,\n",
              " 'spring': 780,\n",
              " 'door': 781,\n",
              " 'fall': 782,\n",
              " 'investigation': 783,\n",
              " 'tom': 784,\n",
              " 'employees': 785,\n",
              " 'test': 786,\n",
              " 'relationship': 787,\n",
              " 'important': 788,\n",
              " 'puts': 789,\n",
              " 'iowa': 790,\n",
              " 'stage': 791,\n",
              " 'murder': 792,\n",
              " 'candidates': 793,\n",
              " 'close': 794,\n",
              " 'magazine': 795,\n",
              " 'early': 796,\n",
              " 'trans': 797,\n",
              " 'side': 798,\n",
              " 'store': 799,\n",
              " 'huge': 800,\n",
              " 'artist': 801,\n",
              " 'christian': 802,\n",
              " 'die': 803,\n",
              " 'abuse': 804,\n",
              " 'longer': 805,\n",
              " 'members': 806,\n",
              " 'worst': 807,\n",
              " 'peace': 808,\n",
              " 'loses': 809,\n",
              " 'act': 810,\n",
              " 'kardashian': 811,\n",
              " 'return': 812,\n",
              " 'audience': 813,\n",
              " 'five': 814,\n",
              " 'steve': 815,\n",
              " 'sean': 816,\n",
              " 'suspect': 817,\n",
              " 'prince': 818,\n",
              " 'ben': 819,\n",
              " 'attacks': 820,\n",
              " 'suicide': 821,\n",
              " 'program': 822,\n",
              " 'decision': 823,\n",
              " 'hits': 824,\n",
              " 'awards': 825,\n",
              " 'queer': 826,\n",
              " 'whether': 827,\n",
              " 'cops': 828,\n",
              " 'workers': 829,\n",
              " 'responds': 830,\n",
              " 'push': 831,\n",
              " 'fuck': 832,\n",
              " 'iraq': 833,\n",
              " 'google': 834,\n",
              " 'leaving': 835,\n",
              " 'carolina': 836,\n",
              " 'final': 837,\n",
              " 'voice': 838,\n",
              " 'chinese': 839,\n",
              " '13': 840,\n",
              " 'lgbt': 841,\n",
              " 'executive': 842,\n",
              " 'seen': 843,\n",
              " 'jimmy': 844,\n",
              " 'given': 845,\n",
              " 'keeps': 846,\n",
              " 'shares': 847,\n",
              " 'industry': 848,\n",
              " 'biggest': 849,\n",
              " 'protesters': 850,\n",
              " 'results': 851,\n",
              " 'press': 852,\n",
              " 'center': 853,\n",
              " 'lose': 854,\n",
              " 'almost': 855,\n",
              " 'light': 856,\n",
              " 'risk': 857,\n",
              " 'asking': 858,\n",
              " 'emotional': 859,\n",
              " 'paris': 860,\n",
              " 'michelle': 861,\n",
              " 'male': 862,\n",
              " 'hands': 863,\n",
              " 'road': 864,\n",
              " 'bathroom': 865,\n",
              " 'allegations': 866,\n",
              " 'bowl': 867,\n",
              " 'begins': 868,\n",
              " 'planned': 869,\n",
              " 'homeless': 870,\n",
              " 'poor': 871,\n",
              " 'planet': 872,\n",
              " 'hoping': 873,\n",
              " 'governor': 874,\n",
              " 'non': 875,\n",
              " 'east': 876,\n",
              " 'fails': 877,\n",
              " 'cool': 878,\n",
              " 'wait': 879,\n",
              " 'moms': 880,\n",
              " 'humans': 881,\n",
              " 'mind': 882,\n",
              " 'defense': 883,\n",
              " 'pizza': 884,\n",
              " 'ferguson': 885,\n",
              " 'feeling': 886,\n",
              " 'amazon': 887,\n",
              " 'hall': 888,\n",
              " 'worse': 889,\n",
              " 'starting': 890,\n",
              " 'fear': 891,\n",
              " 'nasa': 892,\n",
              " 'problems': 893,\n",
              " 'check': 894,\n",
              " 'ebola': 895,\n",
              " 'role': 896,\n",
              " 'building': 897,\n",
              " 'arrested': 898,\n",
              " 'driving': 899,\n",
              " 'four': 900,\n",
              " 'scandal': 901,\n",
              " 'space': 902,\n",
              " 'moore': 903,\n",
              " 'science': 904,\n",
              " 'dance': 905,\n",
              " 'easy': 906,\n",
              " 'hand': 907,\n",
              " 'israel': 908,\n",
              " 'picture': 909,\n",
              " 'eyes': 910,\n",
              " 'voter': 911,\n",
              " \"there's\": 912,\n",
              " 'mental': 913,\n",
              " 'action': 914,\n",
              " 'cut': 915,\n",
              " 'spend': 916,\n",
              " 'spot': 917,\n",
              " 'board': 918,\n",
              " 'chance': 919,\n",
              " 'words': 920,\n",
              " 'suggests': 921,\n",
              " 'urges': 922,\n",
              " 'williams': 923,\n",
              " 'coffee': 924,\n",
              " 'green': 925,\n",
              " 'apologizes': 926,\n",
              " 'celebrate': 927,\n",
              " 'documentary': 928,\n",
              " 'supporters': 929,\n",
              " 'remember': 930,\n",
              " 'depression': 931,\n",
              " 'halloween': 932,\n",
              " 'deadly': 933,\n",
              " 'louis': 934,\n",
              " 'pro': 935,\n",
              " 'oscars': 936,\n",
              " 'interview': 937,\n",
              " 'park': 938,\n",
              " 'completely': 939,\n",
              " 'harassment': 940,\n",
              " 'card': 941,\n",
              " 'university': 942,\n",
              " 'success': 943,\n",
              " 'coworker': 944,\n",
              " 'pick': 945,\n",
              " 'doctor': 946,\n",
              " 'harvey': 947,\n",
              " 'happens': 948,\n",
              " 'throws': 949,\n",
              " 'band': 950,\n",
              " 'pregnant': 951,\n",
              " 'hear': 952,\n",
              " 'crowd': 953,\n",
              " 'deep': 954,\n",
              " 'amazing': 955,\n",
              " 'official': 956,\n",
              " 'chicken': 957,\n",
              " 'ideas': 958,\n",
              " 'refugees': 959,\n",
              " 'forward': 960,\n",
              " 'opening': 961,\n",
              " 'reveal': 962,\n",
              " 'mean': 963,\n",
              " 'simple': 964,\n",
              " 'economy': 965,\n",
              " 'reminds': 966,\n",
              " 'which': 967,\n",
              " 'turn': 968,\n",
              " 'healthy': 969,\n",
              " 'senators': 970,\n",
              " 'swift': 971,\n",
              " 'eat': 972,\n",
              " 'hurricane': 973,\n",
              " '17': 974,\n",
              " 'rest': 975,\n",
              " 'album': 976,\n",
              " 'despite': 977,\n",
              " \"woman's\": 978,\n",
              " 'chicago': 979,\n",
              " 'movies': 980,\n",
              " 'happened': 981,\n",
              " 'officer': 982,\n",
              " 'across': 983,\n",
              " 'desperate': 984,\n",
              " 'uses': 985,\n",
              " 'apartment': 986,\n",
              " 'plane': 987,\n",
              " 'realizes': 988,\n",
              " 'funding': 989,\n",
              " 'holding': 990,\n",
              " 'leading': 991,\n",
              " 'crime': 992,\n",
              " 'demand': 993,\n",
              " \"we're\": 994,\n",
              " 'explains': 995,\n",
              " 'seeing': 996,\n",
              " 'slams': 997,\n",
              " 'residents': 998,\n",
              " 'sales': 999,\n",
              " \"let's\": 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rufbtj6aRo_n",
        "outputId": "cc90ec6f-956f-4206-edcb-824d577d06f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25637"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "len(tokenizer.word_index)   # 전체 row size 26709 = 20000 + 6709 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W23IUjuNRo_s"
      },
      "outputs": [],
      "source": [
        "word_index = tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7-kPEk8Ro_x",
        "outputId": "29d70cc9-b9c0-4f9b-c3a6-aa3bc89dd252"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13894"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "word_index['hi']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aurZjEeQRo_0",
        "outputId": "cf8afe7c-fbe5-4037-b529-d60ee599ae0a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "word_index['<oov>']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozaMIfx_Ro_4"
      },
      "source": [
        "### 문장(sentences)을 토큰으로 변경 (수치형으로 변경) - texts_to_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYNXTtTIRo_5"
      },
      "outputs": [],
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKH_vITVRo_8",
        "outputId": "1f7dfc3e-03cb-4048-9dbf-31d12f04e9f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[328, 1, 799, 1, 1, 47, 389, 1, 1, 6, 1, 1],\n",
              " [4, 1, 1, 1, 23, 2, 161, 1, 390, 1, 6, 251, 9, 889],\n",
              " [153, 890, 2, 891, 1, 1, 595, 1, 221, 133, 36, 45, 2, 1],\n",
              " [1, 38, 213, 382, 2, 1, 29, 288, 23, 10, 1, 1, 1, 958],\n",
              " [715, 672, 1, 1, 1, 662, 553, 5, 4, 92, 1, 90]]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "train_sequences[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfwQcmvXRpAJ",
        "outputId": "edeca5a7-0961-47ba-bd1f-ba76c4b848ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(715, 672, 5652, 1043, 8865, 662)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "word_index['j'], word_index['k'], word_index['rowling'], word_index['wishes'], word_index['snape'], word_index['happy']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op-PCD4uIDVe"
      },
      "source": [
        "### test 문장에도 `Tokenizer`를 적용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAfesimPIDVf"
      },
      "outputs": [],
      "source": [
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRCwRYueRpAQ"
      },
      "source": [
        "### 시퀀스의 길이를 맞춰주기 (pad_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAZyufTlRpAR"
      },
      "source": [
        "3가지 옵션을 입력해 줍니다.\n",
        "\n",
        "* `maxlen`: 최대 문장 길이를 정의합니다. 최대 문장길이보다 길면, 잘라냅니다.\n",
        "* `truncating`: 문장의 길이가 `maxlen`보다 길 때 앞을 자를지 뒤를 자를지 정의합니다.\n",
        "* `padding`: 문장의 길이가 `maxlen`보다 짧을 때 채워줄 값을 앞을 채울지, 뒤를 채울지 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdaO9ErnRpAS"
      },
      "outputs": [],
      "source": [
        "x_train = pad_sequences(train_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAudfwjTRpAV",
        "outputId": "2fae48c8-8f23-416a-f60f-cda65b189f8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 120)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XGNTaoppzpn",
        "outputId": "d286faf1-f1f5-4613-e0fc-7d85fffb5609"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([328,   1, 799,   1,   1,  47, 389,   1,   1,   6,   1,   1,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "x_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7WULHBAIDVw"
      },
      "outputs": [],
      "source": [
        "x_test = pad_sequences(test_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMi3c54vIDV0",
        "outputId": "c91d3596-bc40-4ba3-e1bf-c40b3f2aa3f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6709, 120)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNBCVNd-lj9K",
        "outputId": "a472fef9-3350-4031-a2eb-19c6da45d4f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1,   1,   1,   1,  30,   1,   1,   5, 519, 109,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "x_test[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p77o7VlIDV4"
      },
      "source": [
        "### Label 값을 Numpy Array로 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSTya5jCIDV5"
      },
      "outputs": [],
      "source": [
        "y_train = np.array(train_labels)\n",
        "y_test = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf2KWdtPqTmP",
        "outputId": "08e125b0-4735-4ae4-8638-0bd09d90c5f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "y_train[0], y_test[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV72Y76tRpA3"
      },
      "source": [
        "##4. 모델설계하기\n",
        "###Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8E8liATRpBw"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Flatten(),\n",
        "    Dense(32, activation='relu'),\n",
        "    # YOUR CODE HERE. KEEP THIS OUTPUT LAYER INTACT OR TESTS MAY FAIL\n",
        "    Dense(1, activation='sigmoid')\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCdAX_rXRpB0"
      },
      "source": [
        "### optimizer, loss 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpJgAudwRpB1"
      },
      "source": [
        "`loss`는 이진 분류이기 때문에 `binary_crossentropy`를 사용하고, optimizer는 `adam`을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2iupQVBRpB1"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFx1ZmQ4IDXW"
      },
      "source": [
        "##6. 모델 모니터링하기(체크포인트 생성)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_AvlY0cIDXX"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "checkpoint_path = 'my_checkpoint.ckpt'\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
        "    save_weights_only=True, \n",
        "    save_best_only=True, \n",
        "    monitor='val_loss',\n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKy7ZkL6RpB4"
      },
      "source": [
        "##5. 모델 학습하기 - 0.36931\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byjJCmN_RpB4",
        "outputId": "c3ffb109-7310-4876-e809-6e1d0e527de2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.4545 - accuracy: 0.7717\n",
            "Epoch 1: val_loss improved from inf to 0.38991, saving model to my_checkpoint.ckpt\n",
            "625/625 [==============================] - 48s 53ms/step - loss: 0.4545 - accuracy: 0.7717 - val_loss: 0.3899 - val_accuracy: 0.8193\n",
            "Epoch 2/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.3540 - accuracy: 0.8394\n",
            "Epoch 2: val_loss improved from 0.38991 to 0.37284, saving model to my_checkpoint.ckpt\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.3540 - accuracy: 0.8394 - val_loss: 0.3728 - val_accuracy: 0.8316\n",
            "Epoch 3/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.3247 - accuracy: 0.8559\n",
            "Epoch 3: val_loss improved from 0.37284 to 0.36931, saving model to my_checkpoint.ckpt\n",
            "625/625 [==============================] - 19s 31ms/step - loss: 0.3247 - accuracy: 0.8559 - val_loss: 0.3693 - val_accuracy: 0.8323\n",
            "Epoch 4/50\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.3067 - accuracy: 0.8644\n",
            "Epoch 4: val_loss did not improve from 0.36931\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.3067 - accuracy: 0.8643 - val_loss: 0.3814 - val_accuracy: 0.8281\n",
            "Epoch 5/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2932 - accuracy: 0.8724\n",
            "Epoch 5: val_loss did not improve from 0.36931\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2932 - accuracy: 0.8724 - val_loss: 0.3802 - val_accuracy: 0.8277\n",
            "Epoch 6/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2832 - accuracy: 0.8763\n",
            "Epoch 6: val_loss did not improve from 0.36931\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.2832 - accuracy: 0.8763 - val_loss: 0.3771 - val_accuracy: 0.8284\n",
            "Epoch 7/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2720 - accuracy: 0.8828\n",
            "Epoch 7: val_loss did not improve from 0.36931\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2720 - accuracy: 0.8828 - val_loss: 0.4061 - val_accuracy: 0.8292\n",
            "Epoch 8/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2633 - accuracy: 0.8862\n",
            "Epoch 8: val_loss did not improve from 0.36931\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.2633 - accuracy: 0.8862 - val_loss: 0.4248 - val_accuracy: 0.8259\n",
            "Epoch 9/50\n",
            "623/625 [============================>.] - ETA: 0s - loss: 0.2549 - accuracy: 0.8912\n",
            "Epoch 9: val_loss did not improve from 0.36931\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2553 - accuracy: 0.8911 - val_loss: 0.4133 - val_accuracy: 0.8217\n",
            "Epoch 10/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.8939\n",
            "Epoch 10: val_loss did not improve from 0.36931\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.2471 - accuracy: 0.8939 - val_loss: 0.4174 - val_accuracy: 0.8250\n",
            "Epoch 11/50\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.2408 - accuracy: 0.8953\n",
            "Epoch 11: val_loss did not improve from 0.36931\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.2407 - accuracy: 0.8953 - val_loss: 0.4444 - val_accuracy: 0.8192\n",
            "Epoch 12/50\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.2326 - accuracy: 0.9012\n",
            "Epoch 12: val_loss did not improve from 0.36931\n",
            "625/625 [==============================] - 18s 29ms/step - loss: 0.2326 - accuracy: 0.9011 - val_loss: 0.4453 - val_accuracy: 0.8225\n",
            "Epoch 13/50\n",
            "623/625 [============================>.] - ETA: 0s - loss: 0.2280 - accuracy: 0.9019\n",
            "Epoch 13: val_loss did not improve from 0.36931\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.2281 - accuracy: 0.9018 - val_loss: 0.4803 - val_accuracy: 0.8183\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x_train, y_train, \n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[checkpoint, early_stopping],\n",
        "    epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liPLiUiJR9gd"
      },
      "outputs": [],
      "source": [
        "model.load_weights(checkpoint_path)\n",
        "model.save(\"sarcasm.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R27ntL_ERpB9"
      },
      "source": [
        "##7. 모델 검증하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "CtPbwtZv0B9S",
        "outputId": "92d28b41-ccc5-4798-d71e-095f0b4446a7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "7nQfu0AU0WAQ",
        "outputId": "9fd063f5-4b77-4cb1-a366-2787a1385474"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwZM9b6yBBkn"
      },
      "source": [
        "### 모델 저장하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKBiDVEvMmWG",
        "outputId": "3955c45e-3884-41ec-9e25-0916d6c81cef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "former <oov> store <oov> <oov> over secret <oov> <oov> for <oov> <oov> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
            "mom starting to fear son's web series closest thing she will have to grandchild\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_sentence(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "print(decode_sentence(x_train[0]))\n",
        "print(train_sentences[2])\n",
        "print(labels[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v_Dcl09RpCZ"
      },
      "outputs": [],
      "source": [
        "model.load_weights(checkpoint_path)\n",
        "model.save(\"sarcasm.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A8MvQdfze-Ge"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHMY2mUn0spx"
      },
      "source": [
        "# visualize word2vec using google embedding projector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqOpsXQI6WvV",
        "outputId": "78077e5d-92e3-46f8-8502-1ad0836e4fa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 16)\n"
          ]
        }
      ],
      "source": [
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print(weights.shape) # shape: (vocab_size, embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0YmN4PsZFAd"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "for word_num in range(1, vocab_size):\n",
        "  word = reverse_word_index[word_num]\n",
        "  embeddings = weights[word_num]\n",
        "  out_m.write(word + \"\\n\")\n",
        "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3y-nr5HD3TxJ",
        "outputId": "c742a8c1-0733-454e-a470-21aa655a5596"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4885ee35-1e73-4e5b-99d0-845b206ae879\", \"vecs.tsv\", 194124)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_78be3b3c-5ffd-43ac-b706-28a17af928a8\", \"meta.tsv\", 6275)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download('vecs.tsv')\n",
        "  files.download('meta.tsv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1MaZEFzA1P-h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UKKwRCEk3apz",
        "outputId": "592c7669-7a85-46f2-8de3-c39f09d7fe29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[88], [], [1], [1], [7], [1], [1], [1], [], [632], [1], [88], [1], [672], [], [632], [1], [88], [54], [], [88], [54], [], [7], [1], [7], [1], [88], [1], [1], [], [], [1], [1], [1], [1], [54], [632], []]\n"
          ]
        }
      ],
      "source": [
        "sentence = \"I really think this is amazing. honest.\"\n",
        "sequence = tokenizer.texts_to_sequences(sentence)\n",
        "print(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Bx77X-jrAUIm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 1 : Bidirectional"
      ],
      "metadata": {
        "id": "I6QYfww96ezC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-1. LSTM - 0.37946"
      ],
      "metadata": {
        "id": "42qP2xra_-E-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# NLP QUESTION\n",
        "#\n",
        "# For this task you will build a classifier for the sarcasm dataset\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown\n",
        "# It will be tested against a number of sentences that the network hasn't previously seen\n",
        "# And you will be scored on whether sarcasm was correctly detected in those sentences\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "# import libraries\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Bidirectional, LSTM, Dense, Input, Dropout, BatchNormalization\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    padding_type='post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "    \n",
        "    sentences = []\n",
        "    labels = []\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    # json file load\n",
        "    with open('sarcasm.json') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    # Sentence 와 Label 로 나누기\n",
        "    for d in data:\n",
        "        sentences.append(d['headline'])\n",
        "        labels.append(d['is_sarcastic'])\n",
        "        \n",
        "    # 주어지는 값. 전체 데이터셋을 앞에서 20000개는 training으로 나머지는 test로 사용하기 위해서 나눈다\n",
        "    train_sentences = sentences[:training_size]\n",
        "    train_labels = labels[:training_size]\n",
        "    test_sentences = sentences[training_size:]\n",
        "    test_labels = labels[training_size:]\n",
        "    \n",
        "    # Tokenizer 정의\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "    \n",
        "    # 문자 데이터를 입력받아서 리스트의 형태로 변환\n",
        "    # = 공백 기준으로 토크나이징\n",
        "    # vocab 생성\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "    \n",
        "    # 텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n",
        "    # = 문장 -> 토큰 -> indexing\n",
        "    train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "    \n",
        "    # 문장 길이 맞추기 (padding)\n",
        "    x_train = pad_sequences(train_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\n",
        "    x_test = pad_sequences(test_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\n",
        "    \n",
        "    # Label 값을 Numpy Array로 변환\n",
        "    y_train = np.array(train_labels)\n",
        "    y_test = np.array(test_labels)\n",
        "    \n",
        "    model = tf.keras.Sequential([\n",
        "    # YOUR CODE HERE. KEEP THIS OUTPUT LAYER INTACT OR TESTS MAY FAIL\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        Bidirectional(LSTM(64)),\n",
        "        Flatten(),\n",
        "        Dense(32, activation='relu'),\n",
        "    \n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        \n",
        "    ])\n",
        "    \n",
        "    # optimizer, loss 정의\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    # callback\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "    checkpoint_path = 'my_checkpoint.ckpt'\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
        "        save_weights_only=True, \n",
        "        save_best_only=True, \n",
        "        monitor='val_loss',\n",
        "        verbose=1)\n",
        "    \n",
        "    # Train\n",
        "    history = model.fit(x_train, y_train, \n",
        "        validation_data=(x_test, y_test),\n",
        "        callbacks=[checkpoint, early_stopping],\n",
        "        epochs=50)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ps2Qu6Ss6jzE",
        "outputId": "6529ab40-6b99-41db-c5d8-c1816eccf9c8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.4409 - accuracy: 0.7816\n",
            "Epoch 1: val_loss improved from inf to 0.39326, saving model to my_checkpoint.ckpt\n",
            "625/625 [==============================] - 64s 64ms/step - loss: 0.4409 - accuracy: 0.7816 - val_loss: 0.3933 - val_accuracy: 0.8159\n",
            "Epoch 2/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.3532 - accuracy: 0.8385\n",
            "Epoch 2: val_loss improved from 0.39326 to 0.37946, saving model to my_checkpoint.ckpt\n",
            "625/625 [==============================] - 27s 44ms/step - loss: 0.3532 - accuracy: 0.8385 - val_loss: 0.3795 - val_accuracy: 0.8287\n",
            "Epoch 3/50\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.3261 - accuracy: 0.8549\n",
            "Epoch 3: val_loss did not improve from 0.37946\n",
            "625/625 [==============================] - 23s 37ms/step - loss: 0.3260 - accuracy: 0.8549 - val_loss: 0.3847 - val_accuracy: 0.8269\n",
            "Epoch 4/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.3050 - accuracy: 0.8658\n",
            "Epoch 4: val_loss did not improve from 0.37946\n",
            "625/625 [==============================] - 23s 36ms/step - loss: 0.3050 - accuracy: 0.8658 - val_loss: 0.3859 - val_accuracy: 0.8302\n",
            "Epoch 5/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2909 - accuracy: 0.8759\n",
            "Epoch 5: val_loss did not improve from 0.37946\n",
            "625/625 [==============================] - 21s 34ms/step - loss: 0.2909 - accuracy: 0.8759 - val_loss: 0.3926 - val_accuracy: 0.8241\n",
            "Epoch 6/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2810 - accuracy: 0.8776\n",
            "Epoch 6: val_loss did not improve from 0.37946\n",
            "625/625 [==============================] - 20s 32ms/step - loss: 0.2810 - accuracy: 0.8776 - val_loss: 0.3887 - val_accuracy: 0.8284\n",
            "Epoch 7/50\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.2696 - accuracy: 0.8849\n",
            "Epoch 7: val_loss did not improve from 0.37946\n",
            "625/625 [==============================] - 21s 34ms/step - loss: 0.2698 - accuracy: 0.8847 - val_loss: 0.3957 - val_accuracy: 0.8211\n",
            "Epoch 8/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2592 - accuracy: 0.8896\n",
            "Epoch 8: val_loss did not improve from 0.37946\n",
            "625/625 [==============================] - 21s 33ms/step - loss: 0.2592 - accuracy: 0.8896 - val_loss: 0.4475 - val_accuracy: 0.8183\n",
            "Epoch 9/50\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.2512 - accuracy: 0.8946\n",
            "Epoch 9: val_loss did not improve from 0.37946\n",
            "625/625 [==============================] - 20s 33ms/step - loss: 0.2513 - accuracy: 0.8945 - val_loss: 0.4379 - val_accuracy: 0.8240\n",
            "Epoch 10/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2451 - accuracy: 0.8959\n",
            "Epoch 10: val_loss did not improve from 0.37946\n",
            "625/625 [==============================] - 22s 35ms/step - loss: 0.2451 - accuracy: 0.8959 - val_loss: 0.4189 - val_accuracy: 0.8211\n",
            "Epoch 11/50\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.2463 - accuracy: 0.8970\n",
            "Epoch 11: val_loss did not improve from 0.37946\n",
            "625/625 [==============================] - 20s 33ms/step - loss: 0.2464 - accuracy: 0.8970 - val_loss: 0.4264 - val_accuracy: 0.8171\n",
            "Epoch 12/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2354 - accuracy: 0.9017\n",
            "Epoch 12: val_loss did not improve from 0.37946\n",
            "625/625 [==============================] - 20s 32ms/step - loss: 0.2354 - accuracy: 0.9017 - val_loss: 0.4165 - val_accuracy: 0.8222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-2. GRU - 0.37173"
      ],
      "metadata": {
        "id": "I-u9xMfhABUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# NLP QUESTION\n",
        "#\n",
        "# For this task you will build a classifier for the sarcasm dataset\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown\n",
        "# It will be tested against a number of sentences that the network hasn't previously seen\n",
        "# And you will be scored on whether sarcasm was correctly detected in those sentences\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "# import libraries\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Bidirectional, GRU, Dense, Input, Dropout, BatchNormalization\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    padding_type='post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "    \n",
        "    sentences = []\n",
        "    labels = []\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    # json file load\n",
        "    with open('sarcasm.json') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    # Sentence 와 Label 로 나누기\n",
        "    for d in data:\n",
        "        sentences.append(d['headline'])\n",
        "        labels.append(d['is_sarcastic'])\n",
        "        \n",
        "    # 주어지는 값. 전체 데이터셋을 앞에서 20000개는 training으로 나머지는 test로 사용하기 위해서 나눈다\n",
        "    train_sentences = sentences[:training_size]\n",
        "    train_labels = labels[:training_size]\n",
        "    test_sentences = sentences[training_size:]\n",
        "    test_labels = labels[training_size:]\n",
        "    \n",
        "    # Tokenizer 정의\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "    \n",
        "    # 문자 데이터를 입력받아서 리스트의 형태로 변환\n",
        "    # = 공백 기준으로 토크나이징\n",
        "    # vocab 생성\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "    \n",
        "    # 텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n",
        "    # = 문장 -> 토큰 -> indexing\n",
        "    train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "    \n",
        "    # 문장 길이 맞추기 (padding)\n",
        "    x_train = pad_sequences(train_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\n",
        "    x_test = pad_sequences(test_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\n",
        "    \n",
        "    # Label 값을 Numpy Array로 변환\n",
        "    y_train = np.array(train_labels)\n",
        "    y_test = np.array(test_labels)\n",
        "    \n",
        "    from transformers import TFRobertaForSequenceClassification\n",
        "\n",
        "    roberta_model = TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        roberta_model,\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "\n",
        "    \n",
        "    # optimizer, loss 정의\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    # callback\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "    checkpoint_path = 'my_checkpoint.ckpt'\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
        "        save_weights_only=True, \n",
        "        save_best_only=True, \n",
        "        monitor='val_loss',\n",
        "        verbose=1)\n",
        "    \n",
        "    # Train\n",
        "    history = model.fit(x_train, y_train, \n",
        "        validation_data=(x_test, y_test),\n",
        "        callbacks=[checkpoint, early_stopping],\n",
        "        epochs=50)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "id": "6t7TZ6AB_177",
        "outputId": "29e236a5-d8d3-4c55-a6e2-c95640f4f3b3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-cb2fd31cfb4e>\u001b[0m in \u001b[0;36m<cell line: 117>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;31m# and a score will be returned to you\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolution_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mymodel.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-cb2fd31cfb4e>\u001b[0m in \u001b[0;36msolution_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     history = model.fit(x_train, y_train, \n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    TypeError: Exception encountered when calling layer 'dropout_152' (type Dropout).\n    \n    Expected any non-tensor type, but got a tensor instead.\n    \n    Call arguments received by layer 'dropout_152' (type Dropout):\n      • inputs=TFSequenceClassifierOutput(loss=None, logits='tf.Tensor(shape=(32, 2), dtype=float32)', hidden_states=None, attentions=None)\n      • training=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 2 : Transfer Learning(RoBERTa) - ERROR!\n",
        "\n",
        "업계에서 BERT 보다 널리 쓰이는 RoBERTa 로 Transfer Learning 시도"
      ],
      "metadata": {
        "id": "wf3LFdVZC_yP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX7fdNsmGB-4",
        "outputId": "82fc7c35-418d-4cad-dc31-a156d595770b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 KB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.27.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# NLP QUESTION\n",
        "#\n",
        "# For this task you will build a classifier for the sarcasm dataset\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown\n",
        "# It will be tested against a number of sentences that the network hasn't previously seen\n",
        "# And you will be scored on whether sarcasm was correctly detected in those sentences\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "# import libraries\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Bidirectional, LSTM, Dense, Input, Dropout, BatchNormalization\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from transformers import TFRobertaModel\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    padding_type='post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "    \n",
        "    sentences = []\n",
        "    labels = []\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    # json file load\n",
        "    with open('sarcasm.json') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    # Sentence 와 Label 로 나누기\n",
        "    for d in data:\n",
        "        sentences.append(d['headline'])\n",
        "        labels.append(d['is_sarcastic'])\n",
        "        \n",
        "    # 주어지는 값. 전체 데이터셋을 앞에서 20000개는 training으로 나머지는 test로 사용하기 위해서 나눈다\n",
        "    train_sentences = sentences[:training_size]\n",
        "    train_labels = labels[:training_size]\n",
        "    test_sentences = sentences[training_size:]\n",
        "    test_labels = labels[training_size:]\n",
        "    \n",
        "    # Tokenizer 정의\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "    \n",
        "    # 문자 데이터를 입력받아서 리스트의 형태로 변환\n",
        "    # = 공백 기준으로 토크나이징\n",
        "    # vocab 생성\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "    \n",
        "    # 텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n",
        "    # = 문장 -> 토큰 -> indexing\n",
        "    train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "    \n",
        "    # 문장 길이 맞추기 (padding)\n",
        "    x_train = pad_sequences(train_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\n",
        "    x_test = pad_sequences(test_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\n",
        "    \n",
        "    # Label 값을 Numpy Array로 변환\n",
        "    y_train = np.array(train_labels)\n",
        "    y_test = np.array(test_labels)\n",
        "\n",
        "    roberta_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "    model = Sequential([\n",
        "        roberta_model,\n",
        "        Dropout(0.2),\n",
        "        BatchNormalization(),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        BatchNormalization(),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    # optimizer, loss 정의\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    # callback\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "    checkpoint_path = 'my_checkpoint.ckpt'\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
        "        save_weights_only=True, \n",
        "        save_best_only=True, \n",
        "        monitor='val_loss',\n",
        "        verbose=1)\n",
        "    \n",
        "    # Train\n",
        "    history = model.fit(x_train, y_train, \n",
        "        validation_data=(x_test, y_test),\n",
        "        callbacks=[checkpoint, early_stopping],\n",
        "        epochs=50)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 928
        },
        "id": "srmCRikCBieS",
        "outputId": "76756427-19b5-4fcc-928c-9adccc743677"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-05fcc441f2d4>\u001b[0m in \u001b[0;36m<cell line: 119>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;31m# and a score will be returned to you\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolution_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mymodel.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-05fcc441f2d4>\u001b[0m in \u001b[0;36msolution_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     history = model.fit(x_train, y_train, \n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/sequential.py\", line 351, in _build_graph_network_for_inferred_shape\n        raise ValueError(SINGLE_LAYER_OUTPUT_ERROR_MSG)\n\n    ValueError: Exception encountered when calling layer 'sequential_8' (type Sequential).\n    \n    All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.\n    \n    Call arguments received by layer 'sequential_8' (type Sequential):\n      • inputs=tf.Tensor(shape=(32, 120), dtype=int32)\n      • training=True\n      • mask=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 3. ChatGPT"
      ],
      "metadata": {
        "id": "3avEpzryEOyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-1. Bidirectional - 0.37531\n",
        "다음 코드를 최적화해 줘,  \n",
        "단, 다음 조건을 적용해 줘  \n",
        "Use dropout  \n",
        "Use batch normalization  \n",
        "Use checkpoint with save best only, and save weight only   and verbose = 1  \n",
        "Use early stopping with patience = 10  \n",
        "Use reduce learning rate on plateau  \n",
        "set verbose = 1  \n",
        "Set epochs = 50  \n",
        "do not use data augmentation  \n",
        "Use other methods to make val loss value lower.  \n",
        "\n",
        "\n",
        "ChatGPT Answer:\n",
        "\n",
        "주요 변경 사항:\n",
        "- Dropout layer와 BatchNormalization layer 추가\n",
        "- ReduceLROnPlateau callback 추가\n",
        "- Adam optimizer의 learning rate 조정\n",
        "- 모델 구성 코드를 좀 더 가독성 있게 수정"
      ],
      "metadata": {
        "id": "wze0w-kcGUSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# NLP QUESTION\n",
        "#\n",
        "# For this task you will build a classifier for the sarcasm dataset\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown\n",
        "# It will be tested against a number of sentences that the network hasn't previously seen\n",
        "# And you will be scored on whether sarcasm was correctly detected in those sentences\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "# import libraries\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Flatten, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    padding_type='post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "    \n",
        "    sentences = []\n",
        "    labels = []\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    # json file load\n",
        "    with open('sarcasm.json') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    # Sentence 와 Label 로 나누기\n",
        "    for d in data:\n",
        "        sentences.append(d['headline'])\n",
        "        labels.append(d['is_sarcastic'])\n",
        "        \n",
        "    # 주어지는 값. 전체 데이터셋을 앞에서 20000개는 training으로 나머지는 test로 사용하기 위해서 나눈다\n",
        "    train_sentences = sentences[:training_size]\n",
        "    train_labels = labels[:training_size]\n",
        "    test_sentences = sentences[training_size:]\n",
        "    test_labels = labels[training_size:]\n",
        "    \n",
        "    # Tokenizer 정의\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "    \n",
        "    # 문자 데이터를 입력받아서 리스트의 형태로 변환\n",
        "    # = 공백 기준으로 토크나이징\n",
        "    # vocab 생성\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "    \n",
        "    # 텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n",
        "    # = 문장 -> 토큰 -> indexing\n",
        "    train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "    \n",
        "    # 문장 길이 맞추기 (padding)\n",
        "    x_train = pad_sequences(train_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\n",
        "    x_test = pad_sequences(test_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\n",
        "    \n",
        "    # Label 값을 Numpy Array로 변환\n",
        "    y_train = np.array(train_labels)\n",
        "    y_test = np.array(test_labels)\n",
        "    \n",
        "    # Define the model\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        Dropout(0.2),\n",
        "        BatchNormalization(),\n",
        "        Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        Dropout(0.2),\n",
        "        BatchNormalization(),\n",
        "        Bidirectional(LSTM(64)),\n",
        "        Dropout(0.2),\n",
        "        BatchNormalization(),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        BatchNormalization(),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = Adam(lr=1e-4)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Define the callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "    checkpoint_path = 'my_checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path, save_weights_only=True, save_best_only=True, monitor='val_loss', verbose=1)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7, verbose=1)\n",
        "\n",
        "    # Fit the model\n",
        "    history = model.fit(x_train, y_train, \n",
        "        validation_data=(x_test, y_test),\n",
        "        callbacks=[early_stopping, checkpoint, reduce_lr],\n",
        "        epochs=50, verbose=1)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wqvtZ78Gnng",
        "outputId": "f8ffbe71-e209-4230-dd96-9159bda8790b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.4982 - accuracy: 0.7591\n",
            "Epoch 1: val_loss improved from inf to 0.53683, saving model to my_checkpoint.ckpt\n",
            "625/625 [==============================] - 49s 58ms/step - loss: 0.4982 - accuracy: 0.7591 - val_loss: 0.5368 - val_accuracy: 0.7596 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.3854 - accuracy: 0.8242\n",
            "Epoch 2: val_loss did not improve from 0.53683\n",
            "625/625 [==============================] - 24s 38ms/step - loss: 0.3851 - accuracy: 0.8243 - val_loss: 0.7870 - val_accuracy: 0.6850 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.3527 - accuracy: 0.8453\n",
            "Epoch 3: val_loss improved from 0.53683 to 0.37531, saving model to my_checkpoint.ckpt\n",
            "625/625 [==============================] - 24s 38ms/step - loss: 0.3527 - accuracy: 0.8453 - val_loss: 0.3753 - val_accuracy: 0.8281 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.3391 - accuracy: 0.8506\n",
            "Epoch 4: val_loss did not improve from 0.37531\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 0.3395 - accuracy: 0.8504 - val_loss: 0.4312 - val_accuracy: 0.8006 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.3257 - accuracy: 0.8591\n",
            "Epoch 5: val_loss did not improve from 0.37531\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.3257 - accuracy: 0.8591 - val_loss: 0.3841 - val_accuracy: 0.8280 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.3171 - accuracy: 0.8646\n",
            "Epoch 6: val_loss did not improve from 0.37531\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 0.3171 - accuracy: 0.8646 - val_loss: 0.3985 - val_accuracy: 0.8234 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.8712\n",
            "Epoch 7: val_loss did not improve from 0.37531\n",
            "625/625 [==============================] - 23s 37ms/step - loss: 0.3024 - accuracy: 0.8712 - val_loss: 0.3859 - val_accuracy: 0.8322 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2923 - accuracy: 0.8740\n",
            "Epoch 8: val_loss did not improve from 0.37531\n",
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "625/625 [==============================] - 24s 38ms/step - loss: 0.2923 - accuracy: 0.8740 - val_loss: 0.3878 - val_accuracy: 0.8304 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.2515 - accuracy: 0.8960\n",
            "Epoch 9: val_loss did not improve from 0.37531\n",
            "625/625 [==============================] - 23s 37ms/step - loss: 0.2516 - accuracy: 0.8960 - val_loss: 0.4225 - val_accuracy: 0.8301 - lr: 2.0000e-04\n",
            "Epoch 10/50\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.2389 - accuracy: 0.9020\n",
            "Epoch 10: val_loss did not improve from 0.37531\n",
            "625/625 [==============================] - 22s 35ms/step - loss: 0.2389 - accuracy: 0.9021 - val_loss: 0.4380 - val_accuracy: 0.8293 - lr: 2.0000e-04\n",
            "Epoch 11/50\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.2294 - accuracy: 0.9061\n",
            "Epoch 11: val_loss did not improve from 0.37531\n",
            "625/625 [==============================] - 23s 37ms/step - loss: 0.2296 - accuracy: 0.9060 - val_loss: 0.4694 - val_accuracy: 0.8253 - lr: 2.0000e-04\n",
            "Epoch 12/50\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.2170 - accuracy: 0.9103\n",
            "Epoch 12: val_loss did not improve from 0.37531\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 0.2171 - accuracy: 0.9102 - val_loss: 0.4747 - val_accuracy: 0.8177 - lr: 2.0000e-04\n",
            "Epoch 13/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2077 - accuracy: 0.9161\n",
            "Epoch 13: val_loss did not improve from 0.37531\n",
            "\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.2077 - accuracy: 0.9161 - val_loss: 0.5014 - val_accuracy: 0.8174 - lr: 2.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-2. Transfer Learning - ERROR!\n",
        "방금 코드의 Bidirectional layer 대신 사전 학습된 자연어 분류 모델을 적용해 줘\n",
        "\n",
        "\n",
        "ChatGPT Answer:\n",
        "사전 학습된 자연어 분류 모델을 적용하려면 먼저 해당 모델을 로드해야 합니다. 저는 TensorFlow Hub에서 제공하는 tensorflow_hub.KerasLayer를 사용하여 모델을 로드하겠습니다.\n",
        "\n",
        "모델은 tensorflow_hub.KerasLayer를 사용하여 로드되며, 이후에는 나머지 부분을 그대로 유지하면 됩니다. 다음은 코드 예시입니다.\n",
        "\n",
        "위의 코드에서는 먼저 hub.KerasLayer를 사용하여 사전 학습된 모델을 로드하고, 이후에는 새로운 모델을 정의합니다. 이 모델은 hub.KerasLayer를 첫 번째 레이어로 사용하고, 이후에는 Dropout, BatchNormalization, Dense 레이어를 추가합니다. 최종 출력 레이어는 1차원 Dense 레이어로 이진 분류 문제를 해결합니다.\n",
        "\n",
        "모델을 컴파일하고, 콜백 함수를 설정한 후 fit 메서드를 사용하여 모델을 학습합니다."
      ],
      "metadata": {
        "id": "Oi8kChL0ESJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# NLP QUESTION\n",
        "#\n",
        "# For this task you will build a classifier for the sarcasm dataset\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown\n",
        "# It will be tested against a number of sentences that the network hasn't previously seen\n",
        "# And you will be scored on whether sarcasm was correctly detected in those sentences\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "# import libraries\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Flatten, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    padding_type='post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "    \n",
        "    sentences = []\n",
        "    labels = []\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    # json file load\n",
        "    with open('sarcasm.json') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    # Sentence 와 Label 로 나누기\n",
        "    for d in data:\n",
        "        sentences.append(d['headline'])\n",
        "        labels.append(d['is_sarcastic'])\n",
        "        \n",
        "    # 주어지는 값. 전체 데이터셋을 앞에서 20000개는 training으로 나머지는 test로 사용하기 위해서 나눈다\n",
        "    train_sentences = sentences[:training_size]\n",
        "    train_labels = labels[:training_size]\n",
        "    test_sentences = sentences[training_size:]\n",
        "    test_labels = labels[training_size:]\n",
        "    \n",
        "    # Tokenizer 정의\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "    \n",
        "    # 문자 데이터를 입력받아서 리스트의 형태로 변환\n",
        "    # = 공백 기준으로 토크나이징\n",
        "    # vocab 생성\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "    \n",
        "    # 텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n",
        "    # = 문장 -> 토큰 -> indexing\n",
        "    train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "    \n",
        "    # 문장 길이 맞추기 (padding)\n",
        "    x_train = pad_sequences(train_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\n",
        "    x_test = pad_sequences(test_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\n",
        "    \n",
        "    # Label 값을 Numpy Array로 변환\n",
        "    y_train = np.array(train_labels)\n",
        "    y_test = np.array(test_labels)\n",
        "    \n",
        "\n",
        "    # Pretrained model을 로드합니다\n",
        "    pretrained_model_url = \"https://tfhub.dev/google/nnlm-en-dim128/2\"\n",
        "    hub_layer = hub.KerasLayer(pretrained_model_url, input_shape=[], dtype=tf.string, trainable=True)\n",
        "\n",
        "   # 새로운 모델을 정의합니다\n",
        "    model = tf.keras.Sequential([\n",
        "            hub_layer,\n",
        "            tf.keras.layers.Dropout(0.5),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(16, activation='relu'),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "    \n",
        "    # optimizer, loss 정의\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # callback\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "    checkpoint_path = 'my_checkpoint.ckpt'\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
        "            save_weights_only=True, \n",
        "            save_best_only=True, \n",
        "            monitor='val_loss',\n",
        "            verbose=1)\n",
        "\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7, verbose=1)\n",
        "\n",
        "    # Fit the model\n",
        "    history = model.fit(x_train, y_train, \n",
        "        validation_data=(x_test, y_test),\n",
        "        callbacks=[early_stopping, checkpoint, reduce_lr],\n",
        "        epochs=50, verbose=1)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ShJvd61KERvJ",
        "outputId": "99f3033e-9cc9-4d30-bdf3-46a79f40f9f4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-fe4066be1239>\u001b[0m in \u001b[0;36m<cell line: 123>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;31m# and a score will be returned to you\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolution_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mymodel.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-fe4066be1239>\u001b[0m in \u001b[0;36msolution_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     history = model.fit(x_train, y_train, \n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmart_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_cond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mget_state_6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mif_body_3\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mif_body_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0;32mnonlocal\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0melse_body_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filemmx80vyq.py\", line 74, in tf__call\n        ag__.if_stmt(ag__.not_(ag__.ld(self)._has_training_argument), if_body_3, else_body_3, get_state_3, set_state_3, ('result', 'training'), 1)\n    File \"/tmp/__autograph_generated_filemmx80vyq.py\", line 37, in if_body_3\n        result = ag__.converted_call(ag__.ld(f), (), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'keras_layer' (type KerasLayer).\n    \n    in user code:\n    \n        File \"/usr/local/lib/python3.9/dist-packages/tensorflow_hub/keras_layer.py\", line 234, in call  *\n            result = f()\n    \n        ValueError: Python inputs incompatible with input_signature:\n          inputs: (\n            Tensor(\"sequential_7/Cast:0\", shape=(32, 120), dtype=string))\n          input_signature: (\n            TensorSpec(shape=(None,), dtype=tf.string, name=None)).\n    \n    \n    Call arguments received by layer 'keras_layer' (type KerasLayer):\n      • inputs=tf.Tensor(shape=(32, 120), dtype=string)\n      • training=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F6BddMbvEg4o"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}