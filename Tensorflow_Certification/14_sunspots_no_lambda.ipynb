{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Judy-Choi/Tensorflow_Certificate/blob/main/14_sunspots_no_lambda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Val_loss = 0.07\n",
        "val_acc = 0.1121"
      ],
      "metadata": {
        "id": "XqcE1J8Tc9qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.10.0"
      ],
      "metadata": {
        "id": "ego501nkVipc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 답안"
      ],
      "metadata": {
        "id": "di-Pa417rfxS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3_9wrbxeB9J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a2f17dd-9112-4769-c4ba-9796a06de35b"
      },
      "source": [
        "# QUESTION\n",
        "#\n",
        "# For this task you will need to train a neural network\n",
        "# to predict sunspot activity using the Sunspots.csv dataset.\n",
        "# Your neural network must  have an MAE\n",
        "# of 0.12 or less on the normalized dataset for top marks.\n",
        "# Code for normalizing the data is provided and should not be changed.\n",
        "# At the bottom of this file, we provide  some testing\n",
        "# code should you want to check your model.\n",
        "\n",
        "# Note: Do not use lambda layers in your model, they are not supported\n",
        "# on the grading infrastructure.\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.layers import Dense, LSTM, Lambda, Conv1D, Dropout, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.losses import Huber\n",
        "\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[-1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n",
        "    urllib.request.urlretrieve(url, 'sunspots.csv')\n",
        "\n",
        "    time_step = []\n",
        "    sunspots = []\n",
        "\n",
        "    with open('sunspots.csv') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        next(reader)\n",
        "        for row in reader:\n",
        "            sunspots.append(float(row[2])) # YOUR CODE HERE)\n",
        "            time_step.append(int(row[0]))  # YOUR CODE HERE)\n",
        "\n",
        "    series = np.array(sunspots) # YOUR CODE HERE\n",
        "    time = np.array(time_step)\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # This is the normalization function\n",
        "    min = np.min(series)\n",
        "    max = np.max(series)\n",
        "    series -= min\n",
        "    series /= max\n",
        "    time = np.array(time_step)\n",
        "\n",
        "    # The data should be split into training and validation sets at time step 3000\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    split_time = 3000\n",
        "\n",
        "\n",
        "    time_train = time[:split_time]\n",
        "    x_train = series[:split_time]\n",
        "    time_valid = time[split_time:]\n",
        "    x_valid = series[split_time:]\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    window_size = 30\n",
        "    batch_size = 32\n",
        "    shuffle_buffer_size = 1000\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(53)\n",
        "    np.random.seed(53)\n",
        "\n",
        "    train_set = windowed_dataset(x_train, \n",
        "        window_size=window_size, \n",
        "        batch_size=batch_size, \n",
        "        shuffle_buffer=shuffle_buffer_size)\n",
        "    \n",
        "    validation_set = windowed_dataset(x_valid, \n",
        "        window_size=window_size,\n",
        "        batch_size=batch_size,\n",
        "        shuffle_buffer=shuffle_buffer_size)\n",
        "    \n",
        "    steps_per_epoch = print(len(list(train_set.as_numpy_iterator())))\n",
        "    validation_steps = print(len(list(validation_set.as_numpy_iterator())))\n",
        "\n",
        "    model = Sequential([\n",
        "        tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n",
        "        padding=\"causal\",\n",
        "        activation=\"relu\",\n",
        "        input_shape=[None, 1]),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation=\"relu\"),\n",
        "        Dense(64, activation=\"relu\"),          \n",
        "        \n",
        "                                              \n",
        "      # YOUR CODE HERE. Whatever your first layer is, the input shape will be [None,1] when using the Windowed_dataset above, depending on the layer type chosen\n",
        "      tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    # PLEASE NOTE IF YOU SEE THIS TEXT WHILE TRAINING -- IT IS SAFE TO IGNORE\n",
        "    # BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
        "    # \t [[{{node IteratorGetNext}}]]\n",
        "    #\n",
        "\n",
        "\n",
        "    # YOUR CODE HERE TO COMPILE AND TRAIN THE MODEL\n",
        " #   optimizer = SGD(learning_rate=1e-4, momentum=0.9)\n",
        "    loss= Huber()\n",
        "    model.compile(loss=loss,\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"mae\"])\n",
        "    checkpoint_path = 'checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path, \n",
        "        save_weights_only=True, \n",
        "        save_best_only=True, \n",
        "        monitor='val_mae',\n",
        "        verbose=1)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "    epochs=100\n",
        "    history = model.fit(train_set, \n",
        "        validation_data=(validation_set), \n",
        "        epochs=epochs, \n",
        "        callbacks=[checkpoint, early_stopping],\n",
        "        )\n",
        "    model.load_weights(checkpoint_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, this .h5 model will be\n",
        "# sent to the testing infrastructure for scoring.\n",
        "\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in each category before you finally submit your exam.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"tf5-sunspot-3(no-lambda).h5\")\n",
        "\n",
        "# 시험볼때는 이줄 아래는 복사하지 마세요\n",
        "    # from google.colab import files\n",
        "    # files.download('tf5-sunspot-3(no-lambda).h5')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93\n",
            "7\n",
            "Epoch 1/100\n",
            "     93/Unknown - 16s 69ms/step - loss: 0.0080 - mae: 0.0929\n",
            "Epoch 1: val_mae improved from inf to 0.05265, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 18s 94ms/step - loss: 0.0080 - mae: 0.0929 - val_loss: 0.0022 - val_mae: 0.0527\n",
            "Epoch 2/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0033 - mae: 0.0594\n",
            "Epoch 2: val_mae improved from 0.05265 to 0.03893, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 7s 69ms/step - loss: 0.0033 - mae: 0.0594 - val_loss: 0.0015 - val_mae: 0.0389\n",
            "Epoch 3/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0029 - mae: 0.0562\n",
            "Epoch 3: val_mae did not improve from 0.03893\n",
            "93/93 [==============================] - 7s 69ms/step - loss: 0.0029 - mae: 0.0562 - val_loss: 0.0016 - val_mae: 0.0397\n",
            "Epoch 4/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0030 - mae: 0.0570\n",
            "Epoch 4: val_mae improved from 0.03893 to 0.03817, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 7s 77ms/step - loss: 0.0030 - mae: 0.0570 - val_loss: 0.0015 - val_mae: 0.0382\n",
            "Epoch 5/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0029 - mae: 0.0554\n",
            "Epoch 5: val_mae did not improve from 0.03817\n",
            "93/93 [==============================] - 7s 73ms/step - loss: 0.0029 - mae: 0.0554 - val_loss: 0.0015 - val_mae: 0.0383\n",
            "Epoch 6/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0028 - mae: 0.0548\n",
            "Epoch 6: val_mae did not improve from 0.03817\n",
            "93/93 [==============================] - 7s 78ms/step - loss: 0.0028 - mae: 0.0548 - val_loss: 0.0015 - val_mae: 0.0392\n",
            "Epoch 7/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0027 - mae: 0.0539\n",
            "Epoch 7: val_mae did not improve from 0.03817\n",
            "93/93 [==============================] - 7s 78ms/step - loss: 0.0027 - mae: 0.0539 - val_loss: 0.0019 - val_mae: 0.0428\n",
            "Epoch 8/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0534\n",
            "Epoch 8: val_mae did not improve from 0.03817\n",
            "93/93 [==============================] - 7s 78ms/step - loss: 0.0026 - mae: 0.0534 - val_loss: 0.0017 - val_mae: 0.0409\n",
            "Epoch 9/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0532\n",
            "Epoch 9: val_mae did not improve from 0.03817\n",
            "93/93 [==============================] - 8s 82ms/step - loss: 0.0026 - mae: 0.0532 - val_loss: 0.0020 - val_mae: 0.0430\n",
            "Epoch 10/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0525\n",
            "Epoch 10: val_mae did not improve from 0.03817\n",
            "93/93 [==============================] - 9s 95ms/step - loss: 0.0026 - mae: 0.0525 - val_loss: 0.0016 - val_mae: 0.0417\n",
            "Epoch 11/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0024 - mae: 0.0509\n",
            "Epoch 11: val_mae improved from 0.03817 to 0.03816, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 8s 81ms/step - loss: 0.0024 - mae: 0.0509 - val_loss: 0.0014 - val_mae: 0.0382\n",
            "Epoch 12/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0024 - mae: 0.0504\n",
            "Epoch 12: val_mae did not improve from 0.03816\n",
            "93/93 [==============================] - 8s 83ms/step - loss: 0.0024 - mae: 0.0504 - val_loss: 0.0018 - val_mae: 0.0437\n",
            "Epoch 13/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0527\n",
            "Epoch 13: val_mae did not improve from 0.03816\n",
            "93/93 [==============================] - 8s 83ms/step - loss: 0.0026 - mae: 0.0527 - val_loss: 0.0023 - val_mae: 0.0441\n",
            "Epoch 14/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0528\n",
            "Epoch 14: val_mae did not improve from 0.03816\n",
            "93/93 [==============================] - 8s 83ms/step - loss: 0.0026 - mae: 0.0528 - val_loss: 0.0018 - val_mae: 0.0407\n",
            "Epoch 15/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0024 - mae: 0.0505\n",
            "Epoch 15: val_mae did not improve from 0.03816\n",
            "93/93 [==============================] - 8s 86ms/step - loss: 0.0024 - mae: 0.0505 - val_loss: 0.0022 - val_mae: 0.0441\n",
            "Epoch 16/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0024 - mae: 0.0506\n",
            "Epoch 16: val_mae did not improve from 0.03816\n",
            "93/93 [==============================] - 8s 82ms/step - loss: 0.0024 - mae: 0.0506 - val_loss: 0.0017 - val_mae: 0.0442\n",
            "Epoch 17/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0025 - mae: 0.0519\n",
            "Epoch 17: val_mae did not improve from 0.03816\n",
            "93/93 [==============================] - 8s 80ms/step - loss: 0.0025 - mae: 0.0519 - val_loss: 0.0016 - val_mae: 0.0383\n",
            "Epoch 18/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0025 - mae: 0.0520\n",
            "Epoch 18: val_mae did not improve from 0.03816\n",
            "93/93 [==============================] - 8s 80ms/step - loss: 0.0025 - mae: 0.0520 - val_loss: 0.0018 - val_mae: 0.0404\n",
            "Epoch 19/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0023 - mae: 0.0499\n",
            "Epoch 19: val_mae did not improve from 0.03816\n",
            "93/93 [==============================] - 8s 81ms/step - loss: 0.0023 - mae: 0.0499 - val_loss: 0.0022 - val_mae: 0.0433\n",
            "Epoch 20/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0025 - mae: 0.0521\n",
            "Epoch 20: val_mae did not improve from 0.03816\n",
            "93/93 [==============================] - 8s 85ms/step - loss: 0.0025 - mae: 0.0521 - val_loss: 0.0020 - val_mae: 0.0426\n",
            "Epoch 21/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0025 - mae: 0.0509\n",
            "Epoch 21: val_mae did not improve from 0.03816\n",
            "93/93 [==============================] - 8s 84ms/step - loss: 0.0025 - mae: 0.0509 - val_loss: 0.0018 - val_mae: 0.0407\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9b6074ee-36a0-48fa-90f6-54a59ccbe12d\", \"tf5-sunspot-3(no-lambda).h5\", 1099280)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT\n",
        "\n",
        "Q) 다음 코드의 성능을 개선해 줘.\n",
        "참고로 사용하는 데이터는 시계열 데이터야.\n",
        "\n",
        "A) Batch normalization\n",
        "You can try adding batch normalization after the first convolutional layer and after each LSTM layer. This can improve the convergence of the model and lead to better performance.\n",
        "\n",
        "Reduce the number of LSTM layers\n",
        "Having two stacked Bidirectional LSTM layers can be computationally expensive, especially for long sequences. You can try reducing the number of LSTM layers to one and increase the number of filters in the convolutional layer instead.\n",
        "\n",
        "Increase the number of filters in the convolutional layer\n",
        "Increasing the number of filters in the convolutional layer can help capture more relevant features from the input sequence. You can try increasing the number of filters from 16 to a larger number.\n",
        "\n",
        "Add an additional dense layer\n",
        "You can try adding an additional dense layer before the output layer to allow for more complex interactions between the features extracted from the input sequence.\n",
        "\n",
        "\n",
        "Q) 사용하는 데이터가 float 자료형 시계열 데이터임을 감안해서 다시 코드를 개선해 줘\n",
        "\n",
        "A) 각 레이어의 유닛 수와 하이퍼파라미터는 데이터에 따라 최적화되어야 하므로 이전에 제안한 개선 사항은 유일한 해결책이 아닙니다. 하지만 데이터가 float 자료형 시계열 데이터인 경우, 이전에 제안한 모델을 사용하되 float32 자료형을 사용하도록 수정할 수 있습니다. 이렇게 하면 모델의 속도를 높일 수 있습니다.\n",
        "\n",
        "모든 레이어에서 dtype 매개 변수를 \"float32\"로 설정하고, 모델 컴파일 단계에서 \"float32\"로 데이터 유형을 지정할 수 있습니다."
      ],
      "metadata": {
        "id": "VAFfZOXBrjxt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWKj-mdJXtKz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0239fab-061c-4b1b-c449-4ebd7f3220f9"
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# Sequence Modelling Question.\n",
        "#\n",
        "# For this task you will need to train a neural network\n",
        "# to predict sunspot activity using the Sunspots.csv\n",
        "# provided. Your neural network is expected to have an MAE\n",
        "# of at least 20, with top marks going to one with an MAE\n",
        "# of around 15. At the bottom is provided some testing\n",
        "# code should you want to check before uploading which measures\n",
        "# the MAE for you. Strongly recommend you test your model with\n",
        "# this to be able to see how it performs.\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.layers import Dense, LSTM, Lambda, Conv1D,Dropout, BatchNormalization, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import Huber\n",
        "\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    # Window Sequence Loader 만들기\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    # Dataset loader 만들기\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n",
        "    urllib.request.urlretrieve(url, 'sunspots.csv')\n",
        "\n",
        "    # Your data should be loaded into 2 Python lists called time_step\n",
        "    # and sunspots. They are decleared here.\n",
        "    time_step = []\n",
        "    sunspots = []\n",
        "\n",
        "    with open('sunspots.csv') as csvfile:\n",
        "      reader = csv.reader(csvfile, delimiter=',')\n",
        "      next(reader)\n",
        "      for row in reader:\n",
        "        sunspots.append(float(row[2]))# YOUR CODE HERE\n",
        "        time_step.append(int(row[0]))\n",
        "\n",
        "    \n",
        "    # You should use numpy to create \n",
        "    # - your series from the list of sunspots\n",
        "    # - your time details from the list of time steps\n",
        "    series = np.array(sunspots) # YOUR CODE HERE\n",
        "    time = np.array(time_step)\n",
        "\n",
        "    # You should split the dataset into training and validation splits\n",
        "    # At time 3000. So everything up to 3000 is training, and everything\n",
        "    # after 3000 is validation. Write the code below to achieve that.\n",
        "    split_time = 3000\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    time_train = time[:split_time]\n",
        "    x_train = series[:split_time]\n",
        "    time_valid = time[split_time:]\n",
        "    x_valid = series[split_time:]\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    window_size = 30\n",
        "    batch_size = 32\n",
        "    shuffle_buffer_size = 1000\n",
        "\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    # You can use any random seed you want. We use 53. :)\n",
        "    tf.random.set_seed(53)\n",
        "    np.random.seed(53)\n",
        "    train_set = windowed_dataset(x_train, window_size=window_size, batch_size=batch_size, shuffle_buffer=shuffle_buffer_size)\n",
        "    \n",
        "    validation_set = windowed_dataset(x_valid, \n",
        "        window_size=window_size,\n",
        "        batch_size=batch_size,\n",
        "        shuffle_buffer=shuffle_buffer_size)\n",
        "\n",
        "\n",
        "    model = Sequential([\n",
        "        Conv1D(filters=64, kernel_size=5,\n",
        "        padding=\"causal\",\n",
        "        activation=\"relu\",\n",
        "        input_shape=[None, 1],\n",
        "        dtype=tf.float32),\n",
        "        BatchNormalization(dtype=tf.float32),\n",
        "        Dropout(0.5, dtype=tf.float32),\n",
        "        LSTM(64, return_sequences=True, dtype=tf.float32),\n",
        "        BatchNormalization(dtype=tf.float32),\n",
        "        Dropout(0.5, dtype=tf.float32),\n",
        "        Dense(128, activation=\"relu\", dtype=tf.float32),\n",
        "        LSTM(32, dtype=tf.float32),\n",
        "        Dense(64, activation=\"relu\", dtype=tf.float32),\n",
        "        BatchNormalization(dtype=tf.float32),\n",
        "        Dense(32, activation=\"relu\", dtype=tf.float32),    \n",
        "        Dense(1, dtype=tf.float32)\n",
        "    ])\n",
        "\n",
        "   \n",
        "    # YOUR CODE HERE TO COMPILE AND TRAIN THE MODEL\n",
        "    optimizer = Adam(learning_rate=1e-4)\n",
        "    loss= Huber()\n",
        "    model.compile(loss=loss,\n",
        "        optimizer=optimizer,\n",
        "        metrics=[\"mae\"])\n",
        "\n",
        "    checkpoint_path = 'checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path, \n",
        "        save_weights_only=True, \n",
        "        save_best_only=True, \n",
        "        monitor='val_mae',\n",
        "        verbose=1)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
        "\n",
        "    epochs=100\n",
        "\n",
        "    history = model.fit(train_set, \n",
        "        validation_data=validation_set, \n",
        "        epochs=epochs, \n",
        "        callbacks=[checkpoint, early_stopping, reduce_lr],\n",
        "        )\n",
        "\n",
        "    model.load_weights(checkpoint_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"tf5-sunspot-3.h5\")\n",
        "\n",
        "# 시험때 다음 줄은 복사하지 마세요\n",
        "    # from google.colab import files\n",
        "    # files.download('tf5-sunspot-3.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "     92/Unknown - 19s 69ms/step - loss: 81.6682 - mae: 82.1634\n",
            "Epoch 1: val_mae improved from inf to 70.96032, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 20s 81ms/step - loss: 81.4604 - mae: 81.9556 - val_loss: 70.4679 - val_mae: 70.9603 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 78.9358 - mae: 79.4330\n",
            "Epoch 2: val_mae improved from 70.96032 to 68.90915, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 34ms/step - loss: 78.9358 - mae: 79.4330 - val_loss: 68.4104 - val_mae: 68.9091 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 76.6775 - mae: 77.1745\n",
            "Epoch 3: val_mae improved from 68.90915 to 65.89726, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 4s 41ms/step - loss: 76.6775 - mae: 77.1745 - val_loss: 65.4019 - val_mae: 65.8973 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 74.6843 - mae: 75.1814\n",
            "Epoch 4: val_mae improved from 65.89726 to 63.02328, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 34ms/step - loss: 74.6843 - mae: 75.1814 - val_loss: 62.5302 - val_mae: 63.0233 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 72.6549 - mae: 73.1514\n",
            "Epoch 5: val_mae improved from 63.02328 to 60.55497, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 35ms/step - loss: 72.6549 - mae: 73.1514 - val_loss: 60.0553 - val_mae: 60.5550 - lr: 1.0000e-04\n",
            "Epoch 6/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 70.4358 - mae: 70.9326\n",
            "Epoch 6: val_mae improved from 60.55497 to 58.25653, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 4s 42ms/step - loss: 70.4594 - mae: 70.9562 - val_loss: 57.7593 - val_mae: 58.2565 - lr: 1.0000e-04\n",
            "Epoch 7/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 68.1737 - mae: 68.6706\n",
            "Epoch 7: val_mae improved from 58.25653 to 55.75073, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 4s 37ms/step - loss: 68.1737 - mae: 68.6706 - val_loss: 55.2521 - val_mae: 55.7507 - lr: 1.0000e-04\n",
            "Epoch 8/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 65.6554 - mae: 66.1521\n",
            "Epoch 8: val_mae improved from 55.75073 to 53.48215, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 35ms/step - loss: 65.6554 - mae: 66.1521 - val_loss: 52.9838 - val_mae: 53.4821 - lr: 1.0000e-04\n",
            "Epoch 9/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 62.9484 - mae: 63.4447\n",
            "Epoch 9: val_mae improved from 53.48215 to 49.84710, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 4s 40ms/step - loss: 62.9484 - mae: 63.4447 - val_loss: 49.3498 - val_mae: 49.8471 - lr: 1.0000e-04\n",
            "Epoch 10/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 60.0496 - mae: 60.5458\n",
            "Epoch 10: val_mae improved from 49.84710 to 46.16639, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 35ms/step - loss: 60.0496 - mae: 60.5458 - val_loss: 45.6694 - val_mae: 46.1664 - lr: 1.0000e-04\n",
            "Epoch 11/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 57.1193 - mae: 57.6155\n",
            "Epoch 11: val_mae improved from 46.16639 to 43.51436, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 34ms/step - loss: 57.1193 - mae: 57.6155 - val_loss: 43.0177 - val_mae: 43.5144 - lr: 1.0000e-04\n",
            "Epoch 12/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 53.8629 - mae: 54.3588\n",
            "Epoch 12: val_mae improved from 43.51436 to 40.41602, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 4s 41ms/step - loss: 53.9399 - mae: 54.4358 - val_loss: 39.9196 - val_mae: 40.4160 - lr: 1.0000e-04\n",
            "Epoch 13/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 50.8265 - mae: 51.3226\n",
            "Epoch 13: val_mae improved from 40.41602 to 38.57242, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 35ms/step - loss: 50.9216 - mae: 51.4178 - val_loss: 38.0761 - val_mae: 38.5724 - lr: 1.0000e-04\n",
            "Epoch 14/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 47.9560 - mae: 48.4521\n",
            "Epoch 14: val_mae improved from 38.57242 to 37.52361, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 36ms/step - loss: 47.9560 - mae: 48.4521 - val_loss: 37.0270 - val_mae: 37.5236 - lr: 1.0000e-04\n",
            "Epoch 15/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 44.8853 - mae: 45.3809\n",
            "Epoch 15: val_mae improved from 37.52361 to 33.16261, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 4s 37ms/step - loss: 45.0258 - mae: 45.5215 - val_loss: 32.6670 - val_mae: 33.1626 - lr: 1.0000e-04\n",
            "Epoch 16/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 42.1707 - mae: 42.6664\n",
            "Epoch 16: val_mae improved from 33.16261 to 30.73735, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 35ms/step - loss: 42.1707 - mae: 42.6664 - val_loss: 30.2416 - val_mae: 30.7374 - lr: 1.0000e-04\n",
            "Epoch 17/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 39.5860 - mae: 40.0819\n",
            "Epoch 17: val_mae improved from 30.73735 to 28.02027, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 5s 48ms/step - loss: 39.5860 - mae: 40.0819 - val_loss: 27.5248 - val_mae: 28.0203 - lr: 1.0000e-04\n",
            "Epoch 18/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 37.2135 - mae: 37.7092\n",
            "Epoch 18: val_mae improved from 28.02027 to 25.29490, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 36ms/step - loss: 37.2135 - mae: 37.7092 - val_loss: 24.7996 - val_mae: 25.2949 - lr: 1.0000e-04\n",
            "Epoch 19/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 35.3894 - mae: 35.8851\n",
            "Epoch 19: val_mae improved from 25.29490 to 23.58717, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 35ms/step - loss: 35.3894 - mae: 35.8851 - val_loss: 23.0913 - val_mae: 23.5872 - lr: 1.0000e-04\n",
            "Epoch 20/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 33.1461 - mae: 33.6416\n",
            "Epoch 20: val_mae improved from 23.58717 to 22.71440, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 4s 41ms/step - loss: 33.2252 - mae: 33.7208 - val_loss: 22.2181 - val_mae: 22.7144 - lr: 1.0000e-04\n",
            "Epoch 21/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 32.0766 - mae: 32.5719\n",
            "Epoch 21: val_mae improved from 22.71440 to 22.08304, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 35ms/step - loss: 32.0766 - mae: 32.5719 - val_loss: 21.5881 - val_mae: 22.0830 - lr: 1.0000e-04\n",
            "Epoch 22/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 30.6949 - mae: 31.1901\n",
            "Epoch 22: val_mae improved from 22.08304 to 21.76277, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 34ms/step - loss: 30.7970 - mae: 31.2922 - val_loss: 21.2703 - val_mae: 21.7628 - lr: 1.0000e-04\n",
            "Epoch 23/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 30.1268 - mae: 30.6222\n",
            "Epoch 23: val_mae did not improve from 21.76277\n",
            "93/93 [==============================] - 4s 39ms/step - loss: 30.1268 - mae: 30.6222 - val_loss: 21.3670 - val_mae: 21.8606 - lr: 1.0000e-04\n",
            "Epoch 24/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 29.4876 - mae: 29.9829\n",
            "Epoch 24: val_mae improved from 21.76277 to 21.63791, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 34ms/step - loss: 29.5195 - mae: 30.0148 - val_loss: 21.1457 - val_mae: 21.6379 - lr: 1.0000e-04\n",
            "Epoch 25/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 29.4166 - mae: 29.9118\n",
            "Epoch 25: val_mae did not improve from 21.63791\n",
            "93/93 [==============================] - 3s 35ms/step - loss: 29.4116 - mae: 29.9068 - val_loss: 21.1578 - val_mae: 21.6529 - lr: 1.0000e-04\n",
            "Epoch 26/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 28.9629 - mae: 29.4580\n",
            "Epoch 26: val_mae did not improve from 21.63791\n",
            "93/93 [==============================] - 4s 41ms/step - loss: 28.9971 - mae: 29.4923 - val_loss: 21.4267 - val_mae: 21.9207 - lr: 1.0000e-04\n",
            "Epoch 27/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.5463 - mae: 29.0414\n",
            "Epoch 27: val_mae improved from 21.63791 to 21.18381, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 34ms/step - loss: 28.5463 - mae: 29.0414 - val_loss: 20.6902 - val_mae: 21.1838 - lr: 1.0000e-04\n",
            "Epoch 28/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.4345 - mae: 28.9293\n",
            "Epoch 28: val_mae did not improve from 21.18381\n",
            "93/93 [==============================] - 3s 35ms/step - loss: 28.4345 - mae: 28.9293 - val_loss: 21.4371 - val_mae: 21.9316 - lr: 1.0000e-04\n",
            "Epoch 29/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 28.5073 - mae: 29.0024\n",
            "Epoch 29: val_mae improved from 21.18381 to 20.95937, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 4s 41ms/step - loss: 28.5932 - mae: 29.0883 - val_loss: 20.4660 - val_mae: 20.9594 - lr: 1.0000e-04\n",
            "Epoch 30/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.1993 - mae: 28.6944\n",
            "Epoch 30: val_mae did not improve from 20.95937\n",
            "93/93 [==============================] - 3s 36ms/step - loss: 28.1993 - mae: 28.6944 - val_loss: 22.6067 - val_mae: 23.1017 - lr: 1.0000e-04\n",
            "Epoch 31/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.5410 - mae: 29.0360\n",
            "Epoch 31: val_mae did not improve from 20.95937\n",
            "93/93 [==============================] - 3s 34ms/step - loss: 28.5410 - mae: 29.0360 - val_loss: 20.8676 - val_mae: 21.3618 - lr: 1.0000e-04\n",
            "Epoch 32/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 28.2714 - mae: 28.7657\n",
            "Epoch 32: val_mae did not improve from 20.95937\n",
            "93/93 [==============================] - 4s 38ms/step - loss: 28.2765 - mae: 28.7708 - val_loss: 20.9413 - val_mae: 21.4358 - lr: 1.0000e-04\n",
            "Epoch 33/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.7865 - mae: 28.2812\n",
            "Epoch 33: val_mae did not improve from 20.95937\n",
            "93/93 [==============================] - 3s 34ms/step - loss: 27.7865 - mae: 28.2812 - val_loss: 20.8861 - val_mae: 21.3804 - lr: 1.0000e-04\n",
            "Epoch 34/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.8557 - mae: 29.3507\n",
            "Epoch 34: val_mae improved from 20.95937 to 20.76196, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 34ms/step - loss: 28.8557 - mae: 29.3507 - val_loss: 20.2679 - val_mae: 20.7620 - lr: 1.0000e-04\n",
            "Epoch 35/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.3622 - mae: 28.8571\n",
            "Epoch 35: val_mae did not improve from 20.76196\n",
            "93/93 [==============================] - 3s 36ms/step - loss: 28.3622 - mae: 28.8571 - val_loss: 20.3487 - val_mae: 20.8419 - lr: 1.0000e-04\n",
            "Epoch 36/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.9313 - mae: 28.4263\n",
            "Epoch 36: val_mae did not improve from 20.76196\n",
            "93/93 [==============================] - 3s 36ms/step - loss: 27.9097 - mae: 28.4048 - val_loss: 21.1783 - val_mae: 21.6719 - lr: 1.0000e-04\n",
            "Epoch 37/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 28.0930 - mae: 28.5879\n",
            "Epoch 37: val_mae did not improve from 20.76196\n",
            "93/93 [==============================] - 4s 42ms/step - loss: 28.0941 - mae: 28.5890 - val_loss: 20.5316 - val_mae: 21.0253 - lr: 1.0000e-04\n",
            "Epoch 38/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.9672 - mae: 28.4624\n",
            "Epoch 38: val_mae improved from 20.76196 to 20.74663, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 35ms/step - loss: 27.9672 - mae: 28.4624 - val_loss: 20.2530 - val_mae: 20.7466 - lr: 1.0000e-04\n",
            "Epoch 39/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.8215 - mae: 28.3161\n",
            "Epoch 39: val_mae did not improve from 20.74663\n",
            "93/93 [==============================] - 4s 40ms/step - loss: 27.8494 - mae: 28.3441 - val_loss: 20.5396 - val_mae: 21.0340 - lr: 1.0000e-04\n",
            "Epoch 40/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.4288 - mae: 28.9238\n",
            "Epoch 40: val_mae improved from 20.74663 to 20.74096, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 35ms/step - loss: 28.4288 - mae: 28.9238 - val_loss: 20.2454 - val_mae: 20.7410 - lr: 1.0000e-04\n",
            "Epoch 41/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.1602 - mae: 28.6547\n",
            "Epoch 41: val_mae did not improve from 20.74096\n",
            "93/93 [==============================] - 3s 34ms/step - loss: 28.1602 - mae: 28.6547 - val_loss: 20.3335 - val_mae: 20.8280 - lr: 1.0000e-04\n",
            "Epoch 42/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.2846 - mae: 28.7794\n",
            "Epoch 42: val_mae improved from 20.74096 to 20.44577, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 36ms/step - loss: 28.2846 - mae: 28.7794 - val_loss: 19.9512 - val_mae: 20.4458 - lr: 1.0000e-04\n",
            "Epoch 43/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.1875 - mae: 28.6822\n",
            "Epoch 43: val_mae did not improve from 20.44577\n",
            "93/93 [==============================] - 3s 34ms/step - loss: 28.1875 - mae: 28.6822 - val_loss: 20.1662 - val_mae: 20.6623 - lr: 1.0000e-04\n",
            "Epoch 44/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.9365 - mae: 28.4315\n",
            "Epoch 44: val_mae did not improve from 20.44577\n",
            "93/93 [==============================] - 4s 38ms/step - loss: 27.9576 - mae: 28.4526 - val_loss: 20.3078 - val_mae: 20.8026 - lr: 1.0000e-04\n",
            "Epoch 45/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.4677 - mae: 28.9626\n",
            "Epoch 45: val_mae improved from 20.44577 to 20.20718, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 35ms/step - loss: 28.4677 - mae: 28.9626 - val_loss: 19.7127 - val_mae: 20.2072 - lr: 1.0000e-04\n",
            "Epoch 46/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 28.2088 - mae: 28.7033\n",
            "Epoch 46: val_mae did not improve from 20.20718\n",
            "93/93 [==============================] - 4s 39ms/step - loss: 28.2008 - mae: 28.6953 - val_loss: 20.4199 - val_mae: 20.9143 - lr: 1.0000e-04\n",
            "Epoch 47/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.6504 - mae: 29.1452\n",
            "Epoch 47: val_mae did not improve from 20.20718\n",
            "93/93 [==============================] - 3s 34ms/step - loss: 28.6504 - mae: 29.1452 - val_loss: 20.3342 - val_mae: 20.8280 - lr: 1.0000e-04\n",
            "Epoch 48/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.8044 - mae: 28.2995\n",
            "Epoch 48: val_mae did not improve from 20.20718\n",
            "93/93 [==============================] - 3s 34ms/step - loss: 27.8044 - mae: 28.2995 - val_loss: 20.1093 - val_mae: 20.6031 - lr: 1.0000e-04\n",
            "Epoch 49/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.3394 - mae: 28.8339\n",
            "Epoch 49: val_mae did not improve from 20.20718\n",
            "93/93 [==============================] - 3s 34ms/step - loss: 28.3394 - mae: 28.8339 - val_loss: 19.7650 - val_mae: 20.2593 - lr: 1.0000e-04\n",
            "Epoch 50/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.2532 - mae: 28.7483\n",
            "Epoch 50: val_mae did not improve from 20.20718\n",
            "93/93 [==============================] - 3s 34ms/step - loss: 28.2532 - mae: 28.7483 - val_loss: 19.7176 - val_mae: 20.2123 - lr: 1.0000e-04\n",
            "Epoch 51/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.0394 - mae: 28.5345\n",
            "Epoch 51: val_mae did not improve from 20.20718\n",
            "93/93 [==============================] - 4s 42ms/step - loss: 28.0394 - mae: 28.5345 - val_loss: 20.0135 - val_mae: 20.5062 - lr: 2.0000e-05\n",
            "Epoch 52/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.6522 - mae: 28.1469\n",
            "Epoch 52: val_mae improved from 20.20718 to 20.19846, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 35ms/step - loss: 27.6522 - mae: 28.1469 - val_loss: 19.7059 - val_mae: 20.1985 - lr: 2.0000e-05\n",
            "Epoch 53/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.1889 - mae: 28.6837\n",
            "Epoch 53: val_mae improved from 20.19846 to 20.15272, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 36ms/step - loss: 28.1889 - mae: 28.6837 - val_loss: 19.6602 - val_mae: 20.1527 - lr: 2.0000e-05\n",
            "Epoch 54/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.5953 - mae: 28.0897\n",
            "Epoch 54: val_mae did not improve from 20.15272\n",
            "93/93 [==============================] - 3s 36ms/step - loss: 27.7267 - mae: 28.2211 - val_loss: 19.6804 - val_mae: 20.1726 - lr: 2.0000e-05\n",
            "Epoch 55/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.7157 - mae: 28.2105\n",
            "Epoch 55: val_mae did not improve from 20.15272\n",
            "93/93 [==============================] - 3s 35ms/step - loss: 27.7234 - mae: 28.2182 - val_loss: 19.6861 - val_mae: 20.1787 - lr: 2.0000e-05\n",
            "Epoch 56/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.7188 - mae: 28.2133\n",
            "Epoch 56: val_mae did not improve from 20.15272\n",
            "93/93 [==============================] - 4s 39ms/step - loss: 27.7545 - mae: 28.2490 - val_loss: 20.1325 - val_mae: 20.6252 - lr: 2.0000e-05\n",
            "Epoch 57/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.9788 - mae: 28.4732\n",
            "Epoch 57: val_mae did not improve from 20.15272\n",
            "93/93 [==============================] - 4s 36ms/step - loss: 27.9788 - mae: 28.4732 - val_loss: 19.7008 - val_mae: 20.1935 - lr: 2.0000e-05\n",
            "Epoch 58/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.7238 - mae: 28.2188\n",
            "Epoch 58: val_mae did not improve from 20.15272\n",
            "93/93 [==============================] - 3s 35ms/step - loss: 27.7238 - mae: 28.2188 - val_loss: 19.8278 - val_mae: 20.3203 - lr: 2.0000e-05\n",
            "Epoch 59/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.0632 - mae: 28.5580\n",
            "Epoch 59: val_mae did not improve from 20.15272\n",
            "93/93 [==============================] - 4s 42ms/step - loss: 28.0632 - mae: 28.5580 - val_loss: 20.0807 - val_mae: 20.5741 - lr: 4.0000e-06\n",
            "Epoch 60/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.7228 - mae: 28.2173\n",
            "Epoch 60: val_mae did not improve from 20.15272\n",
            "93/93 [==============================] - 3s 35ms/step - loss: 27.7228 - mae: 28.2173 - val_loss: 19.7723 - val_mae: 20.2660 - lr: 4.0000e-06\n",
            "Epoch 61/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.1658 - mae: 28.6607\n",
            "Epoch 61: val_mae did not improve from 20.15272\n",
            "93/93 [==============================] - 3s 35ms/step - loss: 28.1658 - mae: 28.6607 - val_loss: 19.7445 - val_mae: 20.2380 - lr: 4.0000e-06\n",
            "Epoch 62/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.5446 - mae: 28.0393\n",
            "Epoch 62: val_mae did not improve from 20.15272\n",
            "93/93 [==============================] - 3s 35ms/step - loss: 27.5446 - mae: 28.0393 - val_loss: 19.7702 - val_mae: 20.2636 - lr: 4.0000e-06\n",
            "Epoch 63/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.7960 - mae: 28.2904\n",
            "Epoch 63: val_mae did not improve from 20.15272\n",
            "93/93 [==============================] - 3s 34ms/step - loss: 27.7960 - mae: 28.2904 - val_loss: 19.8388 - val_mae: 20.3315 - lr: 4.0000e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 직접 코딩\n",
        "\n",
        "과제 순서를 바꿔서, ChatGPT 가 짜 준 코드를 직접 개선해 본다"
      ],
      "metadata": {
        "id": "tvSKtG4JsLuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# Sequence Modelling Question.\n",
        "#\n",
        "# For this task you will need to train a neural network\n",
        "# to predict sunspot activity using the Sunspots.csv\n",
        "# provided. Your neural network is expected to have an MAE\n",
        "# of at least 20, with top marks going to one with an MAE\n",
        "# of around 15. At the bottom is provided some testing\n",
        "# code should you want to check before uploading which measures\n",
        "# the MAE for you. Strongly recommend you test your model with\n",
        "# this to be able to see how it performs.\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.layers import Dense, LSTM, Lambda, Conv1D,Dropout, BatchNormalization, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.losses import Huber\n",
        "\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    # Window Sequence Loader 만들기\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    # Dataset loader 만들기\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n",
        "    urllib.request.urlretrieve(url, 'sunspots.csv')\n",
        "\n",
        "    # Your data should be loaded into 2 Python lists called time_step\n",
        "    # and sunspots. They are decleared here.\n",
        "    time_step = []\n",
        "    sunspots = []\n",
        "\n",
        "    with open('sunspots.csv') as csvfile:\n",
        "      reader = csv.reader(csvfile, delimiter=',')\n",
        "      next(reader)\n",
        "      for row in reader:\n",
        "        sunspots.append(float(row[2]))# YOUR CODE HERE\n",
        "        time_step.append(int(row[0]))\n",
        "\n",
        "    \n",
        "    # You should use numpy to create \n",
        "    # - your series from the list of sunspots\n",
        "    # - your time details from the list of time steps\n",
        "    series = np.array(sunspots) # YOUR CODE HERE\n",
        "    time = np.array(time_step)\n",
        "\n",
        "    # You should split the dataset into training and validation splits\n",
        "    # At time 3000. So everything up to 3000 is training, and everything\n",
        "    # after 3000 is validation. Write the code below to achieve that.\n",
        "    split_time = 3000\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    time_train = time[:split_time]\n",
        "    x_train = series[:split_time]\n",
        "    time_valid = time[split_time:]\n",
        "    x_valid = series[split_time:]\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    window_size = 30\n",
        "    batch_size = 32\n",
        "    shuffle_buffer_size = 1000\n",
        "\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    # You can use any random seed you want. We use 53. :)\n",
        "    tf.random.set_seed(53)\n",
        "    np.random.seed(53)\n",
        "    train_set = windowed_dataset(x_train, window_size=window_size, batch_size=batch_size, shuffle_buffer=shuffle_buffer_size)\n",
        "    \n",
        "    validation_set = windowed_dataset(x_valid, \n",
        "        window_size=window_size,\n",
        "        batch_size=batch_size,\n",
        "        shuffle_buffer=shuffle_buffer_size)\n",
        "\n",
        "    # 과도한 Batch Normalization 과 Dropdout 은 성능 저하의 원인이 됨... 😇\n",
        "    model = Sequential([\n",
        "        Conv1D(filters=64, kernel_size=5,\n",
        "        padding=\"causal\",\n",
        "        activation=\"relu\",\n",
        "        input_shape=[None, 1],\n",
        "        dtype=tf.float32),\n",
        "        BatchNormalization(dtype=tf.float32),\n",
        "        Dropout(0.5, dtype=tf.float32),\n",
        "        Bidirectional(LSTM(64, return_sequences=True, dtype=tf.float32)),\n",
        "        BatchNormalization(dtype=tf.float32),\n",
        "        Dropout(0.5, dtype=tf.float32),\n",
        "        Dense(128, activation=\"relu\", dtype=tf.float32),\n",
        "        Bidirectional(LSTM(32, dtype=tf.float32)),\n",
        "        Dense(64, activation=\"relu\", dtype=tf.float32),\n",
        "        BatchNormalization(dtype=tf.float32),\n",
        "        Dense(32, activation=\"relu\", dtype=tf.float32),    \n",
        "        Dense(1, dtype=tf.float32)\n",
        "    ])\n",
        "   \n",
        "    # YOUR CODE HERE TO COMPILE AND TRAIN THE MODEL\n",
        "    # optimizer 는 모범답안의 SGD 그대로.\n",
        "    optimizer = SGD(learning_rate=1e-4, momentum=0.9)\n",
        "    loss= Huber()\n",
        "    model.compile(loss=loss,\n",
        "        optimizer=optimizer,\n",
        "        metrics=[\"mae\"])\n",
        "\n",
        "    checkpoint_path = 'checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path, \n",
        "        save_weights_only=True, \n",
        "        save_best_only=True, \n",
        "        monitor='val_mae',\n",
        "        verbose=1)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "    # lr 추가\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
        "\n",
        "    epochs=100\n",
        "\n",
        "    history = model.fit(train_set, \n",
        "        validation_data=validation_set, \n",
        "        epochs=epochs, \n",
        "        callbacks=[checkpoint, early_stopping, reduce_lr],\n",
        "        )\n",
        "\n",
        "    model.load_weights(checkpoint_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"tf5-sunspot-3.h5\")\n",
        "\n",
        "# 시험때 다음 줄은 복사하지 마세요\n",
        "    # from google.colab import files\n",
        "    # files.download('tf5-sunspot-3.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A1LQR-gsLU4",
        "outputId": "db588c75-c041-4e0a-dcf3-d3ec8666564f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "     93/Unknown - 11s 49ms/step - loss: 81.6226 - mae: 82.1181\n",
            "Epoch 1: val_mae improved from inf to 70.73577, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 13s 74ms/step - loss: 81.6226 - mae: 82.1181 - val_loss: 70.2448 - val_mae: 70.7358 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 79.5923 - mae: 80.0893\n",
            "Epoch 2: val_mae improved from 70.73577 to 69.10538, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 5s 49ms/step - loss: 79.7460 - mae: 80.2429 - val_loss: 68.6103 - val_mae: 69.1054 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 77.2372 - mae: 77.7345\n",
            "Epoch 3: val_mae improved from 69.10538 to 65.64837, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 6s 59ms/step - loss: 77.2372 - mae: 77.7345 - val_loss: 65.1546 - val_mae: 65.6484 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 73.5413 - mae: 74.0379\n",
            "Epoch 4: val_mae improved from 65.64837 to 59.68415, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 5s 51ms/step - loss: 73.5413 - mae: 74.0379 - val_loss: 59.1845 - val_mae: 59.6841 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 67.3930 - mae: 67.8895\n",
            "Epoch 5: val_mae improved from 59.68415 to 49.12346, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 7s 75ms/step - loss: 67.3825 - mae: 67.8790 - val_loss: 48.6280 - val_mae: 49.1235 - lr: 1.0000e-04\n",
            "Epoch 6/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 56.2401 - mae: 56.7368\n",
            "Epoch 6: val_mae improved from 49.12346 to 32.65105, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 5s 52ms/step - loss: 56.1980 - mae: 56.6946 - val_loss: 32.1572 - val_mae: 32.6510 - lr: 1.0000e-04\n",
            "Epoch 7/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 41.1526 - mae: 41.6486\n",
            "Epoch 7: val_mae improved from 32.65105 to 27.50795, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 6s 60ms/step - loss: 41.1416 - mae: 41.6376 - val_loss: 27.0122 - val_mae: 27.5079 - lr: 1.0000e-04\n",
            "Epoch 8/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 33.2154 - mae: 33.7118\n",
            "Epoch 8: val_mae improved from 27.50795 to 24.25471, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 5s 51ms/step - loss: 33.2154 - mae: 33.7118 - val_loss: 23.7601 - val_mae: 24.2547 - lr: 1.0000e-04\n",
            "Epoch 9/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 29.8310 - mae: 30.3268\n",
            "Epoch 9: val_mae did not improve from 24.25471\n",
            "93/93 [==============================] - 5s 54ms/step - loss: 29.8310 - mae: 30.3268 - val_loss: 24.3137 - val_mae: 24.8093 - lr: 1.0000e-04\n",
            "Epoch 10/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 30.0374 - mae: 30.5335\n",
            "Epoch 10: val_mae did not improve from 24.25471\n",
            "93/93 [==============================] - 5s 55ms/step - loss: 30.0747 - mae: 30.5707 - val_loss: 26.2264 - val_mae: 26.7221 - lr: 1.0000e-04\n",
            "Epoch 11/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 29.1394 - mae: 29.6348\n",
            "Epoch 11: val_mae improved from 24.25471 to 24.04190, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 5s 50ms/step - loss: 29.1394 - mae: 29.6348 - val_loss: 23.5457 - val_mae: 24.0419 - lr: 1.0000e-04\n",
            "Epoch 12/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 29.1105 - mae: 29.6059\n",
            "Epoch 12: val_mae improved from 24.04190 to 23.55241, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 6s 60ms/step - loss: 29.1105 - mae: 29.6059 - val_loss: 23.0570 - val_mae: 23.5524 - lr: 1.0000e-04\n",
            "Epoch 13/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 28.4805 - mae: 28.9759\n",
            "Epoch 13: val_mae improved from 23.55241 to 23.05568, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 6s 61ms/step - loss: 28.4858 - mae: 28.9812 - val_loss: 22.5608 - val_mae: 23.0557 - lr: 1.0000e-04\n",
            "Epoch 14/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.1571 - mae: 28.6525\n",
            "Epoch 14: val_mae did not improve from 23.05568\n",
            "93/93 [==============================] - 5s 50ms/step - loss: 28.1571 - mae: 28.6525 - val_loss: 23.4757 - val_mae: 23.9711 - lr: 1.0000e-04\n",
            "Epoch 15/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.4040 - mae: 28.8991\n",
            "Epoch 15: val_mae improved from 23.05568 to 21.57492, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 6s 59ms/step - loss: 28.4040 - mae: 28.8991 - val_loss: 21.0807 - val_mae: 21.5749 - lr: 1.0000e-04\n",
            "Epoch 16/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.8966 - mae: 28.3918\n",
            "Epoch 16: val_mae did not improve from 21.57492\n",
            "93/93 [==============================] - 5s 50ms/step - loss: 27.8624 - mae: 28.3576 - val_loss: 21.1514 - val_mae: 21.6455 - lr: 1.0000e-04\n",
            "Epoch 17/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.1599 - mae: 28.6551\n",
            "Epoch 17: val_mae did not improve from 21.57492\n",
            "93/93 [==============================] - 5s 51ms/step - loss: 28.1599 - mae: 28.6551 - val_loss: 21.5238 - val_mae: 22.0175 - lr: 1.0000e-04\n",
            "Epoch 18/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 28.2694 - mae: 28.7647\n",
            "Epoch 18: val_mae improved from 21.57492 to 20.77290, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 6s 58ms/step - loss: 28.2751 - mae: 28.7704 - val_loss: 20.2796 - val_mae: 20.7729 - lr: 1.0000e-04\n",
            "Epoch 19/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.4119 - mae: 28.9071\n",
            "Epoch 19: val_mae did not improve from 20.77290\n",
            "93/93 [==============================] - 5s 49ms/step - loss: 28.4119 - mae: 28.9071 - val_loss: 21.4053 - val_mae: 21.8995 - lr: 1.0000e-04\n",
            "Epoch 20/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.8786 - mae: 28.3738\n",
            "Epoch 20: val_mae did not improve from 20.77290\n",
            "93/93 [==============================] - 6s 59ms/step - loss: 27.9004 - mae: 28.3956 - val_loss: 20.7398 - val_mae: 21.2327 - lr: 1.0000e-04\n",
            "Epoch 21/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.1928 - mae: 28.6879\n",
            "Epoch 21: val_mae did not improve from 20.77290\n",
            "93/93 [==============================] - 5s 51ms/step - loss: 28.1928 - mae: 28.6879 - val_loss: 20.9235 - val_mae: 21.4171 - lr: 1.0000e-04\n",
            "Epoch 22/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.8791 - mae: 28.3744\n",
            "Epoch 22: val_mae did not improve from 20.77290\n",
            "93/93 [==============================] - 6s 59ms/step - loss: 27.8791 - mae: 28.3744 - val_loss: 20.5674 - val_mae: 21.0600 - lr: 1.0000e-04\n",
            "Epoch 23/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.1447 - mae: 28.6397\n",
            "Epoch 23: val_mae did not improve from 20.77290\n",
            "93/93 [==============================] - 5s 55ms/step - loss: 28.1447 - mae: 28.6397 - val_loss: 21.0021 - val_mae: 21.4943 - lr: 1.0000e-04\n",
            "Epoch 24/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.8646 - mae: 28.3600\n",
            "Epoch 24: val_mae improved from 20.77290 to 20.39543, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 5s 52ms/step - loss: 27.8656 - mae: 28.3610 - val_loss: 19.9024 - val_mae: 20.3954 - lr: 2.0000e-05\n",
            "Epoch 25/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.2175 - mae: 28.7126\n",
            "Epoch 25: val_mae improved from 20.39543 to 20.26581, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 6s 60ms/step - loss: 28.2175 - mae: 28.7126 - val_loss: 19.7739 - val_mae: 20.2658 - lr: 2.0000e-05\n",
            "Epoch 26/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.7748 - mae: 28.2700\n",
            "Epoch 26: val_mae did not improve from 20.26581\n",
            "93/93 [==============================] - 5s 50ms/step - loss: 27.7748 - mae: 28.2700 - val_loss: 19.8239 - val_mae: 20.3156 - lr: 2.0000e-05\n",
            "Epoch 27/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.5003 - mae: 27.9955\n",
            "Epoch 27: val_mae did not improve from 20.26581\n",
            "93/93 [==============================] - 6s 60ms/step - loss: 27.5090 - mae: 28.0041 - val_loss: 19.9092 - val_mae: 20.4010 - lr: 2.0000e-05\n",
            "Epoch 28/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.4480 - mae: 27.9432\n",
            "Epoch 28: val_mae did not improve from 20.26581\n",
            "93/93 [==============================] - 5s 51ms/step - loss: 27.4085 - mae: 27.9037 - val_loss: 19.8238 - val_mae: 20.3156 - lr: 2.0000e-05\n",
            "Epoch 29/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.5652 - mae: 28.0604\n",
            "Epoch 29: val_mae improved from 20.26581 to 20.24275, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 6s 61ms/step - loss: 27.5652 - mae: 28.0604 - val_loss: 19.7511 - val_mae: 20.2428 - lr: 2.0000e-05\n",
            "Epoch 30/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.4039 - mae: 27.8990\n",
            "Epoch 30: val_mae did not improve from 20.24275\n",
            "93/93 [==============================] - 5s 52ms/step - loss: 27.4429 - mae: 27.9380 - val_loss: 19.8020 - val_mae: 20.2938 - lr: 2.0000e-05\n",
            "Epoch 31/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.9875 - mae: 28.4830\n",
            "Epoch 31: val_mae did not improve from 20.24275\n",
            "93/93 [==============================] - 6s 58ms/step - loss: 27.9875 - mae: 28.4830 - val_loss: 19.8181 - val_mae: 20.3101 - lr: 2.0000e-05\n",
            "Epoch 32/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.5947 - mae: 28.0899\n",
            "Epoch 32: val_mae did not improve from 20.24275\n",
            "93/93 [==============================] - 6s 61ms/step - loss: 27.5947 - mae: 28.0899 - val_loss: 19.8759 - val_mae: 20.3671 - lr: 2.0000e-05\n",
            "Epoch 33/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.1674 - mae: 27.6626\n",
            "Epoch 33: val_mae did not improve from 20.24275\n",
            "93/93 [==============================] - 5s 51ms/step - loss: 27.2011 - mae: 27.6963 - val_loss: 19.7855 - val_mae: 20.2767 - lr: 2.0000e-05\n",
            "Epoch 34/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.8987 - mae: 28.3941\n",
            "Epoch 34: val_mae did not improve from 20.24275\n",
            "93/93 [==============================] - 6s 60ms/step - loss: 27.8987 - mae: 28.3941 - val_loss: 19.9285 - val_mae: 20.4203 - lr: 2.0000e-05\n",
            "Epoch 35/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.4142 - mae: 27.9090\n",
            "Epoch 35: val_mae did not improve from 20.24275\n",
            "93/93 [==============================] - 5s 55ms/step - loss: 27.4142 - mae: 27.9090 - val_loss: 19.8945 - val_mae: 20.3857 - lr: 4.0000e-06\n",
            "Epoch 36/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.2481 - mae: 27.7430\n",
            "Epoch 36: val_mae did not improve from 20.24275\n",
            "93/93 [==============================] - 5s 56ms/step - loss: 27.2284 - mae: 27.7233 - val_loss: 19.8687 - val_mae: 20.3606 - lr: 4.0000e-06\n",
            "Epoch 37/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.4143 - mae: 27.9091\n",
            "Epoch 37: val_mae did not improve from 20.24275\n",
            "93/93 [==============================] - 6s 60ms/step - loss: 27.4143 - mae: 27.9091 - val_loss: 19.9128 - val_mae: 20.4041 - lr: 4.0000e-06\n",
            "Epoch 38/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.2581 - mae: 27.7533\n",
            "Epoch 38: val_mae did not improve from 20.24275\n",
            "93/93 [==============================] - 6s 60ms/step - loss: 27.2580 - mae: 27.7532 - val_loss: 19.9173 - val_mae: 20.4091 - lr: 4.0000e-06\n",
            "Epoch 39/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.3416 - mae: 27.8365\n",
            "Epoch 39: val_mae did not improve from 20.24275\n",
            "93/93 [==============================] - 5s 56ms/step - loss: 27.3416 - mae: 27.8365 - val_loss: 19.8893 - val_mae: 20.3806 - lr: 4.0000e-06\n"
          ]
        }
      ]
    }
  ]
}