{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Judy-Choi/Tensorflow_Certificate/blob/main/13_sunspot_lamda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Val_loss = 13.3\n",
        "val_acc = 13.7369\n",
        "\n",
        "요즈음에는 lambda 버전은 잘 안나옴.."
      ],
      "metadata": {
        "id": "ywBYb8yubs8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.10.0"
      ],
      "metadata": {
        "id": "azz-G6HGV0pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 답안"
      ],
      "metadata": {
        "id": "DHwGAnpDd9TF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKuzYLLQOcFB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f4b706a3-a45c-4047-fd45-290abe4f4d0d"
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# Sequence Modelling Question.\n",
        "#\n",
        "# For this task you will need to train a neural network\n",
        "# to predict sunspot activity using the Sunspots.csv\n",
        "# provided. Your neural network is expected to have an MAE\n",
        "# of at least 20, with top marks going to one with an MAE\n",
        "# of around 15. At the bottom is provided some testing\n",
        "# code should you want to check before uploading which measures\n",
        "# the MAE for you. Strongly recommend you test your model with\n",
        "# this to be able to see how it performs.\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.layers import Dense, LSTM, Lambda, Conv1D,Dropout, BatchNormalization, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.losses import Huber\n",
        "\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    # Window Sequence Loader 만들기\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    # Dataset loader 만들기\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n",
        "    urllib.request.urlretrieve(url, 'sunspots.csv')\n",
        "\n",
        "    # Your data should be loaded into 2 Python lists called time_step\n",
        "    # and sunspots. They are decleared here.\n",
        "    time_step = []\n",
        "    sunspots = []\n",
        "\n",
        "    with open('sunspots.csv') as csvfile:\n",
        "      reader = csv.reader(csvfile, delimiter=',')\n",
        "      next(reader)\n",
        "      for row in reader:\n",
        "        sunspots.append(float(row[2]))# YOUR CODE HERE\n",
        "        time_step.append(int(row[0]))\n",
        "\n",
        "    \n",
        "    # You should use numpy to create \n",
        "    # - your series from the list of sunspots\n",
        "    # - your time details from the list of time steps\n",
        "    series = np.array(sunspots) # YOUR CODE HERE\n",
        "    time = np.array(time_step)\n",
        "\n",
        "    # You should split the dataset into training and validation splits\n",
        "    # At time 3000. So everything up to 3000 is training, and everything\n",
        "    # after 3000 is validation. Write the code below to achieve that.\n",
        "    split_time = 3000\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    time_train = time[:split_time]\n",
        "    x_train = series[:split_time]\n",
        "    time_valid = time[split_time:]\n",
        "    x_valid = series[split_time:]\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    window_size = 30\n",
        "    batch_size = 32\n",
        "    shuffle_buffer_size = 1000\n",
        "\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    # You can use any random seed you want. We use 53. :)\n",
        "    tf.random.set_seed(53)\n",
        "    np.random.seed(53)\n",
        "    train_set = windowed_dataset(x_train, window_size=window_size, batch_size=batch_size, shuffle_buffer=shuffle_buffer_size)\n",
        "    \n",
        "    validation_set = windowed_dataset(x_valid, \n",
        "        window_size=window_size,\n",
        "        batch_size=batch_size,\n",
        "        shuffle_buffer=shuffle_buffer_size)\n",
        "\n",
        "\n",
        "    model = Sequential([\n",
        "        # YOUR CODE HERE. Whatever your first layer is, \n",
        "        # the input shape will be [None,1] when using the Windowed_dataset above, \n",
        "        # depending on the layer type chosen\n",
        "        Conv1D(filters=16, kernel_size=5,\n",
        "            padding=\"causal\",\n",
        "            activation=\"relu\",\n",
        "            input_shape=[None, 1]),\n",
        "        Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation=\"relu\"),\n",
        "        Dense(32, activation=\"relu\"),       \n",
        "\n",
        "    # YOUR CODE HERE. DO NOT CHANGE THE FINAL TWO LAYERS FROM BELOW\n",
        "        Dense(1),\n",
        "    # The data is not normalized, so this lambda layer helps\n",
        "    # keep the MAE in line with expectations. Do not modify.\n",
        "        Lambda(lambda x: x * 400)\n",
        "    ])\n",
        "\n",
        "   \n",
        "    # YOUR CODE HERE TO COMPILE AND TRAIN THE MODEL\n",
        "    optimizer = SGD(learning_rate=1e-4, momentum=0.9)\n",
        "    loss= Huber()\n",
        "    model.compile(loss=loss,\n",
        "        optimizer=optimizer,\n",
        "        metrics=[\"mae\"])\n",
        "    \n",
        "    checkpoint_path = 'checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path, \n",
        "        save_weights_only=True, \n",
        "        save_best_only=True, \n",
        "        monitor='val_mae',\n",
        "        verbose=1)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "    epochs=150\n",
        "\n",
        "    history = model.fit(train_set, \n",
        "        validation_data=(validation_set), \n",
        "        epochs=epochs, \n",
        "        callbacks=[checkpoint, early_stopping],\n",
        "        )\n",
        "    model.load_weights(checkpoint_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"tf5-sunspot-3.h5\")\n",
        "\n",
        "# 시험때 다음 줄은 복사하지 마세요\n",
        "    # from google.colab import files\n",
        "    # files.download('tf5-sunspot-3.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "     93/Unknown - 24s 15ms/step - loss: 36.8542 - mae: 37.3504\n",
            "Epoch 1: val_mae improved from inf to 18.29640, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 26s 37ms/step - loss: 36.8542 - mae: 37.3504 - val_loss: 17.8041 - val_mae: 18.2964\n",
            "Epoch 2/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 22.7031 - mae: 23.1972\n",
            "Epoch 2: val_mae did not improve from 18.29640\n",
            "93/93 [==============================] - 3s 25ms/step - loss: 22.7031 - mae: 23.1972 - val_loss: 18.6595 - val_mae: 19.1531\n",
            "Epoch 3/150\n",
            "90/93 [============================>.] - ETA: 0s - loss: 17.6839 - mae: 18.1763\n",
            "Epoch 3: val_mae improved from 18.29640 to 17.19674, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 17.7402 - mae: 18.2326 - val_loss: 16.7063 - val_mae: 17.1967\n",
            "Epoch 4/150\n",
            "90/93 [============================>.] - ETA: 0s - loss: 16.5308 - mae: 17.0227\n",
            "Epoch 4: val_mae improved from 17.19674 to 11.40538, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 16.5327 - mae: 17.0245 - val_loss: 10.9146 - val_mae: 11.4054\n",
            "Epoch 5/150\n",
            "90/93 [============================>.] - ETA: 0s - loss: 15.9256 - mae: 16.4173\n",
            "Epoch 5: val_mae did not improve from 11.40538\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 16.0187 - mae: 16.5106 - val_loss: 14.9916 - val_mae: 15.4809\n",
            "Epoch 6/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 15.4082 - mae: 15.8997\n",
            "Epoch 6: val_mae improved from 11.40538 to 11.00499, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 15.4082 - mae: 15.8997 - val_loss: 10.5150 - val_mae: 11.0050\n",
            "Epoch 7/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 14.4331 - mae: 14.9238\n",
            "Epoch 7: val_mae improved from 11.00499 to 10.17039, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 26ms/step - loss: 14.4173 - mae: 14.9081 - val_loss: 9.6846 - val_mae: 10.1704\n",
            "Epoch 8/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 14.6302 - mae: 15.1210\n",
            "Epoch 8: val_mae did not improve from 10.17039\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 14.6302 - mae: 15.1210 - val_loss: 17.5205 - val_mae: 18.0156\n",
            "Epoch 9/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 14.0882 - mae: 14.5792\n",
            "Epoch 9: val_mae did not improve from 10.17039\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 14.0882 - mae: 14.5792 - val_loss: 11.6993 - val_mae: 12.1851\n",
            "Epoch 10/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 13.9712 - mae: 14.4616\n",
            "Epoch 10: val_mae did not improve from 10.17039\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 13.9712 - mae: 14.4616 - val_loss: 11.9504 - val_mae: 12.4428\n",
            "Epoch 11/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 16.6339 - mae: 17.1261\n",
            "Epoch 11: val_mae did not improve from 10.17039\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 16.6104 - mae: 17.1026 - val_loss: 11.1823 - val_mae: 11.6753\n",
            "Epoch 12/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 13.9179 - mae: 14.4079\n",
            "Epoch 12: val_mae did not improve from 10.17039\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 13.9478 - mae: 14.4378 - val_loss: 14.7887 - val_mae: 15.2771\n",
            "Epoch 13/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 12.4971 - mae: 12.9869\n",
            "Epoch 13: val_mae did not improve from 10.17039\n",
            "93/93 [==============================] - 4s 38ms/step - loss: 12.4971 - mae: 12.9869 - val_loss: 13.5817 - val_mae: 14.0731\n",
            "Epoch 14/150\n",
            "90/93 [============================>.] - ETA: 0s - loss: 12.2236 - mae: 12.7130\n",
            "Epoch 14: val_mae did not improve from 10.17039\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 12.2440 - mae: 12.7335 - val_loss: 11.6703 - val_mae: 12.1594\n",
            "Epoch 15/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 18.3392 - mae: 18.8327\n",
            "Epoch 15: val_mae did not improve from 10.17039\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 18.3250 - mae: 18.8184 - val_loss: 12.3076 - val_mae: 12.7955\n",
            "Epoch 16/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 15.4331 - mae: 15.9250\n",
            "Epoch 16: val_mae improved from 10.17039 to 9.19852, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 18ms/step - loss: 15.4331 - mae: 15.9250 - val_loss: 8.7086 - val_mae: 9.1985\n",
            "Epoch 17/150\n",
            "90/93 [============================>.] - ETA: 0s - loss: 11.9992 - mae: 12.4882\n",
            "Epoch 17: val_mae did not improve from 9.19852\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 12.1561 - mae: 12.6453 - val_loss: 12.2040 - val_mae: 12.6995\n",
            "Epoch 18/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 12.2291 - mae: 12.7185\n",
            "Epoch 18: val_mae improved from 9.19852 to 8.19689, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 26ms/step - loss: 12.2291 - mae: 12.7185 - val_loss: 7.7122 - val_mae: 8.1969\n",
            "Epoch 19/150\n",
            "90/93 [============================>.] - ETA: 0s - loss: 11.2548 - mae: 11.7430\n",
            "Epoch 19: val_mae improved from 8.19689 to 6.94241, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 11.3828 - mae: 11.8711 - val_loss: 6.4660 - val_mae: 6.9424\n",
            "Epoch 20/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 11.6687 - mae: 12.1579\n",
            "Epoch 20: val_mae did not improve from 6.94241\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 11.6705 - mae: 12.1597 - val_loss: 7.1771 - val_mae: 7.6593\n",
            "Epoch 21/150\n",
            "90/93 [============================>.] - ETA: 0s - loss: 14.1943 - mae: 14.6850\n",
            "Epoch 21: val_mae did not improve from 6.94241\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 14.4452 - mae: 14.9360 - val_loss: 17.0158 - val_mae: 17.5096\n",
            "Epoch 22/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 12.6978 - mae: 13.1872\n",
            "Epoch 22: val_mae improved from 6.94241 to 5.75284, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 12.6702 - mae: 13.1596 - val_loss: 5.2772 - val_mae: 5.7528\n",
            "Epoch 23/150\n",
            "90/93 [============================>.] - ETA: 0s - loss: 13.0876 - mae: 13.5773\n",
            "Epoch 23: val_mae did not improve from 5.75284\n",
            "93/93 [==============================] - 2s 18ms/step - loss: 13.0471 - mae: 13.5368 - val_loss: 13.1575 - val_mae: 13.6441\n",
            "Epoch 24/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 10.7346 - mae: 11.2225\n",
            "Epoch 24: val_mae did not improve from 5.75284\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 10.8365 - mae: 11.3245 - val_loss: 5.4641 - val_mae: 5.9449\n",
            "Epoch 25/150\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 11.3702 - mae: 11.8592\n",
            "Epoch 25: val_mae did not improve from 5.75284\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 11.3252 - mae: 11.8142 - val_loss: 5.5075 - val_mae: 5.9901\n",
            "Epoch 26/150\n",
            "90/93 [============================>.] - ETA: 0s - loss: 10.8156 - mae: 11.3034\n",
            "Epoch 26: val_mae did not improve from 5.75284\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 10.7401 - mae: 11.2278 - val_loss: 6.8431 - val_mae: 7.3366\n",
            "Epoch 27/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 11.2150 - mae: 11.7035\n",
            "Epoch 27: val_mae did not improve from 5.75284\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 11.2150 - mae: 11.7035 - val_loss: 13.4882 - val_mae: 13.9863\n",
            "Epoch 28/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 14.1288 - mae: 14.6196\n",
            "Epoch 28: val_mae did not improve from 5.75284\n",
            "93/93 [==============================] - 2s 18ms/step - loss: 14.1288 - mae: 14.6196 - val_loss: 7.3424 - val_mae: 7.8311\n",
            "Epoch 29/150\n",
            "90/93 [============================>.] - ETA: 0s - loss: 12.8849 - mae: 13.3752\n",
            "Epoch 29: val_mae did not improve from 5.75284\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 12.8233 - mae: 13.3135 - val_loss: 7.5999 - val_mae: 8.0784\n",
            "Epoch 30/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 10.3380 - mae: 10.8260\n",
            "Epoch 30: val_mae did not improve from 5.75284\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 10.4053 - mae: 10.8934 - val_loss: 20.6343 - val_mae: 21.1330\n",
            "Epoch 31/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 14.5559 - mae: 15.0472\n",
            "Epoch 31: val_mae did not improve from 5.75284\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 14.5122 - mae: 15.0035 - val_loss: 6.9362 - val_mae: 7.4261\n",
            "Epoch 32/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 11.1788 - mae: 11.6674\n",
            "Epoch 32: val_mae did not improve from 5.75284\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 11.1788 - mae: 11.6674 - val_loss: 7.6369 - val_mae: 8.1277\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_11cccf05-e154-4b07-b027-b614325fc3ad\", \"tf5-sunspot-3.h5\", 1271376)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT\n",
        "\n",
        "Q) 다음 코드의 성능을 개선해 줘.\n",
        "참고로 사용하는 데이터는 시계열 데이터야.\n",
        "\n",
        "A) Batch normalization\n",
        "You can try adding batch normalization after the first convolutional layer and after each LSTM layer. This can improve the convergence of the model and lead to better performance.\n",
        "\n",
        "Reduce the number of LSTM layers\n",
        "Having two stacked Bidirectional LSTM layers can be computationally expensive, especially for long sequences. You can try reducing the number of LSTM layers to one and increase the number of filters in the convolutional layer instead.\n",
        "\n",
        "Increase the number of filters in the convolutional layer\n",
        "Increasing the number of filters in the convolutional layer can help capture more relevant features from the input sequence. You can try increasing the number of filters from 16 to a larger number.\n",
        "\n",
        "Add an additional dense layer\n",
        "You can try adding an additional dense layer before the output layer to allow for more complex interactions between the features extracted from the input sequence.\n",
        "\n",
        "\n",
        "Q) 사용하는 데이터가 float 자료형 시계열 데이터임을 감안해서 다시 코드를 개선해 줘\n",
        "\n",
        "A) 각 레이어의 유닛 수와 하이퍼파라미터는 데이터에 따라 최적화되어야 하므로 이전에 제안한 개선 사항은 유일한 해결책이 아닙니다. 하지만 데이터가 float 자료형 시계열 데이터인 경우, 이전에 제안한 모델을 사용하되 float32 자료형을 사용하도록 수정할 수 있습니다. 이렇게 하면 모델의 속도를 높일 수 있습니다.\n",
        "\n",
        "모든 레이어에서 dtype 매개 변수를 \"float32\"로 설정하고, 모델 컴파일 단계에서 \"float32\"로 데이터 유형을 지정할 수 있습니다."
      ],
      "metadata": {
        "id": "am1pu2d4eCbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# Sequence Modelling Question.\n",
        "#\n",
        "# For this task you will need to train a neural network\n",
        "# to predict sunspot activity using the Sunspots.csv\n",
        "# provided. Your neural network is expected to have an MAE\n",
        "# of at least 20, with top marks going to one with an MAE\n",
        "# of around 15. At the bottom is provided some testing\n",
        "# code should you want to check before uploading which measures\n",
        "# the MAE for you. Strongly recommend you test your model with\n",
        "# this to be able to see how it performs.\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.layers import Dense, LSTM, Lambda, Conv1D,Dropout, BatchNormalization, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import Huber\n",
        "\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    # Window Sequence Loader 만들기\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    # Dataset loader 만들기\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n",
        "    urllib.request.urlretrieve(url, 'sunspots.csv')\n",
        "\n",
        "    # Your data should be loaded into 2 Python lists called time_step\n",
        "    # and sunspots. They are decleared here.\n",
        "    time_step = []\n",
        "    sunspots = []\n",
        "\n",
        "    with open('sunspots.csv') as csvfile:\n",
        "      reader = csv.reader(csvfile, delimiter=',')\n",
        "      next(reader)\n",
        "      for row in reader:\n",
        "        sunspots.append(float(row[2]))# YOUR CODE HERE\n",
        "        time_step.append(int(row[0]))\n",
        "\n",
        "    \n",
        "    # You should use numpy to create \n",
        "    # - your series from the list of sunspots\n",
        "    # - your time details from the list of time steps\n",
        "    series = np.array(sunspots) # YOUR CODE HERE\n",
        "    time = np.array(time_step)\n",
        "\n",
        "    # You should split the dataset into training and validation splits\n",
        "    # At time 3000. So everything up to 3000 is training, and everything\n",
        "    # after 3000 is validation. Write the code below to achieve that.\n",
        "    split_time = 3000\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    time_train = time[:split_time]\n",
        "    x_train = series[:split_time]\n",
        "    time_valid = time[split_time:]\n",
        "    x_valid = series[split_time:]\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    window_size = 30\n",
        "    batch_size = 32\n",
        "    shuffle_buffer_size = 1000\n",
        "\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    # You can use any random seed you want. We use 53. :)\n",
        "    tf.random.set_seed(53)\n",
        "    np.random.seed(53)\n",
        "    train_set = windowed_dataset(x_train, window_size=window_size, batch_size=batch_size, shuffle_buffer=shuffle_buffer_size)\n",
        "    \n",
        "    validation_set = windowed_dataset(x_valid, \n",
        "        window_size=window_size,\n",
        "        batch_size=batch_size,\n",
        "        shuffle_buffer=shuffle_buffer_size)\n",
        "\n",
        "\n",
        "    model = Sequential([\n",
        "        Conv1D(filters=64, kernel_size=5,\n",
        "        padding=\"causal\",\n",
        "        activation=\"relu\",\n",
        "        input_shape=[None, 1],\n",
        "        dtype=tf.float32),\n",
        "        BatchNormalization(dtype=tf.float32),\n",
        "        Dropout(0.5, dtype=tf.float32),\n",
        "        LSTM(64, return_sequences=True, dtype=tf.float32),\n",
        "        BatchNormalization(dtype=tf.float32),\n",
        "        Dropout(0.5, dtype=tf.float32),\n",
        "        Dense(128, activation=\"relu\", dtype=tf.float32),\n",
        "        LSTM(32, dtype=tf.float32),\n",
        "        Dense(64, activation=\"relu\", dtype=tf.float32),\n",
        "        BatchNormalization(dtype=tf.float32),\n",
        "        Dense(32, activation=\"relu\", dtype=tf.float32),    \n",
        "        Dense(1, dtype=tf.float32),\n",
        "        Lambda(lambda x: x * 400, dtype=tf.float32)\n",
        "    ])\n",
        "\n",
        "   \n",
        "    # YOUR CODE HERE TO COMPILE AND TRAIN THE MODEL\n",
        "    optimizer = Adam(learning_rate=1e-4)\n",
        "    loss= Huber()\n",
        "    model.compile(loss=loss,\n",
        "        optimizer=optimizer,\n",
        "        metrics=[\"mae\"])\n",
        "\n",
        "    checkpoint_path = 'checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path, \n",
        "        save_weights_only=True, \n",
        "        save_best_only=True, \n",
        "        monitor='val_mae',\n",
        "        verbose=1)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
        "\n",
        "    epochs=100\n",
        "\n",
        "    history = model.fit(train_set, \n",
        "        validation_data=validation_set, \n",
        "        epochs=epochs, \n",
        "        callbacks=[checkpoint, early_stopping, reduce_lr],\n",
        "        )\n",
        "\n",
        "    model.load_weights(checkpoint_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"tf5-sunspot-3.h5\")\n",
        "\n",
        "# 시험때 다음 줄은 복사하지 마세요\n",
        "    # from google.colab import files\n",
        "    # files.download('tf5-sunspot-3.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCVo3_mCeDlb",
        "outputId": "11ae9f73-c8d3-4aa9-8baa-8234b1e0921f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "     93/Unknown - 7s 13ms/step - loss: 175.2579 - mae: 175.7573\n",
            "Epoch 1: val_mae improved from inf to 67.85394, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 9s 29ms/step - loss: 175.2579 - mae: 175.7573 - val_loss: 67.3547 - val_mae: 67.8539 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 97.7120 - mae: 98.2109\n",
            "Epoch 2: val_mae improved from 67.85394 to 66.92551, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 24ms/step - loss: 97.7120 - mae: 98.2109 - val_loss: 66.4264 - val_mae: 66.9255 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 74.2553 - mae: 74.7539\n",
            "Epoch 3: val_mae improved from 66.92551 to 61.03418, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 74.2230 - mae: 74.7216 - val_loss: 60.5361 - val_mae: 61.0342 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 62.4314 - mae: 62.9297\n",
            "Epoch 4: val_mae improved from 61.03418 to 50.07566, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 62.2917 - mae: 62.7898 - val_loss: 49.5766 - val_mae: 50.0757 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 54.7112 - mae: 55.2090\n",
            "Epoch 5: val_mae improved from 50.07566 to 45.07799, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 54.4895 - mae: 54.9874 - val_loss: 44.5793 - val_mae: 45.0780 - lr: 1.0000e-04\n",
            "Epoch 6/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 49.1892 - mae: 49.6869\n",
            "Epoch 6: val_mae did not improve from 45.07799\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 49.1892 - mae: 49.6869 - val_loss: 45.2023 - val_mae: 45.7001 - lr: 1.0000e-04\n",
            "Epoch 7/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 44.8292 - mae: 45.3266\n",
            "Epoch 7: val_mae improved from 45.07799 to 36.62551, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 18ms/step - loss: 44.7721 - mae: 45.2695 - val_loss: 36.1313 - val_mae: 36.6255 - lr: 1.0000e-04\n",
            "Epoch 8/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 42.9072 - mae: 43.4043\n",
            "Epoch 8: val_mae did not improve from 36.62551\n",
            "93/93 [==============================] - 3s 28ms/step - loss: 42.9025 - mae: 43.3995 - val_loss: 38.6384 - val_mae: 39.1340 - lr: 1.0000e-04\n",
            "Epoch 9/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 39.7936 - mae: 40.2906\n",
            "Epoch 9: val_mae improved from 36.62551 to 35.53352, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 24ms/step - loss: 39.7355 - mae: 40.2326 - val_loss: 35.0382 - val_mae: 35.5335 - lr: 1.0000e-04\n",
            "Epoch 10/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 40.2430 - mae: 40.7399\n",
            "Epoch 10: val_mae improved from 35.53352 to 34.54020, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 40.2419 - mae: 40.7388 - val_loss: 34.0424 - val_mae: 34.5402 - lr: 1.0000e-04\n",
            "Epoch 11/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 37.6772 - mae: 38.1741\n",
            "Epoch 11: val_mae did not improve from 34.54020\n",
            "93/93 [==============================] - 2s 18ms/step - loss: 37.6772 - mae: 38.1741 - val_loss: 35.0286 - val_mae: 35.5260 - lr: 1.0000e-04\n",
            "Epoch 12/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 36.3191 - mae: 36.8156\n",
            "Epoch 12: val_mae improved from 34.54020 to 31.97972, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 26ms/step - loss: 36.3306 - mae: 36.8272 - val_loss: 31.4835 - val_mae: 31.9797 - lr: 1.0000e-04\n",
            "Epoch 13/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 36.0281 - mae: 36.5246\n",
            "Epoch 13: val_mae did not improve from 31.97972\n",
            "93/93 [==============================] - 2s 18ms/step - loss: 36.0013 - mae: 36.4978 - val_loss: 32.4553 - val_mae: 32.9515 - lr: 1.0000e-04\n",
            "Epoch 14/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 35.4418 - mae: 35.9384\n",
            "Epoch 14: val_mae did not improve from 31.97972\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 35.4418 - mae: 35.9384 - val_loss: 32.0308 - val_mae: 32.5267 - lr: 1.0000e-04\n",
            "Epoch 15/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 34.3237 - mae: 34.8200\n",
            "Epoch 15: val_mae improved from 31.97972 to 28.93519, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 34.3237 - mae: 34.8200 - val_loss: 28.4392 - val_mae: 28.9352 - lr: 1.0000e-04\n",
            "Epoch 16/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 34.0611 - mae: 34.5572\n",
            "Epoch 16: val_mae did not improve from 28.93519\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 33.9895 - mae: 34.4857 - val_loss: 30.6787 - val_mae: 31.1745 - lr: 1.0000e-04\n",
            "Epoch 17/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 33.1658 - mae: 33.6623\n",
            "Epoch 17: val_mae improved from 28.93519 to 27.71017, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 33.3496 - mae: 33.8461 - val_loss: 27.2145 - val_mae: 27.7102 - lr: 1.0000e-04\n",
            "Epoch 18/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 33.4987 - mae: 33.9952\n",
            "Epoch 18: val_mae improved from 27.71017 to 27.68307, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 33.4837 - mae: 33.9802 - val_loss: 27.1881 - val_mae: 27.6831 - lr: 1.0000e-04\n",
            "Epoch 19/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 32.9915 - mae: 33.4878\n",
            "Epoch 19: val_mae improved from 27.68307 to 26.72135, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 33.0725 - mae: 33.5689 - val_loss: 26.2263 - val_mae: 26.7214 - lr: 1.0000e-04\n",
            "Epoch 20/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 31.8652 - mae: 32.3610\n",
            "Epoch 20: val_mae improved from 26.72135 to 24.99168, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 31.9339 - mae: 32.4298 - val_loss: 24.4943 - val_mae: 24.9917 - lr: 1.0000e-04\n",
            "Epoch 21/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 32.3321 - mae: 32.8282\n",
            "Epoch 21: val_mae did not improve from 24.99168\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 32.2954 - mae: 32.7915 - val_loss: 28.0287 - val_mae: 28.5210 - lr: 1.0000e-04\n",
            "Epoch 22/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 31.5310 - mae: 32.0269\n",
            "Epoch 22: val_mae did not improve from 24.99168\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 31.6189 - mae: 32.1148 - val_loss: 27.3689 - val_mae: 27.8639 - lr: 1.0000e-04\n",
            "Epoch 23/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 31.7050 - mae: 32.2010\n",
            "Epoch 23: val_mae did not improve from 24.99168\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 31.7050 - mae: 32.2010 - val_loss: 25.4088 - val_mae: 25.9040 - lr: 1.0000e-04\n",
            "Epoch 24/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 30.9901 - mae: 31.4859\n",
            "Epoch 24: val_mae did not improve from 24.99168\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 30.9931 - mae: 31.4889 - val_loss: 25.6070 - val_mae: 26.1029 - lr: 1.0000e-04\n",
            "Epoch 25/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 31.4288 - mae: 31.9248\n",
            "Epoch 25: val_mae did not improve from 24.99168\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 31.5773 - mae: 32.0733 - val_loss: 26.5334 - val_mae: 27.0295 - lr: 1.0000e-04\n",
            "Epoch 26/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 31.2462 - mae: 31.7420\n",
            "Epoch 26: val_mae improved from 24.99168 to 24.65668, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 31.2462 - mae: 31.7420 - val_loss: 24.1633 - val_mae: 24.6567 - lr: 2.0000e-05\n",
            "Epoch 27/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 30.5054 - mae: 31.0013\n",
            "Epoch 27: val_mae did not improve from 24.65668\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 30.5149 - mae: 31.0108 - val_loss: 24.3295 - val_mae: 24.8254 - lr: 2.0000e-05\n",
            "Epoch 28/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 30.3680 - mae: 30.8636\n",
            "Epoch 28: val_mae did not improve from 24.65668\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 30.3032 - mae: 30.7987 - val_loss: 24.3741 - val_mae: 24.8693 - lr: 2.0000e-05\n",
            "Epoch 29/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 30.7476 - mae: 31.2434\n",
            "Epoch 29: val_mae did not improve from 24.65668\n",
            "93/93 [==============================] - 3s 29ms/step - loss: 30.7476 - mae: 31.2434 - val_loss: 24.2568 - val_mae: 24.7521 - lr: 2.0000e-05\n",
            "Epoch 30/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 30.1282 - mae: 30.6238\n",
            "Epoch 30: val_mae improved from 24.65668 to 24.13524, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 18ms/step - loss: 30.1960 - mae: 30.6916 - val_loss: 23.6388 - val_mae: 24.1352 - lr: 2.0000e-05\n",
            "Epoch 31/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 30.7427 - mae: 31.2384\n",
            "Epoch 31: val_mae did not improve from 24.13524\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 30.7165 - mae: 31.2121 - val_loss: 24.2536 - val_mae: 24.7486 - lr: 2.0000e-05\n",
            "Epoch 32/100\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 30.3420 - mae: 30.8377\n",
            "Epoch 32: val_mae improved from 24.13524 to 24.08455, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 30.3272 - mae: 30.8229 - val_loss: 23.5887 - val_mae: 24.0846 - lr: 2.0000e-05\n",
            "Epoch 33/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 29.9857 - mae: 30.4815\n",
            "Epoch 33: val_mae did not improve from 24.08455\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 30.1014 - mae: 30.5972 - val_loss: 23.6587 - val_mae: 24.1543 - lr: 2.0000e-05\n",
            "Epoch 34/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 30.8767 - mae: 31.3723\n",
            "Epoch 34: val_mae did not improve from 24.08455\n",
            "93/93 [==============================] - 2s 24ms/step - loss: 30.9190 - mae: 31.4146 - val_loss: 23.8437 - val_mae: 24.3400 - lr: 2.0000e-05\n",
            "Epoch 35/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 30.4170 - mae: 30.9127\n",
            "Epoch 35: val_mae improved from 24.08455 to 23.76631, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 18ms/step - loss: 30.4332 - mae: 30.9290 - val_loss: 23.2703 - val_mae: 23.7663 - lr: 2.0000e-05\n",
            "Epoch 36/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 29.9301 - mae: 30.4254\n",
            "Epoch 36: val_mae improved from 23.76631 to 23.55009, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 18ms/step - loss: 29.9229 - mae: 30.4182 - val_loss: 23.0544 - val_mae: 23.5501 - lr: 2.0000e-05\n",
            "Epoch 37/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 30.2814 - mae: 30.7771\n",
            "Epoch 37: val_mae did not improve from 23.55009\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 30.2669 - mae: 30.7627 - val_loss: 23.2680 - val_mae: 23.7620 - lr: 2.0000e-05\n",
            "Epoch 38/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 29.9064 - mae: 30.4021\n",
            "Epoch 38: val_mae did not improve from 23.55009\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 29.9263 - mae: 30.4220 - val_loss: 23.8051 - val_mae: 24.3005 - lr: 2.0000e-05\n",
            "Epoch 39/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 30.1852 - mae: 30.6809\n",
            "Epoch 39: val_mae did not improve from 23.55009\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 30.2129 - mae: 30.7086 - val_loss: 23.5738 - val_mae: 24.0691 - lr: 2.0000e-05\n",
            "Epoch 40/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 30.2810 - mae: 30.7768\n",
            "Epoch 40: val_mae did not improve from 23.55009\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 30.2402 - mae: 30.7359 - val_loss: 23.5651 - val_mae: 24.0609 - lr: 2.0000e-05\n",
            "Epoch 41/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 30.1395 - mae: 30.6351\n",
            "Epoch 41: val_mae did not improve from 23.55009\n",
            "93/93 [==============================] - 2s 18ms/step - loss: 30.1395 - mae: 30.6351 - val_loss: 23.7938 - val_mae: 24.2899 - lr: 2.0000e-05\n",
            "Epoch 42/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 30.0133 - mae: 30.5091\n",
            "Epoch 42: val_mae improved from 23.55009 to 23.33984, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 18ms/step - loss: 30.0133 - mae: 30.5091 - val_loss: 22.8445 - val_mae: 23.3398 - lr: 4.0000e-06\n",
            "Epoch 43/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 29.7604 - mae: 30.2559\n",
            "Epoch 43: val_mae improved from 23.33984 to 23.03432, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 18ms/step - loss: 29.7604 - mae: 30.2559 - val_loss: 22.5382 - val_mae: 23.0343 - lr: 4.0000e-06\n",
            "Epoch 44/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 29.9368 - mae: 30.4325\n",
            "Epoch 44: val_mae improved from 23.03432 to 22.96275, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 18ms/step - loss: 29.9368 - mae: 30.4325 - val_loss: 22.4668 - val_mae: 22.9627 - lr: 4.0000e-06\n",
            "Epoch 45/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 30.5058 - mae: 31.0014\n",
            "Epoch 45: val_mae improved from 22.96275 to 22.94889, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 30.5058 - mae: 31.0014 - val_loss: 22.4532 - val_mae: 22.9489 - lr: 4.0000e-06\n",
            "Epoch 46/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 30.1290 - mae: 30.6243\n",
            "Epoch 46: val_mae improved from 22.94889 to 22.82777, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 30.1122 - mae: 30.6074 - val_loss: 22.3326 - val_mae: 22.8278 - lr: 4.0000e-06\n",
            "Epoch 47/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 30.2239 - mae: 30.7199\n",
            "Epoch 47: val_mae did not improve from 22.82777\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 30.2239 - mae: 30.7199 - val_loss: 22.3698 - val_mae: 22.8656 - lr: 4.0000e-06\n",
            "Epoch 48/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 29.7014 - mae: 30.1970\n",
            "Epoch 48: val_mae did not improve from 22.82777\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 29.6971 - mae: 30.1927 - val_loss: 22.5281 - val_mae: 23.0234 - lr: 4.0000e-06\n",
            "Epoch 49/100\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 29.9670 - mae: 30.4627\n",
            "Epoch 49: val_mae did not improve from 22.82777\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 29.9555 - mae: 30.4512 - val_loss: 22.5149 - val_mae: 23.0103 - lr: 4.0000e-06\n",
            "Epoch 50/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 29.8998 - mae: 30.3953\n",
            "Epoch 50: val_mae did not improve from 22.82777\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 29.8998 - mae: 30.3953 - val_loss: 22.5784 - val_mae: 23.0739 - lr: 4.0000e-06\n",
            "Epoch 51/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 29.6800 - mae: 30.1758\n",
            "Epoch 51: val_mae did not improve from 22.82777\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 29.7183 - mae: 30.2141 - val_loss: 22.7138 - val_mae: 23.2087 - lr: 4.0000e-06\n",
            "Epoch 52/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 29.3337 - mae: 29.8294\n",
            "Epoch 52: val_mae did not improve from 22.82777\n",
            "93/93 [==============================] - 2s 24ms/step - loss: 29.3337 - mae: 29.8294 - val_loss: 22.7437 - val_mae: 23.2389 - lr: 1.0000e-06\n",
            "Epoch 53/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 30.0563 - mae: 30.5519\n",
            "Epoch 53: val_mae did not improve from 22.82777\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 30.0794 - mae: 30.5750 - val_loss: 22.7907 - val_mae: 23.2866 - lr: 1.0000e-06\n",
            "Epoch 54/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 29.4957 - mae: 29.9912\n",
            "Epoch 54: val_mae did not improve from 22.82777\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 29.6464 - mae: 30.1419 - val_loss: 22.6574 - val_mae: 23.1524 - lr: 1.0000e-06\n",
            "Epoch 55/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 29.4573 - mae: 29.9529\n",
            "Epoch 55: val_mae did not improve from 22.82777\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 29.4755 - mae: 29.9710 - val_loss: 22.5548 - val_mae: 23.0498 - lr: 1.0000e-06\n",
            "Epoch 56/100\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 29.7145 - mae: 30.2100\n",
            "Epoch 56: val_mae did not improve from 22.82777\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 29.7969 - mae: 30.2924 - val_loss: 22.7370 - val_mae: 23.2317 - lr: 1.0000e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 직접 코딩\n",
        "\n",
        "과제 순서를 바꿔서, ChatGPT 가 짜 준 코드를 직접 개선해 본다"
      ],
      "metadata": {
        "id": "Anndke16dItt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# Sequence Modelling Question.\n",
        "#\n",
        "# For this task you will need to train a neural network\n",
        "# to predict sunspot activity using the Sunspots.csv\n",
        "# provided. Your neural network is expected to have an MAE\n",
        "# of at least 20, with top marks going to one with an MAE\n",
        "# of around 15. At the bottom is provided some testing\n",
        "# code should you want to check before uploading which measures\n",
        "# the MAE for you. Strongly recommend you test your model with\n",
        "# this to be able to see how it performs.\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.layers import Dense, LSTM, Lambda, Conv1D,Dropout, BatchNormalization, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.losses import Huber\n",
        "\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    # Window Sequence Loader 만들기\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    # Dataset loader 만들기\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n",
        "    urllib.request.urlretrieve(url, 'sunspots.csv')\n",
        "\n",
        "    # Your data should be loaded into 2 Python lists called time_step\n",
        "    # and sunspots. They are decleared here.\n",
        "    time_step = []\n",
        "    sunspots = []\n",
        "\n",
        "    with open('sunspots.csv') as csvfile:\n",
        "      reader = csv.reader(csvfile, delimiter=',')\n",
        "      next(reader)\n",
        "      for row in reader:\n",
        "        sunspots.append(float(row[2]))# YOUR CODE HERE\n",
        "        time_step.append(int(row[0]))\n",
        "\n",
        "    \n",
        "    # You should use numpy to create \n",
        "    # - your series from the list of sunspots\n",
        "    # - your time details from the list of time steps\n",
        "    series = np.array(sunspots) # YOUR CODE HERE\n",
        "    time = np.array(time_step)\n",
        "\n",
        "    # You should split the dataset into training and validation splits\n",
        "    # At time 3000. So everything up to 3000 is training, and everything\n",
        "    # after 3000 is validation. Write the code below to achieve that.\n",
        "    split_time = 3000\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    time_train = time[:split_time]\n",
        "    x_train = series[:split_time]\n",
        "    time_valid = time[split_time:]\n",
        "    x_valid = series[split_time:]\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    window_size = 30\n",
        "    batch_size = 32\n",
        "    shuffle_buffer_size = 1000\n",
        "\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    # You can use any random seed you want. We use 53. :)\n",
        "    tf.random.set_seed(53)\n",
        "    np.random.seed(53)\n",
        "    train_set = windowed_dataset(x_train, window_size=window_size, batch_size=batch_size, shuffle_buffer=shuffle_buffer_size)\n",
        "    \n",
        "    validation_set = windowed_dataset(x_valid, \n",
        "        window_size=window_size,\n",
        "        batch_size=batch_size,\n",
        "        shuffle_buffer=shuffle_buffer_size)\n",
        "\n",
        "    # 과도한 Batch Normalization 과 Dropdout 은 성능 저하의 원인이 되므로 제거한다 😇\n",
        "    model = Sequential([\n",
        "        Conv1D(filters=64, kernel_size=5,\n",
        "        padding=\"causal\",\n",
        "        activation=\"relu\",\n",
        "        input_shape=[None, 1],\n",
        "        dtype=tf.float32),\n",
        "        BatchNormalization(dtype=tf.float32),\n",
        "        Dropout(0.5, dtype=tf.float32),\n",
        "        Bidirectional(LSTM(64, return_sequences=True, dtype=tf.float32)),\n",
        "        BatchNormalization(dtype=tf.float32),\n",
        "        Dropout(0.5, dtype=tf.float32),\n",
        "        Dense(128, activation=\"relu\", dtype=tf.float32),\n",
        "        Bidirectional(LSTM(32, dtype=tf.float32)),\n",
        "        Dense(64, activation=\"relu\", dtype=tf.float32),\n",
        "        BatchNormalization(dtype=tf.float32),\n",
        "        Dense(32, activation=\"relu\", dtype=tf.float32),    \n",
        "        Dense(1, dtype=tf.float32),\n",
        "        Lambda(lambda x: x * 400, dtype=tf.float32)\n",
        "    ])\n",
        "   \n",
        "    # YOUR CODE HERE TO COMPILE AND TRAIN THE MODEL\n",
        "    # optimizer 는 모범답안의 SGD 그대로.\n",
        "    optimizer = SGD(learning_rate=1e-4, momentum=0.9)\n",
        "    loss= Huber()\n",
        "    model.compile(loss=loss,\n",
        "        optimizer=optimizer,\n",
        "        metrics=[\"mae\"])\n",
        "\n",
        "    checkpoint_path = 'checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path, \n",
        "        save_weights_only=True, \n",
        "        save_best_only=True, \n",
        "        monitor='val_mae',\n",
        "        verbose=1)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "    # lr 추가\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
        "\n",
        "    epochs=100\n",
        "\n",
        "    history = model.fit(train_set, \n",
        "        validation_data=validation_set, \n",
        "        epochs=epochs, \n",
        "        callbacks=[checkpoint, early_stopping, reduce_lr],\n",
        "        )\n",
        "\n",
        "    model.load_weights(checkpoint_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"tf5-sunspot-3.h5\")\n",
        "\n",
        "# 시험때 다음 줄은 복사하지 마세요\n",
        "    # from google.colab import files\n",
        "    # files.download('tf5-sunspot-3.h5')"
      ],
      "metadata": {
        "id": "wPFm48A1uhey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "543f27f6-4bb6-499c-9ef1-889aac250abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "     90/Unknown - 9s 21ms/step - loss: 55.9400 - mae: 56.4374\n",
            "Epoch 1: val_mae improved from inf to 25.10450, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 11s 39ms/step - loss: 55.2828 - mae: 55.7802 - val_loss: 24.6101 - val_mae: 25.1045 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 33.8299 - mae: 34.3265\n",
            "Epoch 2: val_mae did not improve from 25.10450\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 33.8883 - mae: 34.3850 - val_loss: 30.7728 - val_mae: 31.2696 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 31.1712 - mae: 31.6674\n",
            "Epoch 3: val_mae improved from 25.10450 to 21.74508, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 31.1712 - mae: 31.6674 - val_loss: 21.2504 - val_mae: 21.7451 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 30.8952 - mae: 31.3913\n",
            "Epoch 4: val_mae did not improve from 21.74508\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 30.9181 - mae: 31.4142 - val_loss: 22.1923 - val_mae: 22.6870 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 31.8769 - mae: 32.3730\n",
            "Epoch 5: val_mae did not improve from 21.74508\n",
            "93/93 [==============================] - 3s 27ms/step - loss: 31.8699 - mae: 32.3660 - val_loss: 23.3069 - val_mae: 23.8033 - lr: 1.0000e-04\n",
            "Epoch 6/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 31.5336 - mae: 32.0298\n",
            "Epoch 6: val_mae did not improve from 21.74508\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 31.6326 - mae: 32.1289 - val_loss: 25.6788 - val_mae: 26.1756 - lr: 1.0000e-04\n",
            "Epoch 7/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 29.7719 - mae: 30.2680\n",
            "Epoch 7: val_mae did not improve from 21.74508\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 29.7884 - mae: 30.2845 - val_loss: 21.6022 - val_mae: 22.0979 - lr: 1.0000e-04\n",
            "Epoch 8/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 31.0479 - mae: 31.5439\n",
            "Epoch 8: val_mae did not improve from 21.74508\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 31.0479 - mae: 31.5439 - val_loss: 24.2348 - val_mae: 24.7303 - lr: 1.0000e-04\n",
            "Epoch 9/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.5469 - mae: 29.0426\n",
            "Epoch 9: val_mae improved from 21.74508 to 20.90508, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 28ms/step - loss: 28.5469 - mae: 29.0426 - val_loss: 20.4124 - val_mae: 20.9051 - lr: 2.0000e-05\n",
            "Epoch 10/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 28.8573 - mae: 29.3529\n",
            "Epoch 10: val_mae did not improve from 20.90508\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 28.8611 - mae: 29.3567 - val_loss: 23.3972 - val_mae: 23.8935 - lr: 2.0000e-05\n",
            "Epoch 11/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 28.1254 - mae: 28.6209\n",
            "Epoch 11: val_mae did not improve from 20.90508\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 28.1156 - mae: 28.6111 - val_loss: 23.5035 - val_mae: 24.0004 - lr: 2.0000e-05\n",
            "Epoch 12/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 28.3490 - mae: 28.8443\n",
            "Epoch 12: val_mae did not improve from 20.90508\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 28.3635 - mae: 28.8588 - val_loss: 23.0275 - val_mae: 23.5223 - lr: 2.0000e-05\n",
            "Epoch 13/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 29.3718 - mae: 29.8676\n",
            "Epoch 13: val_mae did not improve from 20.90508\n",
            "93/93 [==============================] - 3s 29ms/step - loss: 29.4399 - mae: 29.9357 - val_loss: 21.7294 - val_mae: 22.2248 - lr: 2.0000e-05\n",
            "Epoch 14/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 27.4665 - mae: 27.9620\n",
            "Epoch 14: val_mae improved from 20.90508 to 20.60523, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 27.4564 - mae: 27.9519 - val_loss: 20.1132 - val_mae: 20.6052 - lr: 2.0000e-05\n",
            "Epoch 15/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.9866 - mae: 28.4817\n",
            "Epoch 15: val_mae did not improve from 20.60523\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 28.0413 - mae: 28.5365 - val_loss: 20.4563 - val_mae: 20.9498 - lr: 2.0000e-05\n",
            "Epoch 16/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 27.6451 - mae: 28.1404\n",
            "Epoch 16: val_mae improved from 20.60523 to 20.55362, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 27.6087 - mae: 28.1040 - val_loss: 20.0612 - val_mae: 20.5536 - lr: 2.0000e-05\n",
            "Epoch 17/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.9466 - mae: 28.4423\n",
            "Epoch 17: val_mae did not improve from 20.55362\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 27.9466 - mae: 28.4423 - val_loss: 23.7789 - val_mae: 24.2748 - lr: 2.0000e-05\n",
            "Epoch 18/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 29.2411 - mae: 29.7369\n",
            "Epoch 18: val_mae did not improve from 20.55362\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 29.2411 - mae: 29.7369 - val_loss: 20.3390 - val_mae: 20.8330 - lr: 2.0000e-05\n",
            "Epoch 19/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 28.2341 - mae: 28.7295\n",
            "Epoch 19: val_mae did not improve from 20.55362\n",
            "93/93 [==============================] - 2s 24ms/step - loss: 28.2542 - mae: 28.7495 - val_loss: 22.4196 - val_mae: 22.9156 - lr: 2.0000e-05\n",
            "Epoch 20/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 28.0792 - mae: 28.5745\n",
            "Epoch 20: val_mae improved from 20.55362 to 20.33611, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 28.1342 - mae: 28.6296 - val_loss: 19.8443 - val_mae: 20.3361 - lr: 2.0000e-05\n",
            "Epoch 21/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 28.8448 - mae: 29.3407\n",
            "Epoch 21: val_mae did not improve from 20.33611\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 28.8391 - mae: 29.3349 - val_loss: 20.1860 - val_mae: 20.6773 - lr: 2.0000e-05\n",
            "Epoch 22/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.1000 - mae: 28.5954\n",
            "Epoch 22: val_mae did not improve from 20.33611\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 28.1000 - mae: 28.5954 - val_loss: 20.2839 - val_mae: 20.7790 - lr: 2.0000e-05\n",
            "Epoch 23/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 28.7030 - mae: 29.1988\n",
            "Epoch 23: val_mae improved from 20.33611 to 20.31940, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 28.8116 - mae: 29.3074 - val_loss: 19.8250 - val_mae: 20.3194 - lr: 2.0000e-05\n",
            "Epoch 24/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 27.6887 - mae: 28.1840\n",
            "Epoch 24: val_mae improved from 20.31940 to 20.28636, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 33ms/step - loss: 27.7776 - mae: 28.2728 - val_loss: 19.7917 - val_mae: 20.2864 - lr: 2.0000e-05\n",
            "Epoch 25/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 28.0178 - mae: 28.5131\n",
            "Epoch 25: val_mae did not improve from 20.28636\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 28.1465 - mae: 28.6418 - val_loss: 20.3262 - val_mae: 20.8217 - lr: 2.0000e-05\n",
            "Epoch 26/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 28.2083 - mae: 28.7039\n",
            "Epoch 26: val_mae did not improve from 20.28636\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 28.2278 - mae: 28.7235 - val_loss: 19.9968 - val_mae: 20.4881 - lr: 2.0000e-05\n",
            "Epoch 27/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 27.5977 - mae: 28.0930\n",
            "Epoch 27: val_mae did not improve from 20.28636\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 27.6511 - mae: 28.1463 - val_loss: 21.1248 - val_mae: 21.6198 - lr: 2.0000e-05\n",
            "Epoch 28/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 28.0087 - mae: 28.5042\n",
            "Epoch 28: val_mae improved from 20.28636 to 20.27106, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 3s 26ms/step - loss: 27.9828 - mae: 28.4783 - val_loss: 19.7771 - val_mae: 20.2711 - lr: 2.0000e-05\n",
            "Epoch 29/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.9944 - mae: 28.4897\n",
            "Epoch 29: val_mae did not improve from 20.27106\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 28.0990 - mae: 28.5943 - val_loss: 20.9558 - val_mae: 21.4476 - lr: 2.0000e-05\n",
            "Epoch 30/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 27.4927 - mae: 27.9881\n",
            "Epoch 30: val_mae improved from 20.27106 to 20.25530, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 27.6035 - mae: 28.0988 - val_loss: 19.7592 - val_mae: 20.2553 - lr: 2.0000e-05\n",
            "Epoch 31/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 28.2506 - mae: 28.7460\n",
            "Epoch 31: val_mae did not improve from 20.25530\n",
            "93/93 [==============================] - 2s 18ms/step - loss: 28.2221 - mae: 28.7176 - val_loss: 20.2588 - val_mae: 20.7544 - lr: 2.0000e-05\n",
            "Epoch 32/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.5300 - mae: 28.0252\n",
            "Epoch 32: val_mae did not improve from 20.25530\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 27.5300 - mae: 28.0252 - val_loss: 20.2338 - val_mae: 20.7293 - lr: 2.0000e-05\n",
            "Epoch 33/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.4782 - mae: 27.9737\n",
            "Epoch 33: val_mae did not improve from 20.25530\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 27.5169 - mae: 28.0124 - val_loss: 22.2635 - val_mae: 22.7590 - lr: 2.0000e-05\n",
            "Epoch 34/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.9426 - mae: 28.4379\n",
            "Epoch 34: val_mae did not improve from 20.25530\n",
            "93/93 [==============================] - 3s 34ms/step - loss: 27.9426 - mae: 28.4379 - val_loss: 20.6743 - val_mae: 21.1655 - lr: 2.0000e-05\n",
            "Epoch 35/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.2317 - mae: 28.7273\n",
            "Epoch 35: val_mae did not improve from 20.25530\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 28.2317 - mae: 28.7273 - val_loss: 20.0344 - val_mae: 20.5263 - lr: 2.0000e-05\n",
            "Epoch 36/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 27.4914 - mae: 27.9865\n",
            "Epoch 36: val_mae did not improve from 20.25530\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 27.4509 - mae: 27.9460 - val_loss: 19.7673 - val_mae: 20.2580 - lr: 4.0000e-06\n",
            "Epoch 37/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 27.1032 - mae: 27.5985\n",
            "Epoch 37: val_mae did not improve from 20.25530\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 27.0848 - mae: 27.5801 - val_loss: 20.7273 - val_mae: 21.2231 - lr: 4.0000e-06\n",
            "Epoch 38/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 27.4640 - mae: 27.9592\n",
            "Epoch 38: val_mae improved from 20.25530 to 20.10376, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 27.4537 - mae: 27.9489 - val_loss: 19.6102 - val_mae: 20.1038 - lr: 4.0000e-06\n",
            "Epoch 39/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.2709 - mae: 27.7659\n",
            "Epoch 39: val_mae did not improve from 20.10376\n",
            "93/93 [==============================] - 2s 24ms/step - loss: 27.2895 - mae: 27.7844 - val_loss: 19.8158 - val_mae: 20.3085 - lr: 4.0000e-06\n",
            "Epoch 40/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 27.6613 - mae: 28.1561\n",
            "Epoch 40: val_mae did not improve from 20.10376\n",
            "93/93 [==============================] - 3s 25ms/step - loss: 27.6421 - mae: 28.1369 - val_loss: 19.6853 - val_mae: 20.1783 - lr: 4.0000e-06\n",
            "Epoch 41/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.5250 - mae: 28.0203\n",
            "Epoch 41: val_mae did not improve from 20.10376\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 27.5250 - mae: 28.0203 - val_loss: 19.9603 - val_mae: 20.4544 - lr: 4.0000e-06\n",
            "Epoch 42/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.5753 - mae: 28.0705\n",
            "Epoch 42: val_mae improved from 20.10376 to 20.07807, saving model to checkpoint.ckpt\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 27.5572 - mae: 28.0524 - val_loss: 19.5857 - val_mae: 20.0781 - lr: 4.0000e-06\n",
            "Epoch 43/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 27.4635 - mae: 27.9587\n",
            "Epoch 43: val_mae did not improve from 20.07807\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 27.4658 - mae: 27.9609 - val_loss: 19.5992 - val_mae: 20.0908 - lr: 4.0000e-06\n",
            "Epoch 44/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.2187 - mae: 27.7137\n",
            "Epoch 44: val_mae did not improve from 20.07807\n",
            "93/93 [==============================] - 3s 26ms/step - loss: 27.2461 - mae: 27.7411 - val_loss: 19.8816 - val_mae: 20.3765 - lr: 4.0000e-06\n",
            "Epoch 45/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 28.1124 - mae: 28.6076\n",
            "Epoch 45: val_mae did not improve from 20.07807\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 28.1124 - mae: 28.6076 - val_loss: 19.6696 - val_mae: 20.1611 - lr: 4.0000e-06\n",
            "Epoch 46/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 27.4211 - mae: 27.9161\n",
            "Epoch 46: val_mae did not improve from 20.07807\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 27.4324 - mae: 27.9274 - val_loss: 19.7935 - val_mae: 20.2867 - lr: 4.0000e-06\n",
            "Epoch 47/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 27.5460 - mae: 28.0409\n",
            "Epoch 47: val_mae did not improve from 20.07807\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 27.6491 - mae: 28.1440 - val_loss: 19.6707 - val_mae: 20.1654 - lr: 4.0000e-06\n",
            "Epoch 48/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.5304 - mae: 28.0257\n",
            "Epoch 48: val_mae did not improve from 20.07807\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 27.5304 - mae: 28.0257 - val_loss: 19.8766 - val_mae: 20.3728 - lr: 1.0000e-06\n",
            "Epoch 49/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 27.7414 - mae: 28.2364\n",
            "Epoch 49: val_mae did not improve from 20.07807\n",
            "93/93 [==============================] - 3s 29ms/step - loss: 27.7691 - mae: 28.2640 - val_loss: 19.9602 - val_mae: 20.4561 - lr: 1.0000e-06\n",
            "Epoch 50/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 27.8110 - mae: 28.3061\n",
            "Epoch 50: val_mae did not improve from 20.07807\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 27.7783 - mae: 28.2734 - val_loss: 19.8767 - val_mae: 20.3732 - lr: 1.0000e-06\n",
            "Epoch 51/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 27.5662 - mae: 28.0613\n",
            "Epoch 51: val_mae did not improve from 20.07807\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 27.5662 - mae: 28.0613 - val_loss: 19.9538 - val_mae: 20.4495 - lr: 1.0000e-06\n",
            "Epoch 52/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 27.4339 - mae: 27.9290\n",
            "Epoch 52: val_mae did not improve from 20.07807\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 27.5135 - mae: 28.0085 - val_loss: 19.8193 - val_mae: 20.3154 - lr: 1.0000e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1woWUC4WeNZv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}