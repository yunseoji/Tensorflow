{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Judy-Choi/Tensorflow_Certificate/blob/main/15_diesel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# val_mae = 0.026"
      ],
      "metadata": {
        "id": "QxrNF5GpvZlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "S6khUlk0P3jI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96ad2ac6-d490-4550-c334-6ee7fc4c11f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R4auozdsPLhk",
        "outputId": "cd7523e0-d8d4-4d74-a96d-cfd891223583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.12.0\n",
            "Uninstalling tensorflow-2.12.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.9/dist-packages/tensorflow-2.12.0.dist-info/*\n",
            "    /usr/local/lib/python3.9/dist-packages/tensorflow/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.12.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.10.0\n",
            "  Downloading tensorflow-2.10.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.1/578.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.11,>=2.10.0\n",
            "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (0.32.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (0.2.0)\n",
            "Collecting keras<2.11,>=2.10.0\n",
            "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.53.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (16.0.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (23.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (3.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (2.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (67.6.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (23.3.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.0) (4.5.0)\n",
            "Collecting protobuf<3.20,>=3.9.2\n",
            "  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard<2.11,>=2.10\n",
            "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow==2.10.0) (0.40.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.2.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.8.1)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.4.3)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (6.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.2.2)\n",
            "Installing collected packages: keras, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.0\n",
            "    Uninstalling tensorboard-data-server-0.7.0:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.1\n",
            "    Uninstalling tensorboard-2.12.1:\n",
            "      Successfully uninstalled tensorboard-2.12.1\n",
            "Successfully installed google-auth-oauthlib-0.4.6 keras-2.10.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 정답"
      ],
      "metadata": {
        "id": "_i6Q6y4ozaoU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvkRuTNCNDOf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c1cbfee-bde0-4a00-d5ca-d2cb7bc95a64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "     34/Unknown - 11s 53ms/step - loss: 0.0504 - mae: 0.2440\n",
            "Epoch 1: val_mae improved from inf to 0.29766, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 14s 146ms/step - loss: 0.0500 - mae: 0.2446 - val_loss: 0.0463 - val_mae: 0.2977\n",
            "Epoch 2/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0280 - mae: 0.1705\n",
            "Epoch 2: val_mae improved from 0.29766 to 0.17731, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0277 - mae: 0.1719 - val_loss: 0.0174 - val_mae: 0.1773\n",
            "Epoch 3/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0124 - mae: 0.1331\n",
            "Epoch 3: val_mae improved from 0.17731 to 0.05640, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0119 - mae: 0.1291 - val_loss: 0.0025 - val_mae: 0.0564\n",
            "Epoch 4/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0097 - mae: 0.1237\n",
            "Epoch 4: val_mae improved from 0.05640 to 0.04323, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0097 - mae: 0.1237 - val_loss: 0.0014 - val_mae: 0.0432\n",
            "Epoch 5/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0084 - mae: 0.1143\n",
            "Epoch 5: val_mae improved from 0.04323 to 0.04120, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0080 - mae: 0.1109 - val_loss: 0.0013 - val_mae: 0.0412\n",
            "Epoch 6/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0915\n",
            "Epoch 6: val_mae improved from 0.04120 to 0.04055, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 47ms/step - loss: 0.0055 - mae: 0.0904 - val_loss: 0.0013 - val_mae: 0.0405\n",
            "Epoch 7/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0033 - mae: 0.0684\n",
            "Epoch 7: val_mae improved from 0.04055 to 0.03925, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 59ms/step - loss: 0.0033 - mae: 0.0679 - val_loss: 0.0012 - val_mae: 0.0392\n",
            "Epoch 8/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0021 - mae: 0.0499\n",
            "Epoch 8: val_mae improved from 0.03925 to 0.03768, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 59ms/step - loss: 0.0021 - mae: 0.0499 - val_loss: 0.0012 - val_mae: 0.0377\n",
            "Epoch 9/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0018 - mae: 0.0414\n",
            "Epoch 9: val_mae improved from 0.03768 to 0.03601, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 62ms/step - loss: 0.0018 - mae: 0.0414 - val_loss: 0.0011 - val_mae: 0.0360\n",
            "Epoch 10/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0017 - mae: 0.0380\n",
            "Epoch 10: val_mae improved from 0.03601 to 0.03437, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 3s 71ms/step - loss: 0.0017 - mae: 0.0380 - val_loss: 9.8652e-04 - val_mae: 0.0344\n",
            "Epoch 11/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0016 - mae: 0.0367\n",
            "Epoch 11: val_mae improved from 0.03437 to 0.03307, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0016 - mae: 0.0367 - val_loss: 9.2293e-04 - val_mae: 0.0331\n",
            "Epoch 12/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0015 - mae: 0.0356\n",
            "Epoch 12: val_mae improved from 0.03307 to 0.03194, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0015 - mae: 0.0356 - val_loss: 8.6975e-04 - val_mae: 0.0319\n",
            "Epoch 13/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0014 - mae: 0.0332\n",
            "Epoch 13: val_mae improved from 0.03194 to 0.03097, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0015 - mae: 0.0346 - val_loss: 8.2606e-04 - val_mae: 0.0310\n",
            "Epoch 14/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0014 - mae: 0.0325\n",
            "Epoch 14: val_mae improved from 0.03097 to 0.03017, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0014 - mae: 0.0339 - val_loss: 7.9138e-04 - val_mae: 0.0302\n",
            "Epoch 15/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0330\n",
            "Epoch 15: val_mae improved from 0.03017 to 0.02950, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 54ms/step - loss: 0.0014 - mae: 0.0333 - val_loss: 7.6322e-04 - val_mae: 0.0295\n",
            "Epoch 16/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0324\n",
            "Epoch 16: val_mae improved from 0.02950 to 0.02894, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 57ms/step - loss: 0.0014 - mae: 0.0327 - val_loss: 7.4026e-04 - val_mae: 0.0289\n",
            "Epoch 17/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0013 - mae: 0.0320\n",
            "Epoch 17: val_mae improved from 0.02894 to 0.02847, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 56ms/step - loss: 0.0013 - mae: 0.0323 - val_loss: 7.2175e-04 - val_mae: 0.0285\n",
            "Epoch 18/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0013 - mae: 0.0320\n",
            "Epoch 18: val_mae improved from 0.02847 to 0.02810, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 0.0013 - mae: 0.0320 - val_loss: 7.0678e-04 - val_mae: 0.0281\n",
            "Epoch 19/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0013 - mae: 0.0317\n",
            "Epoch 19: val_mae improved from 0.02810 to 0.02781, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0013 - mae: 0.0317 - val_loss: 6.9469e-04 - val_mae: 0.0278\n",
            "Epoch 20/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0013 - mae: 0.0315\n",
            "Epoch 20: val_mae improved from 0.02781 to 0.02757, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0013 - mae: 0.0315 - val_loss: 6.8484e-04 - val_mae: 0.0276\n",
            "Epoch 21/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0012 - mae: 0.0297\n",
            "Epoch 21: val_mae improved from 0.02757 to 0.02739, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0013 - mae: 0.0312 - val_loss: 6.7681e-04 - val_mae: 0.0274\n",
            "Epoch 22/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0012 - mae: 0.0295\n",
            "Epoch 22: val_mae improved from 0.02739 to 0.02724, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0013 - mae: 0.0310 - val_loss: 6.7043e-04 - val_mae: 0.0272\n",
            "Epoch 23/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0012 - mae: 0.0294\n",
            "Epoch 23: val_mae improved from 0.02724 to 0.02715, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0013 - mae: 0.0309 - val_loss: 6.6558e-04 - val_mae: 0.0271\n",
            "Epoch 24/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0013 - mae: 0.0307\n",
            "Epoch 24: val_mae improved from 0.02715 to 0.02707, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 0.0013 - mae: 0.0307 - val_loss: 6.6198e-04 - val_mae: 0.0271\n",
            "Epoch 25/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0012 - mae: 0.0306\n",
            "Epoch 25: val_mae improved from 0.02707 to 0.02698, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0012 - mae: 0.0306 - val_loss: 6.5763e-04 - val_mae: 0.0270\n",
            "Epoch 26/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0012 - mae: 0.0300\n",
            "Epoch 26: val_mae improved from 0.02698 to 0.02693, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 50ms/step - loss: 0.0012 - mae: 0.0305 - val_loss: 6.5474e-04 - val_mae: 0.0269\n",
            "Epoch 27/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0012 - mae: 0.0304\n",
            "Epoch 27: val_mae improved from 0.02693 to 0.02689, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 3s 83ms/step - loss: 0.0012 - mae: 0.0304 - val_loss: 6.5243e-04 - val_mae: 0.0269\n",
            "Epoch 28/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0012 - mae: 0.0298\n",
            "Epoch 28: val_mae improved from 0.02689 to 0.02687, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 70ms/step - loss: 0.0012 - mae: 0.0302 - val_loss: 6.5072e-04 - val_mae: 0.0269\n",
            "Epoch 29/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0012 - mae: 0.0301\n",
            "Epoch 29: val_mae improved from 0.02687 to 0.02685, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 38ms/step - loss: 0.0012 - mae: 0.0301 - val_loss: 6.4916e-04 - val_mae: 0.0269\n",
            "Epoch 30/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0012 - mae: 0.0301\n",
            "Epoch 30: val_mae did not improve from 0.02685\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0012 - mae: 0.0301 - val_loss: 6.4831e-04 - val_mae: 0.0269\n",
            "Epoch 31/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0012 - mae: 0.0300\n",
            "Epoch 31: val_mae improved from 0.02685 to 0.02685, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0012 - mae: 0.0300 - val_loss: 6.4728e-04 - val_mae: 0.0269\n",
            "Epoch 32/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0011 - mae: 0.0283\n",
            "Epoch 32: val_mae did not improve from 0.02685\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0012 - mae: 0.0299 - val_loss: 6.4643e-04 - val_mae: 0.0269\n",
            "Epoch 33/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0012 - mae: 0.0298\n",
            "Epoch 33: val_mae did not improve from 0.02685\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0012 - mae: 0.0298 - val_loss: 6.4605e-04 - val_mae: 0.0269\n",
            "Epoch 34/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0011 - mae: 0.0281\n",
            "Epoch 34: val_mae did not improve from 0.02685\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0012 - mae: 0.0298 - val_loss: 6.4588e-04 - val_mae: 0.0269\n",
            "Epoch 35/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0011 - mae: 0.0281\n",
            "Epoch 35: val_mae did not improve from 0.02685\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0012 - mae: 0.0297 - val_loss: 6.4509e-04 - val_mae: 0.0269\n",
            "Epoch 36/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0012 - mae: 0.0291\n",
            "Epoch 36: val_mae did not improve from 0.02685\n",
            "35/35 [==============================] - 2s 53ms/step - loss: 0.0012 - mae: 0.0296 - val_loss: 6.4409e-04 - val_mae: 0.0269\n",
            "Epoch 37/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0012 - mae: 0.0290\n",
            "Epoch 37: val_mae did not improve from 0.02685\n",
            "35/35 [==============================] - 2s 57ms/step - loss: 0.0012 - mae: 0.0295 - val_loss: 6.4348e-04 - val_mae: 0.0269\n",
            "Epoch 38/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0012 - mae: 0.0290\n",
            "Epoch 38: val_mae did not improve from 0.02685\n",
            "35/35 [==============================] - 2s 56ms/step - loss: 0.0012 - mae: 0.0295 - val_loss: 6.4308e-04 - val_mae: 0.0269\n",
            "Epoch 39/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0012 - mae: 0.0289\n",
            "Epoch 39: val_mae did not improve from 0.02685\n",
            "35/35 [==============================] - 3s 81ms/step - loss: 0.0012 - mae: 0.0294 - val_loss: 6.4277e-04 - val_mae: 0.0270\n",
            "Epoch 40/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0012 - mae: 0.0293\n",
            "Epoch 40: val_mae did not improve from 0.02685\n",
            "35/35 [==============================] - 2s 45ms/step - loss: 0.0012 - mae: 0.0293 - val_loss: 6.4245e-04 - val_mae: 0.0270\n",
            "Epoch 41/100\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0011 - mae: 0.0276\n",
            "Epoch 41: val_mae did not improve from 0.02685\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0012 - mae: 0.0293 - val_loss: 6.4197e-04 - val_mae: 0.0270\n"
          ]
        }
      ],
      "source": [
        "#release\n",
        "# ==============================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative to its\n",
        "# difficulty. So your Category 1 question will score significantly less than\n",
        "# your Category 5 question.\n",
        "#\n",
        "# WARNING: Do not use lambda layers in your model, they are not supported\n",
        "# on the grading infrastructure. You do not need them to solve the question.\n",
        "#\n",
        "# WARNING: If you are using the GRU layer, it is advised not to use the\n",
        "# recurrent_dropout argument (you can alternatively set it to 0),\n",
        "# since it has not been implemented in the cuDNN kernel and may\n",
        "# result in much longer training times.\n",
        "#\n",
        "# WARNING: Input and output shape requirements are laid down in the section \n",
        "# 'INSTRUCTIONS' below and also reiterated in code comments. \n",
        "# Please read them thoroughly. After submitting the trained model for scoring, \n",
        "# if you are receiving a score of 0 or an error, please recheck the input and \n",
        "# output shapes of the model to see if it exactly matches our requirements. \n",
        "# Grading infrastrcuture is very strict about the shape requirements. Most common \n",
        "# issues occur when the shapes are not matching our expectations.\n",
        "#\n",
        "# TIP: You can print the output of model.summary() to review the model\n",
        "# architecture, input and output shapes of each layer.\n",
        "# If you have made sure that you have matched the shape requirements\n",
        "# and all the other instructions we have laid down, and still\n",
        "# receive a bad score, you must work on improving your model.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ==============================================================================\n",
        "#\n",
        "# TIME SERIES QUESTION\n",
        "#\n",
        "# Build and train a neural network to predict the time indexed variable of\n",
        "# the univariate US diesel prices (On - Highway) All types for the period of\n",
        "# 1994 - 2021.\n",
        "# Using a window of past 10 observations of 1 feature , train the model\n",
        "# to predict the next 10 observations of that feature.\n",
        "#\n",
        "# ==============================================================================\n",
        "#\n",
        "# ABOUT THE DATASET\n",
        "#\n",
        "# Original Source:\n",
        "# https://www.eia.gov/dnav/pet/pet_pri_gnd_dcus_nus_w.htm#\n",
        "#\n",
        "# For the purpose of the examination we have used the Diesel (On - Highway) -\n",
        "# All Types time series data for the period of 1994 - 2021 from the\n",
        "# aforementioned link. The dataset has 1 time indexed feature.\n",
        "# We have provided a cleaned version of the data.\n",
        "#\n",
        "# ==============================================================================\n",
        "#\n",
        "# INSTRUCTIONS\n",
        "#\n",
        "# Complete the code in following functions:\n",
        "# 1. solution_model()\n",
        "#\n",
        "# You may receive a score of 0 or your code will fail to be graded if the \n",
        "# following criteria are not met:\n",
        "#\n",
        "# 1. Model input shape must be (BATCH_SIZE, N_PAST = 10, N_FEATURES = 1),\n",
        "#    since the testing infrastructure expects a window of past N_PAST = 10\n",
        "#    observations of the 1 feature to predict the next N_FUTURE = 10\n",
        "#    observations of the same feature.\n",
        "#\n",
        "# 2. Model output shape must be (BATCH_SIZE, N_FUTURE = 10, N_FEATURES = 1)\n",
        "#\n",
        "# 3. The last layer of your model must be a Dense layer with 1 neuron since\n",
        "#    the model is expected to predict observations of 1 feature.\n",
        "#\n",
        "# 4. Don't change the values of the following constants:\n",
        "#    SPLIT_TIME, N_FEATURES, BATCH_SIZE, N_PAST, N_FUTURE, SHIFT, in\n",
        "#    solution_model() (See code for additional note on BATCH_SIZE).\n",
        "#\n",
        "# 5. Code for normalizing the data is provided - don't change it.\n",
        "#    Changing the normalizing code will affect your score.\n",
        "#\n",
        "# 6. Code for converting the dataset into windows is provided - don't change it.\n",
        "#    Changing the windowing code will affect your score.\n",
        "#\n",
        "# 7. Code for setting the seed is provided - don't change it.\n",
        "#\n",
        "# Make sure that the model architecture and input, output shapes match our\n",
        "# requirements by printing model.summary() and reviewing its output.\n",
        "#\n",
        "# HINT: If you follow all the rules mentioned above and throughout this\n",
        "# question while training your neural network, there is a possibility that a\n",
        "# validation MAE of approximately 0.02 or less on the normalized validation\n",
        "# dataset may fetch you top marks.\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Bidirectional, BatchNormalization, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "\n",
        "# This function normalizes the dataset using min max scaling.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def normalize_series(data, min, max):\n",
        "    data = data - min\n",
        "    data = data / max\n",
        "    return data\n",
        "\n",
        "\n",
        "# This function is used to map the time series dataset into windows of\n",
        "# features and respective targets, to prepare it for training and validation.\n",
        "# The first element of the first window will be the first element of\n",
        "# the dataset.\n",
        "#\n",
        "# Consecutive windows are constructed by shifting the starting position\n",
        "# of the first window forward, one at a time (indicated by shift=1).\n",
        "#\n",
        "# For a window of n_past number of observations of the time\n",
        "# indexed variable in the dataset, the target for the window is the next\n",
        "# n_future number of observations of the variable, after the\n",
        "# end of the window.\n",
        "\n",
        "# DO NOT CHANGE THIS.\n",
        "def windowed_dataset(series, batch_size, n_past=10, n_future=10, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "# This function loads the data from the CSV file, normalizes the data and\n",
        "# splits the dataset into train and validation data. It also uses\n",
        "# windowed_dataset() to split the data into windows of observations and\n",
        "# targets. Finally it defines, compiles and trains a neural network. This\n",
        "# function returns the final trained model.\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION\n",
        "def solution_model():\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # Reads the dataset.\n",
        "    df = pd.read_csv('/content/drive/MyDrive/WISET/deep_learning/5. diesel/Weekly_U.S.Diesel_Retail_Prices.csv',\n",
        "                     infer_datetime_format=True, index_col='Week of', header=0)\n",
        "\n",
        "    # Number of features in the dataset. We use all features as predictors to\n",
        "    # predict all features of future time steps.\n",
        "    N_FEATURES = len(df.columns) # DO NOT CHANGE THIS\n",
        "\n",
        "    # Normalizes the data\n",
        "    data = df.values\n",
        "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        "\n",
        "    # Splits the data into training and validation sets.\n",
        "    SPLIT_TIME = int(len(data) * 0.8) # DO NOT CHANGE THIS\n",
        "    x_train = data[:SPLIT_TIME]\n",
        "    x_valid = data[SPLIT_TIME:]\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    # DO NOT CHANGE BATCH_SIZE IF YOU ARE USING STATEFUL LSTM/RNN/GRU.\n",
        "    # THE TEST WILL FAIL TO GRADE YOUR SCORE IN SUCH CASES.\n",
        "    # In other cases, it is advised not to change the batch size since it\n",
        "    # might affect your final scores. While setting it to a lower size\n",
        "    # might not do any harm, higher sizes might affect your scores.\n",
        "    BATCH_SIZE = 32  # ADVISED NOT TO CHANGE THIS\n",
        "\n",
        "    # DO NOT CHANGE N_PAST, N_FUTURE, SHIFT. The tests will fail to run\n",
        "    # on the server.\n",
        "    # Number of past time steps based on which future observations should be\n",
        "    # predicted\n",
        "    N_PAST = 10  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Number of future time steps which are to be predicted.\n",
        "    N_FUTURE = 10  # DO NOT CHANGE THIS\n",
        "\n",
        "    # By how many positions the window slides to create a new window\n",
        "    # of observations.\n",
        "    SHIFT = 1  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Code to create windowed train and validation datasets.\n",
        "    train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "\n",
        "    # Code to define your model.\n",
        "    model = tf.keras.models.Sequential([\n",
        "\n",
        "        # ADD YOUR LAYERS HERE.\n",
        "        Conv1D(filters=32, kernel_size=5, padding='causal', activation='relu', input_shape=[N_PAST, 1]),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(16, activation='relu'),\n",
        "        # If you don't follow the instructions in the following comments,\n",
        "        # tests will fail to grade your code:\n",
        "        # The input layer of your model must have an input shape of:\n",
        "        # (BATCH_SIZE, N_PAST = 10, N_FEATURES = 1)\n",
        "        # The model must have an output shape of:\n",
        "        # (BATCH_SIZE, N_FUTURE = 10, N_FEATURES = 1).\n",
        "        # Make sure that there are N_FEATURES = 1 neurons in the final dense\n",
        "        # layer since the model predicts 1 feature.\n",
        "\n",
        "        # HINT: Bidirectional LSTMs may help boost your score. This is only a\n",
        "        # suggestion.\n",
        "\n",
        "        # WARNING: After submitting the trained model for scoring, if you are\n",
        "        # receiving a score of 0 or an error, please recheck the input and \n",
        "        # output shapes of the model to see if it exactly matches our requirements. \n",
        "        # The grading infrastructure is very strict about the shape requirements. \n",
        "        # Most common issues occur when the shapes are not matching our \n",
        "        # expectations.\n",
        "        #\n",
        "        # TIP: You can print the output of model.summary() to review the model\n",
        "        # architecture, input and output shapes of each layer.\n",
        "        # If you have made sure that you have matched the shape requirements\n",
        "        # and all the other instructions we have laid down, and still\n",
        "        # receive a bad score, you must work on improving your model.\n",
        "\n",
        "        # WARNING: If you are using the GRU layer, it is advised not to use the\n",
        "        # recurrent_dropout argument (you can alternatively set it to 0),\n",
        "        # since it has not been implemented in the cuDNN kernel and may\n",
        "        # result in much longer training times.\n",
        "        tf.keras.layers.Dense(N_FEATURES)\n",
        "    ])\n",
        "\n",
        "    # Code to train and compile the model\n",
        "    checkpoint_path='model/my_checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path,\n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True,\n",
        "                             monitor='val_mae',\n",
        "                             verbose=1,\n",
        "                             )\n",
        "    early_stopping = EarlyStopping(monitor='val_mae', patience=10)\n",
        "   \n",
        "    optimizer = tf.keras.optimizers.Adam(0.0001)\n",
        "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
        "\n",
        "    model.fit(train_set,\n",
        "              validation_data=(valid_set),\n",
        "              epochs=100,\n",
        "              callbacks=[checkpoint, early_stopping])\n",
        "\n",
        "    model.load_weights(checkpoint_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n",
        "\n",
        "\n",
        "# THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL\n",
        "# BEFORE UPLOADING YOU CAN DO IT WITH THIS\n",
        "\n",
        "#def model_forecast(model, series, window_size, batch_size):\n",
        "#    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "#    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "#    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "#    ds = ds.batch(batch_size, drop_remainder=True).prefetch(1)\n",
        "#    forecast = model.predict(ds)\n",
        "#    return forecast\n",
        "\n",
        "# PASS THE NORMALIZED data IN THE FOLLOWING CODE\n",
        "\n",
        "# rnn_forecast = model_forecast(model, data, N_PAST, BATCH_SIZE)\n",
        "# rnn_forecast = rnn_forecast[SPLIT_TIME - N_PAST:-1, 0, 0]\n",
        "\n",
        "# x_valid = np.squeeze(x_valid[:rnn_forecast.shape[0]])\n",
        "# result = tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT\n",
        "\n",
        "Q) 다음 코드의 성능을 개선해 줘.\n",
        "사용하는 데이터는 float 형의 시계열 데이터이고\n",
        "다음 조건을 적용해 줘.\n",
        "Use dropout\n",
        "Use batch normalization\n",
        "Use checkpoint with save best only, and save weight only and verbose = 1\n",
        "Use early stopping with patience = 10\n",
        "Use reduce learning rate on plateau\n",
        "set verbose = 1\n",
        "Set epochs = 100\n",
        "do not use data augmentation\n",
        "Use other methods to make val loss value lower.\n",
        "\n",
        "A) 여러 방법을 사용하여 모델 성능을 개선할 수 있습니다. 제안된 조건을 따르면서 다음과 같은 방법을 시도할 수 있습니다.\n",
        "\n",
        "Batch Normalization: 모델의 안정성을 높일 수 있는 Batch Normalization 레이어를 추가합니다.\n",
        "Dropout: 과적합을 방지하기 위해 Dropout 레이어를 추가합니다.\n",
        "Learning Rate Schedule: ReduceLROnPlateau 콜백을 사용하여 학습률을 조정합니다.\n",
        "Optimizer: 모델의 최적화 방법으로 Adam optimizer를 사용합니다."
      ],
      "metadata": {
        "id": "h0PQngo-vokj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#release\n",
        "# ==============================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative to its\n",
        "# difficulty. So your Category 1 question will score significantly less than\n",
        "# your Category 5 question.\n",
        "#\n",
        "# WARNING: Do not use lambda layers in your model, they are not supported\n",
        "# on the grading infrastructure. You do not need them to solve the question.\n",
        "#\n",
        "# WARNING: If you are using the GRU layer, it is advised not to use the\n",
        "# recurrent_dropout argument (you can alternatively set it to 0),\n",
        "# since it has not been implemented in the cuDNN kernel and may\n",
        "# result in much longer training times.\n",
        "#\n",
        "# WARNING: Input and output shape requirements are laid down in the section \n",
        "# 'INSTRUCTIONS' below and also reiterated in code comments. \n",
        "# Please read them thoroughly. After submitting the trained model for scoring, \n",
        "# if you are receiving a score of 0 or an error, please recheck the input and \n",
        "# output shapes of the model to see if it exactly matches our requirements. \n",
        "# Grading infrastrcuture is very strict about the shape requirements. Most common \n",
        "# issues occur when the shapes are not matching our expectations.\n",
        "#\n",
        "# TIP: You can print the output of model.summary() to review the model\n",
        "# architecture, input and output shapes of each layer.\n",
        "# If you have made sure that you have matched the shape requirements\n",
        "# and all the other instructions we have laid down, and still\n",
        "# receive a bad score, you must work on improving your model.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ==============================================================================\n",
        "#\n",
        "# TIME SERIES QUESTION\n",
        "#\n",
        "# Build and train a neural network to predict the time indexed variable of\n",
        "# the univariate US diesel prices (On - Highway) All types for the period of\n",
        "# 1994 - 2021.\n",
        "# Using a window of past 10 observations of 1 feature , train the model\n",
        "# to predict the next 10 observations of that feature.\n",
        "#\n",
        "# ==============================================================================\n",
        "#\n",
        "# ABOUT THE DATASET\n",
        "#\n",
        "# Original Source:\n",
        "# https://www.eia.gov/dnav/pet/pet_pri_gnd_dcus_nus_w.htm#\n",
        "#\n",
        "# For the purpose of the examination we have used the Diesel (On - Highway) -\n",
        "# All Types time series data for the period of 1994 - 2021 from the\n",
        "# aforementioned link. The dataset has 1 time indexed feature.\n",
        "# We have provided a cleaned version of the data.\n",
        "#\n",
        "# ==============================================================================\n",
        "#\n",
        "# INSTRUCTIONS\n",
        "#\n",
        "# Complete the code in following functions:\n",
        "# 1. solution_model()\n",
        "#\n",
        "# You may receive a score of 0 or your code will fail to be graded if the \n",
        "# following criteria are not met:\n",
        "#\n",
        "# 1. Model input shape must be (BATCH_SIZE, N_PAST = 10, N_FEATURES = 1),\n",
        "#    since the testing infrastructure expects a window of past N_PAST = 10\n",
        "#    observations of the 1 feature to predict the next N_FUTURE = 10\n",
        "#    observations of the same feature.\n",
        "#\n",
        "# 2. Model output shape must be (BATCH_SIZE, N_FUTURE = 10, N_FEATURES = 1)\n",
        "#\n",
        "# 3. The last layer of your model must be a Dense layer with 1 neuron since\n",
        "#    the model is expected to predict observations of 1 feature.\n",
        "#\n",
        "# 4. Don't change the values of the following constants:\n",
        "#    SPLIT_TIME, N_FEATURES, BATCH_SIZE, N_PAST, N_FUTURE, SHIFT, in\n",
        "#    solution_model() (See code for additional note on BATCH_SIZE).\n",
        "#\n",
        "# 5. Code for normalizing the data is provided - don't change it.\n",
        "#    Changing the normalizing code will affect your score.\n",
        "#\n",
        "# 6. Code for converting the dataset into windows is provided - don't change it.\n",
        "#    Changing the windowing code will affect your score.\n",
        "#\n",
        "# 7. Code for setting the seed is provided - don't change it.\n",
        "#\n",
        "# Make sure that the model architecture and input, output shapes match our\n",
        "# requirements by printing model.summary() and reviewing its output.\n",
        "#\n",
        "# HINT: If you follow all the rules mentioned above and throughout this\n",
        "# question while training your neural network, there is a possibility that a\n",
        "# validation MAE of approximately 0.02 or less on the normalized validation\n",
        "# dataset may fetch you top marks.\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Bidirectional, BatchNormalization, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "# This function normalizes the dataset using min max scaling.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def normalize_series(data, min, max):\n",
        "    data = data - min\n",
        "    data = data / max\n",
        "    return data\n",
        "\n",
        "\n",
        "# This function is used to map the time series dataset into windows of\n",
        "# features and respective targets, to prepare it for training and validation.\n",
        "# The first element of the first window will be the first element of\n",
        "# the dataset.\n",
        "#\n",
        "# Consecutive windows are constructed by shifting the starting position\n",
        "# of the first window forward, one at a time (indicated by shift=1).\n",
        "#\n",
        "# For a window of n_past number of observations of the time\n",
        "# indexed variable in the dataset, the target for the window is the next\n",
        "# n_future number of observations of the variable, after the\n",
        "# end of the window.\n",
        "\n",
        "# DO NOT CHANGE THIS.\n",
        "def windowed_dataset(series, batch_size, n_past=10, n_future=10, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "# This function loads the data from the CSV file, normalizes the data and\n",
        "# splits the dataset into train and validation data. It also uses\n",
        "# windowed_dataset() to split the data into windows of observations and\n",
        "# targets. Finally it defines, compiles and trains a neural network. This\n",
        "# function returns the final trained model.\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION\n",
        "def solution_model():\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # Reads the dataset.\n",
        "    df = pd.read_csv('/content/drive/MyDrive/TF_WISET/Weekly_U.S.Diesel_Retail_Prices.csv',\n",
        "                     infer_datetime_format=True, index_col='Week of', header=0)\n",
        "\n",
        "    # Number of features in the dataset. We use all features as predictors to\n",
        "    # predict all features of future time steps.\n",
        "    N_FEATURES = len(df.columns) # DO NOT CHANGE THIS\n",
        "\n",
        "    # Normalizes the data\n",
        "    data = df.values\n",
        "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        "\n",
        "    # Splits the data into training and validation sets.\n",
        "    SPLIT_TIME = int(len(data) * 0.8) # DO NOT CHANGE THIS\n",
        "    x_train = data[:SPLIT_TIME]\n",
        "    x_valid = data[SPLIT_TIME:]\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    # DO NOT CHANGE BATCH_SIZE IF YOU ARE USING STATEFUL LSTM/RNN/GRU.\n",
        "    # THE TEST WILL FAIL TO GRADE YOUR SCORE IN SUCH CASES.\n",
        "    # In other cases, it is advised not to change the batch size since it\n",
        "    # might affect your final scores. While setting it to a lower size\n",
        "    # might not do any harm, higher sizes might affect your scores.\n",
        "    BATCH_SIZE = 32  # ADVISED NOT TO CHANGE THIS\n",
        "\n",
        "    # DO NOT CHANGE N_PAST, N_FUTURE, SHIFT. The tests will fail to run\n",
        "    # on the server.\n",
        "    # Number of past time steps based on which future observations should be\n",
        "    # predicted\n",
        "    N_PAST = 10  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Number of future time steps which are to be predicted.\n",
        "    N_FUTURE = 10  # DO NOT CHANGE THIS\n",
        "\n",
        "    # By how many positions the window slides to create a new window\n",
        "    # of observations.\n",
        "    SHIFT = 1  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Code to create windowed train and validation datasets.\n",
        "    train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "\n",
        "    # Code to define your model.\n",
        "    model = tf.keras.models.Sequential([\n",
        "        Conv1D(filters=32, kernel_size=5, padding='causal', activation='relu', input_shape=[N_PAST, 1]),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(N_FEATURES)\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
        "\n",
        "    # Define callbacks\n",
        "    checkpoint_path = 'model/my_checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path, save_weights_only=True, save_best_only=True, monitor='val_mae', verbose=1)\n",
        "    early_stopping = EarlyStopping(monitor='val_mae', patience=10)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_mae', factor=0.5, patience=5, verbose=1)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(train_set, epochs=100, validation_data=valid_set, callbacks=[checkpoint, early_stopping, reduce_lr], verbose=1)\n",
        "\n",
        "    model.load_weights(checkpoint_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAQdL5qSwrCO",
        "outputId": "b8e6467e-c978-422a-cfd3-82e8aa21eac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "     35/Unknown - 13s 31ms/step - loss: 0.7551 - mae: 0.6841\n",
            "Epoch 1: val_mae improved from inf to 0.48094, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 15s 93ms/step - loss: 0.7551 - mae: 0.6841 - val_loss: 0.2361 - val_mae: 0.4809 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.3979 - mae: 0.4845\n",
            "Epoch 2: val_mae did not improve from 0.48094\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 0.3979 - mae: 0.4845 - val_loss: 0.2945 - val_mae: 0.5380 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2880 - mae: 0.4216\n",
            "Epoch 3: val_mae did not improve from 0.48094\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 0.2880 - mae: 0.4216 - val_loss: 0.3413 - val_mae: 0.5798 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2294 - mae: 0.3754\n",
            "Epoch 4: val_mae did not improve from 0.48094\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.2294 - mae: 0.3754 - val_loss: 0.3647 - val_mae: 0.5994 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1981 - mae: 0.3496\n",
            "Epoch 5: val_mae did not improve from 0.48094\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.1981 - mae: 0.3496 - val_loss: 0.3688 - val_mae: 0.6025 - lr: 1.0000e-04\n",
            "Epoch 6/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1686 - mae: 0.3207\n",
            "Epoch 6: val_mae did not improve from 0.48094\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "35/35 [==============================] - 2s 70ms/step - loss: 0.1686 - mae: 0.3207 - val_loss: 0.3769 - val_mae: 0.6082 - lr: 1.0000e-04\n",
            "Epoch 7/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1491 - mae: 0.3024\n",
            "Epoch 7: val_mae did not improve from 0.48094\n",
            "35/35 [==============================] - 3s 69ms/step - loss: 0.1491 - mae: 0.3024 - val_loss: 0.4046 - val_mae: 0.6288 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1416 - mae: 0.2936\n",
            "Epoch 8: val_mae did not improve from 0.48094\n",
            "35/35 [==============================] - 2s 68ms/step - loss: 0.1416 - mae: 0.2936 - val_loss: 0.4295 - val_mae: 0.6461 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1297 - mae: 0.2826\n",
            "Epoch 9: val_mae did not improve from 0.48094\n",
            "35/35 [==============================] - 2s 56ms/step - loss: 0.1297 - mae: 0.2826 - val_loss: 0.4459 - val_mae: 0.6556 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.1243 - mae: 0.2749\n",
            "Epoch 10: val_mae did not improve from 0.48094\n",
            "35/35 [==============================] - 2s 60ms/step - loss: 0.1238 - mae: 0.2744 - val_loss: 0.4507 - val_mae: 0.6563 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1207 - mae: 0.2699\n",
            "Epoch 11: val_mae did not improve from 0.48094\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.1207 - mae: 0.2699 - val_loss: 0.4167 - val_mae: 0.6288 - lr: 5.0000e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 직접 코딩\n",
        "\n",
        "과제 순서를 바꿔서, ChatGPT 가 짜 준 코드를 직접 개선해 본다"
      ],
      "metadata": {
        "id": "jQG2ZTEXzDcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#release\n",
        "# ==============================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative to its\n",
        "# difficulty. So your Category 1 question will score significantly less than\n",
        "# your Category 5 question.\n",
        "#\n",
        "# WARNING: Do not use lambda layers in your model, they are not supported\n",
        "# on the grading infrastructure. You do not need them to solve the question.\n",
        "#\n",
        "# WARNING: If you are using the GRU layer, it is advised not to use the\n",
        "# recurrent_dropout argument (you can alternatively set it to 0),\n",
        "# since it has not been implemented in the cuDNN kernel and may\n",
        "# result in much longer training times.\n",
        "#\n",
        "# WARNING: Input and output shape requirements are laid down in the section \n",
        "# 'INSTRUCTIONS' below and also reiterated in code comments. \n",
        "# Please read them thoroughly. After submitting the trained model for scoring, \n",
        "# if you are receiving a score of 0 or an error, please recheck the input and \n",
        "# output shapes of the model to see if it exactly matches our requirements. \n",
        "# Grading infrastrcuture is very strict about the shape requirements. Most common \n",
        "# issues occur when the shapes are not matching our expectations.\n",
        "#\n",
        "# TIP: You can print the output of model.summary() to review the model\n",
        "# architecture, input and output shapes of each layer.\n",
        "# If you have made sure that you have matched the shape requirements\n",
        "# and all the other instructions we have laid down, and still\n",
        "# receive a bad score, you must work on improving your model.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ==============================================================================\n",
        "#\n",
        "# TIME SERIES QUESTION\n",
        "#\n",
        "# Build and train a neural network to predict the time indexed variable of\n",
        "# the univariate US diesel prices (On - Highway) All types for the period of\n",
        "# 1994 - 2021.\n",
        "# Using a window of past 10 observations of 1 feature , train the model\n",
        "# to predict the next 10 observations of that feature.\n",
        "#\n",
        "# ==============================================================================\n",
        "#\n",
        "# ABOUT THE DATASET\n",
        "#\n",
        "# Original Source:\n",
        "# https://www.eia.gov/dnav/pet/pet_pri_gnd_dcus_nus_w.htm#\n",
        "#\n",
        "# For the purpose of the examination we have used the Diesel (On - Highway) -\n",
        "# All Types time series data for the period of 1994 - 2021 from the\n",
        "# aforementioned link. The dataset has 1 time indexed feature.\n",
        "# We have provided a cleaned version of the data.\n",
        "#\n",
        "# ==============================================================================\n",
        "#\n",
        "# INSTRUCTIONS\n",
        "#\n",
        "# Complete the code in following functions:\n",
        "# 1. solution_model()\n",
        "#\n",
        "# You may receive a score of 0 or your code will fail to be graded if the \n",
        "# following criteria are not met:\n",
        "#\n",
        "# 1. Model input shape must be (BATCH_SIZE, N_PAST = 10, N_FEATURES = 1),\n",
        "#    since the testing infrastructure expects a window of past N_PAST = 10\n",
        "#    observations of the 1 feature to predict the next N_FUTURE = 10\n",
        "#    observations of the same feature.\n",
        "#\n",
        "# 2. Model output shape must be (BATCH_SIZE, N_FUTURE = 10, N_FEATURES = 1)\n",
        "#\n",
        "# 3. The last layer of your model must be a Dense layer with 1 neuron since\n",
        "#    the model is expected to predict observations of 1 feature.\n",
        "#\n",
        "# 4. Don't change the values of the following constants:\n",
        "#    SPLIT_TIME, N_FEATURES, BATCH_SIZE, N_PAST, N_FUTURE, SHIFT, in\n",
        "#    solution_model() (See code for additional note on BATCH_SIZE).\n",
        "#\n",
        "# 5. Code for normalizing the data is provided - don't change it.\n",
        "#    Changing the normalizing code will affect your score.\n",
        "#\n",
        "# 6. Code for converting the dataset into windows is provided - don't change it.\n",
        "#    Changing the windowing code will affect your score.\n",
        "#\n",
        "# 7. Code for setting the seed is provided - don't change it.\n",
        "#\n",
        "# Make sure that the model architecture and input, output shapes match our\n",
        "# requirements by printing model.summary() and reviewing its output.\n",
        "#\n",
        "# HINT: If you follow all the rules mentioned above and throughout this\n",
        "# question while training your neural network, there is a possibility that a\n",
        "# validation MAE of approximately 0.02 or less on the normalized validation\n",
        "# dataset may fetch you top marks.\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Bidirectional, BatchNormalization, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "# This function normalizes the dataset using min max scaling.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def normalize_series(data, min, max):\n",
        "    data = data - min\n",
        "    data = data / max\n",
        "    return data\n",
        "\n",
        "\n",
        "# This function is used to map the time series dataset into windows of\n",
        "# features and respective targets, to prepare it for training and validation.\n",
        "# The first element of the first window will be the first element of\n",
        "# the dataset.\n",
        "#\n",
        "# Consecutive windows are constructed by shifting the starting position\n",
        "# of the first window forward, one at a time (indicated by shift=1).\n",
        "#\n",
        "# For a window of n_past number of observations of the time\n",
        "# indexed variable in the dataset, the target for the window is the next\n",
        "# n_future number of observations of the variable, after the\n",
        "# end of the window.\n",
        "\n",
        "# DO NOT CHANGE THIS.\n",
        "def windowed_dataset(series, batch_size, n_past=10, n_future=10, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "# This function loads the data from the CSV file, normalizes the data and\n",
        "# splits the dataset into train and validation data. It also uses\n",
        "# windowed_dataset() to split the data into windows of observations and\n",
        "# targets. Finally it defines, compiles and trains a neural network. This\n",
        "# function returns the final trained model.\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION\n",
        "def solution_model():\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # Reads the dataset.\n",
        "    df = pd.read_csv('/content/drive/MyDrive/TF_WISET/Weekly_U.S.Diesel_Retail_Prices.csv',\n",
        "                     infer_datetime_format=True, index_col='Week of', header=0)\n",
        "\n",
        "    # Number of features in the dataset. We use all features as predictors to\n",
        "    # predict all features of future time steps.\n",
        "    N_FEATURES = len(df.columns) # DO NOT CHANGE THIS\n",
        "\n",
        "    # Normalizes the data\n",
        "    data = df.values\n",
        "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        "\n",
        "    # Splits the data into training and validation sets.\n",
        "    SPLIT_TIME = int(len(data) * 0.8) # DO NOT CHANGE THIS\n",
        "    x_train = data[:SPLIT_TIME]\n",
        "    x_valid = data[SPLIT_TIME:]\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    # DO NOT CHANGE BATCH_SIZE IF YOU ARE USING STATEFUL LSTM/RNN/GRU.\n",
        "    # THE TEST WILL FAIL TO GRADE YOUR SCORE IN SUCH CASES.\n",
        "    # In other cases, it is advised not to change the batch size since it\n",
        "    # might affect your final scores. While setting it to a lower size\n",
        "    # might not do any harm, higher sizes might affect your scores.\n",
        "    BATCH_SIZE = 32  # ADVISED NOT TO CHANGE THIS\n",
        "\n",
        "    # DO NOT CHANGE N_PAST, N_FUTURE, SHIFT. The tests will fail to run\n",
        "    # on the server.\n",
        "    # Number of past time steps based on which future observations should be\n",
        "    # predicted\n",
        "    N_PAST = 10  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Number of future time steps which are to be predicted.\n",
        "    N_FUTURE = 10  # DO NOT CHANGE THIS\n",
        "\n",
        "    # By how many positions the window slides to create a new window\n",
        "    # of observations.\n",
        "    SHIFT = 1  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Code to create windowed train and validation datasets.\n",
        "    train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "\n",
        "    # Code to define your model.\n",
        "    model = tf.keras.models.Sequential([\n",
        "        Conv1D(filters=32, kernel_size=5, padding='causal', activation='relu', input_shape=[N_PAST, 1]),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(N_FEATURES)\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    # optimizer = SGD 로 교체 시 val_mae 가 일정하지만 너무 느리게 감소함.\n",
        "    # optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.0001)\n",
        "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
        "\n",
        "    # Define callbacks\n",
        "    checkpoint_path = 'model/my_checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path, save_weights_only=True, save_best_only=True, monitor='val_mae', verbose=1)\n",
        "    early_stopping = EarlyStopping(monitor='val_mae', patience=10)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_mae', factor=0.5, patience=5, verbose=1)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(train_set, epochs=150, validation_data=valid_set, callbacks=[checkpoint, early_stopping, reduce_lr], verbose=1)\n",
        "\n",
        "    model.load_weights(checkpoint_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZC8ew0KzEuN",
        "outputId": "88012137-da41-4393-d738-4fc907749366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "     35/Unknown - 10s 46ms/step - loss: 0.1237 - mae: 0.2732\n",
            "Epoch 1: val_mae improved from inf to 0.37470, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 13s 118ms/step - loss: 0.1237 - mae: 0.2732 - val_loss: 0.1448 - val_mae: 0.3747 - lr: 1.0000e-04\n",
            "Epoch 2/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.1202 - mae: 0.2647\n",
            "Epoch 2: val_mae improved from 0.37470 to 0.36767, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 0.1199 - mae: 0.2666 - val_loss: 0.1396 - val_mae: 0.3677 - lr: 1.0000e-04\n",
            "Epoch 3/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.1166 - mae: 0.2583\n",
            "Epoch 3: val_mae improved from 0.36767 to 0.36072, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.1163 - mae: 0.2602 - val_loss: 0.1345 - val_mae: 0.3607 - lr: 1.0000e-04\n",
            "Epoch 4/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1128 - mae: 0.2539\n",
            "Epoch 4: val_mae improved from 0.36072 to 0.35384, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 0.1128 - mae: 0.2539 - val_loss: 0.1296 - val_mae: 0.3538 - lr: 1.0000e-04\n",
            "Epoch 5/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1093 - mae: 0.2479\n",
            "Epoch 5: val_mae improved from 0.35384 to 0.34718, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 31ms/step - loss: 0.1093 - mae: 0.2479 - val_loss: 0.1249 - val_mae: 0.3472 - lr: 1.0000e-04\n",
            "Epoch 6/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.1041 - mae: 0.2350\n",
            "Epoch 6: val_mae improved from 0.34718 to 0.34074, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 30ms/step - loss: 0.1061 - mae: 0.2422 - val_loss: 0.1204 - val_mae: 0.3407 - lr: 1.0000e-04\n",
            "Epoch 7/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.1034 - mae: 0.2355\n",
            "Epoch 7: val_mae improved from 0.34074 to 0.33445, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 31ms/step - loss: 0.1030 - mae: 0.2373 - val_loss: 0.1162 - val_mae: 0.3345 - lr: 1.0000e-04\n",
            "Epoch 8/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0983 - mae: 0.2259\n",
            "Epoch 8: val_mae improved from 0.33445 to 0.32827, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 31ms/step - loss: 0.1001 - mae: 0.2330 - val_loss: 0.1121 - val_mae: 0.3283 - lr: 1.0000e-04\n",
            "Epoch 9/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0977 - mae: 0.2275\n",
            "Epoch 9: val_mae improved from 0.32827 to 0.32219, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 0.0973 - mae: 0.2291 - val_loss: 0.1081 - val_mae: 0.3222 - lr: 1.0000e-04\n",
            "Epoch 10/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0951 - mae: 0.2241\n",
            "Epoch 10: val_mae improved from 0.32219 to 0.31618, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 55ms/step - loss: 0.0947 - mae: 0.2256 - val_loss: 0.1043 - val_mae: 0.3162 - lr: 1.0000e-04\n",
            "Epoch 11/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0926 - mae: 0.2209\n",
            "Epoch 11: val_mae improved from 0.31618 to 0.31027, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 0.0921 - mae: 0.2224 - val_loss: 0.1006 - val_mae: 0.3103 - lr: 1.0000e-04\n",
            "Epoch 12/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0883 - mae: 0.2129\n",
            "Epoch 12: val_mae improved from 0.31027 to 0.30448, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0897 - mae: 0.2193 - val_loss: 0.0970 - val_mae: 0.3045 - lr: 1.0000e-04\n",
            "Epoch 13/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0879 - mae: 0.2151\n",
            "Epoch 13: val_mae improved from 0.30448 to 0.29881, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 0.0874 - mae: 0.2165 - val_loss: 0.0936 - val_mae: 0.2988 - lr: 1.0000e-04\n",
            "Epoch 14/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0839 - mae: 0.2079\n",
            "Epoch 14: val_mae improved from 0.29881 to 0.29326, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0851 - mae: 0.2140 - val_loss: 0.0903 - val_mae: 0.2933 - lr: 1.0000e-04\n",
            "Epoch 15/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0830 - mae: 0.2117\n",
            "Epoch 15: val_mae improved from 0.29326 to 0.28785, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0830 - mae: 0.2117 - val_loss: 0.0871 - val_mae: 0.2879 - lr: 1.0000e-04\n",
            "Epoch 16/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0810 - mae: 0.2096\n",
            "Epoch 16: val_mae improved from 0.28785 to 0.28259, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0810 - mae: 0.2096 - val_loss: 0.0841 - val_mae: 0.2826 - lr: 1.0000e-04\n",
            "Epoch 17/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0797 - mae: 0.2066\n",
            "Epoch 17: val_mae improved from 0.28259 to 0.27747, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0791 - mae: 0.2075 - val_loss: 0.0813 - val_mae: 0.2775 - lr: 1.0000e-04\n",
            "Epoch 18/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0779 - mae: 0.2049\n",
            "Epoch 18: val_mae improved from 0.27747 to 0.27249, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 0.0773 - mae: 0.2057 - val_loss: 0.0785 - val_mae: 0.2725 - lr: 1.0000e-04\n",
            "Epoch 19/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0756 - mae: 0.2042\n",
            "Epoch 19: val_mae improved from 0.27249 to 0.26765, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 57ms/step - loss: 0.0756 - mae: 0.2042 - val_loss: 0.0759 - val_mae: 0.2677 - lr: 1.0000e-04\n",
            "Epoch 20/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0746 - mae: 0.2022\n",
            "Epoch 20: val_mae improved from 0.26765 to 0.26295, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0740 - mae: 0.2028 - val_loss: 0.0734 - val_mae: 0.2629 - lr: 1.0000e-04\n",
            "Epoch 21/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0717 - mae: 0.1970\n",
            "Epoch 21: val_mae improved from 0.26295 to 0.25837, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0725 - mae: 0.2017 - val_loss: 0.0710 - val_mae: 0.2584 - lr: 1.0000e-04\n",
            "Epoch 22/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.2007\n",
            "Epoch 22: val_mae improved from 0.25837 to 0.25392, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 0.0710 - mae: 0.2007 - val_loss: 0.0687 - val_mae: 0.2539 - lr: 1.0000e-04\n",
            "Epoch 23/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0697 - mae: 0.1999\n",
            "Epoch 23: val_mae improved from 0.25392 to 0.24959, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0697 - mae: 0.1999 - val_loss: 0.0666 - val_mae: 0.2496 - lr: 1.0000e-04\n",
            "Epoch 24/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0691 - mae: 0.1988\n",
            "Epoch 24: val_mae improved from 0.24959 to 0.24538, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 0.0684 - mae: 0.1991 - val_loss: 0.0645 - val_mae: 0.2454 - lr: 1.0000e-04\n",
            "Epoch 25/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0679 - mae: 0.1982\n",
            "Epoch 25: val_mae improved from 0.24538 to 0.24129, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0672 - mae: 0.1984 - val_loss: 0.0625 - val_mae: 0.2413 - lr: 1.0000e-04\n",
            "Epoch 26/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0667 - mae: 0.1976\n",
            "Epoch 26: val_mae improved from 0.24129 to 0.23731, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 31ms/step - loss: 0.0660 - mae: 0.1977 - val_loss: 0.0606 - val_mae: 0.2373 - lr: 1.0000e-04\n",
            "Epoch 27/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0649 - mae: 0.1971\n",
            "Epoch 27: val_mae improved from 0.23731 to 0.23345, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 0.0649 - mae: 0.1971 - val_loss: 0.0587 - val_mae: 0.2334 - lr: 1.0000e-04\n",
            "Epoch 28/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0646 - mae: 0.1966\n",
            "Epoch 28: val_mae improved from 0.23345 to 0.22969, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 56ms/step - loss: 0.0638 - mae: 0.1965 - val_loss: 0.0570 - val_mae: 0.2297 - lr: 1.0000e-04\n",
            "Epoch 29/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0626 - mae: 0.1929\n",
            "Epoch 29: val_mae improved from 0.22969 to 0.22603, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0628 - mae: 0.1959 - val_loss: 0.0553 - val_mae: 0.2260 - lr: 1.0000e-04\n",
            "Epoch 30/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0619 - mae: 0.1954\n",
            "Epoch 30: val_mae improved from 0.22603 to 0.22247, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 31ms/step - loss: 0.0619 - mae: 0.1954 - val_loss: 0.0537 - val_mae: 0.2225 - lr: 1.0000e-04\n",
            "Epoch 31/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0618 - mae: 0.1953\n",
            "Epoch 31: val_mae improved from 0.22247 to 0.21902, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0610 - mae: 0.1950 - val_loss: 0.0522 - val_mae: 0.2190 - lr: 1.0000e-04\n",
            "Epoch 32/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0610 - mae: 0.1950\n",
            "Epoch 32: val_mae improved from 0.21902 to 0.21566, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0602 - mae: 0.1945 - val_loss: 0.0507 - val_mae: 0.2157 - lr: 1.0000e-04\n",
            "Epoch 33/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0594 - mae: 0.1941\n",
            "Epoch 33: val_mae improved from 0.21566 to 0.21240, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0594 - mae: 0.1941 - val_loss: 0.0493 - val_mae: 0.2124 - lr: 1.0000e-04\n",
            "Epoch 34/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0586 - mae: 0.1937\n",
            "Epoch 34: val_mae improved from 0.21240 to 0.20922, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0586 - mae: 0.1937 - val_loss: 0.0480 - val_mae: 0.2092 - lr: 1.0000e-04\n",
            "Epoch 35/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0587 - mae: 0.1941\n",
            "Epoch 35: val_mae improved from 0.20922 to 0.20614, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0579 - mae: 0.1934 - val_loss: 0.0467 - val_mae: 0.2061 - lr: 1.0000e-04\n",
            "Epoch 36/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0572 - mae: 0.1931\n",
            "Epoch 36: val_mae improved from 0.20614 to 0.20314, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 44ms/step - loss: 0.0572 - mae: 0.1931 - val_loss: 0.0454 - val_mae: 0.2031 - lr: 1.0000e-04\n",
            "Epoch 37/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0566 - mae: 0.1928\n",
            "Epoch 37: val_mae improved from 0.20314 to 0.20023, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 60ms/step - loss: 0.0566 - mae: 0.1928 - val_loss: 0.0443 - val_mae: 0.2002 - lr: 1.0000e-04\n",
            "Epoch 38/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0561 - mae: 0.1910\n",
            "Epoch 38: val_mae improved from 0.20023 to 0.19739, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 44ms/step - loss: 0.0559 - mae: 0.1925 - val_loss: 0.0431 - val_mae: 0.1974 - lr: 1.0000e-04\n",
            "Epoch 39/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0554 - mae: 0.1922\n",
            "Epoch 39: val_mae improved from 0.19739 to 0.19463, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 31ms/step - loss: 0.0554 - mae: 0.1922 - val_loss: 0.0420 - val_mae: 0.1946 - lr: 1.0000e-04\n",
            "Epoch 40/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0557 - mae: 0.1930\n",
            "Epoch 40: val_mae improved from 0.19463 to 0.19195, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 31ms/step - loss: 0.0548 - mae: 0.1919 - val_loss: 0.0410 - val_mae: 0.1919 - lr: 1.0000e-04\n",
            "Epoch 41/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0552 - mae: 0.1928\n",
            "Epoch 41: val_mae improved from 0.19195 to 0.18934, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0543 - mae: 0.1917 - val_loss: 0.0400 - val_mae: 0.1893 - lr: 1.0000e-04\n",
            "Epoch 42/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0540 - mae: 0.1905\n",
            "Epoch 42: val_mae improved from 0.18934 to 0.18680, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 0.0538 - mae: 0.1914 - val_loss: 0.0390 - val_mae: 0.1868 - lr: 1.0000e-04\n",
            "Epoch 43/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0536 - mae: 0.1904\n",
            "Epoch 43: val_mae improved from 0.18680 to 0.18433, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 0.0533 - mae: 0.1912 - val_loss: 0.0381 - val_mae: 0.1843 - lr: 1.0000e-04\n",
            "Epoch 44/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0537 - mae: 0.1923\n",
            "Epoch 44: val_mae improved from 0.18433 to 0.18193, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0528 - mae: 0.1909 - val_loss: 0.0372 - val_mae: 0.1819 - lr: 1.0000e-04\n",
            "Epoch 45/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0528 - mae: 0.1902\n",
            "Epoch 45: val_mae improved from 0.18193 to 0.17959, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0524 - mae: 0.1907 - val_loss: 0.0364 - val_mae: 0.1796 - lr: 1.0000e-04\n",
            "Epoch 46/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0529 - mae: 0.1920\n",
            "Epoch 46: val_mae improved from 0.17959 to 0.17731, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 59ms/step - loss: 0.0520 - mae: 0.1905 - val_loss: 0.0356 - val_mae: 0.1773 - lr: 1.0000e-04\n",
            "Epoch 47/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0516 - mae: 0.1903\n",
            "Epoch 47: val_mae improved from 0.17731 to 0.17510, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 55ms/step - loss: 0.0516 - mae: 0.1903 - val_loss: 0.0348 - val_mae: 0.1751 - lr: 1.0000e-04\n",
            "Epoch 48/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0517 - mae: 0.1899\n",
            "Epoch 48: val_mae improved from 0.17510 to 0.17295, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 0.0512 - mae: 0.1901 - val_loss: 0.0340 - val_mae: 0.1729 - lr: 1.0000e-04\n",
            "Epoch 49/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0514 - mae: 0.1898\n",
            "Epoch 49: val_mae improved from 0.17295 to 0.17085, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0509 - mae: 0.1899 - val_loss: 0.0333 - val_mae: 0.1709 - lr: 1.0000e-04\n",
            "Epoch 50/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0511 - mae: 0.1898\n",
            "Epoch 50: val_mae improved from 0.17085 to 0.16881, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0505 - mae: 0.1897 - val_loss: 0.0326 - val_mae: 0.1688 - lr: 1.0000e-04\n",
            "Epoch 51/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0502 - mae: 0.1895\n",
            "Epoch 51: val_mae improved from 0.16881 to 0.16683, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0502 - mae: 0.1895 - val_loss: 0.0319 - val_mae: 0.1668 - lr: 1.0000e-04\n",
            "Epoch 52/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0509 - mae: 0.1911\n",
            "Epoch 52: val_mae improved from 0.16683 to 0.16490, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 0.0499 - mae: 0.1893 - val_loss: 0.0313 - val_mae: 0.1649 - lr: 1.0000e-04\n",
            "Epoch 53/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0496 - mae: 0.1891\n",
            "Epoch 53: val_mae improved from 0.16490 to 0.16302, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0496 - mae: 0.1891 - val_loss: 0.0307 - val_mae: 0.1630 - lr: 1.0000e-04\n",
            "Epoch 54/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0493 - mae: 0.1889\n",
            "Epoch 54: val_mae improved from 0.16302 to 0.16120, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 0.0493 - mae: 0.1889 - val_loss: 0.0301 - val_mae: 0.1612 - lr: 1.0000e-04\n",
            "Epoch 55/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0500 - mae: 0.1907\n",
            "Epoch 55: val_mae improved from 0.16120 to 0.15942, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 51ms/step - loss: 0.0491 - mae: 0.1888 - val_loss: 0.0295 - val_mae: 0.1594 - lr: 1.0000e-04\n",
            "Epoch 56/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0498 - mae: 0.1906\n",
            "Epoch 56: val_mae improved from 0.15942 to 0.15770, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 61ms/step - loss: 0.0488 - mae: 0.1886 - val_loss: 0.0290 - val_mae: 0.1577 - lr: 1.0000e-04\n",
            "Epoch 57/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0486 - mae: 0.1885\n",
            "Epoch 57: val_mae improved from 0.15770 to 0.15601, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 48ms/step - loss: 0.0486 - mae: 0.1885 - val_loss: 0.0284 - val_mae: 0.1560 - lr: 1.0000e-04\n",
            "Epoch 58/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0493 - mae: 0.1904\n",
            "Epoch 58: val_mae improved from 0.15601 to 0.15438, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0483 - mae: 0.1883 - val_loss: 0.0279 - val_mae: 0.1544 - lr: 1.0000e-04\n",
            "Epoch 59/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0489 - mae: 0.1892\n",
            "Epoch 59: val_mae improved from 0.15438 to 0.15279, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0481 - mae: 0.1882 - val_loss: 0.0274 - val_mae: 0.1528 - lr: 1.0000e-04\n",
            "Epoch 60/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0479 - mae: 0.1880\n",
            "Epoch 60: val_mae improved from 0.15279 to 0.15124, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0479 - mae: 0.1880 - val_loss: 0.0270 - val_mae: 0.1512 - lr: 1.0000e-04\n",
            "Epoch 61/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0485 - mae: 0.1891\n",
            "Epoch 61: val_mae improved from 0.15124 to 0.14974, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0477 - mae: 0.1879 - val_loss: 0.0265 - val_mae: 0.1497 - lr: 1.0000e-04\n",
            "Epoch 62/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0483 - mae: 0.1890\n",
            "Epoch 62: val_mae improved from 0.14974 to 0.14827, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 0.0475 - mae: 0.1877 - val_loss: 0.0261 - val_mae: 0.1483 - lr: 1.0000e-04\n",
            "Epoch 63/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0481 - mae: 0.1890\n",
            "Epoch 63: val_mae improved from 0.14827 to 0.14685, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0473 - mae: 0.1876 - val_loss: 0.0256 - val_mae: 0.1468 - lr: 1.0000e-04\n",
            "Epoch 64/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0471 - mae: 0.1875\n",
            "Epoch 64: val_mae improved from 0.14685 to 0.14546, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0471 - mae: 0.1875 - val_loss: 0.0252 - val_mae: 0.1455 - lr: 1.0000e-04\n",
            "Epoch 65/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0480 - mae: 0.1897\n",
            "Epoch 65: val_mae improved from 0.14546 to 0.14411, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 38ms/step - loss: 0.0470 - mae: 0.1874 - val_loss: 0.0248 - val_mae: 0.1441 - lr: 1.0000e-04\n",
            "Epoch 66/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0468 - mae: 0.1872\n",
            "Epoch 66: val_mae improved from 0.14411 to 0.14280, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 60ms/step - loss: 0.0468 - mae: 0.1872 - val_loss: 0.0244 - val_mae: 0.1428 - lr: 1.0000e-04\n",
            "Epoch 67/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0467 - mae: 0.1871\n",
            "Epoch 67: val_mae improved from 0.14280 to 0.14152, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0467 - mae: 0.1871 - val_loss: 0.0241 - val_mae: 0.1415 - lr: 1.0000e-04\n",
            "Epoch 68/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0474 - mae: 0.1887\n",
            "Epoch 68: val_mae improved from 0.14152 to 0.14028, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0465 - mae: 0.1870 - val_loss: 0.0237 - val_mae: 0.1403 - lr: 1.0000e-04\n",
            "Epoch 69/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0464 - mae: 0.1869\n",
            "Epoch 69: val_mae improved from 0.14028 to 0.13907, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0464 - mae: 0.1869 - val_loss: 0.0234 - val_mae: 0.1391 - lr: 1.0000e-04\n",
            "Epoch 70/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0462 - mae: 0.1868\n",
            "Epoch 70: val_mae improved from 0.13907 to 0.13789, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0462 - mae: 0.1868 - val_loss: 0.0231 - val_mae: 0.1379 - lr: 1.0000e-04\n",
            "Epoch 71/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0470 - mae: 0.1886\n",
            "Epoch 71: val_mae improved from 0.13789 to 0.13674, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0461 - mae: 0.1867 - val_loss: 0.0227 - val_mae: 0.1367 - lr: 1.0000e-04\n",
            "Epoch 72/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0469 - mae: 0.1885\n",
            "Epoch 72: val_mae improved from 0.13674 to 0.13563, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0460 - mae: 0.1866 - val_loss: 0.0224 - val_mae: 0.1356 - lr: 1.0000e-04\n",
            "Epoch 73/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0468 - mae: 0.1885\n",
            "Epoch 73: val_mae improved from 0.13563 to 0.13454, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0458 - mae: 0.1864 - val_loss: 0.0221 - val_mae: 0.1345 - lr: 1.0000e-04\n",
            "Epoch 74/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0467 - mae: 0.1884\n",
            "Epoch 74: val_mae improved from 0.13454 to 0.13348, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0457 - mae: 0.1863 - val_loss: 0.0219 - val_mae: 0.1335 - lr: 1.0000e-04\n",
            "Epoch 75/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0466 - mae: 0.1889\n",
            "Epoch 75: val_mae improved from 0.13348 to 0.13246, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 54ms/step - loss: 0.0456 - mae: 0.1862 - val_loss: 0.0216 - val_mae: 0.1325 - lr: 1.0000e-04\n",
            "Epoch 76/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0455 - mae: 0.1861\n",
            "Epoch 76: val_mae improved from 0.13246 to 0.13146, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 0.0455 - mae: 0.1861 - val_loss: 0.0213 - val_mae: 0.1315 - lr: 1.0000e-04\n",
            "Epoch 77/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0464 - mae: 0.1888\n",
            "Epoch 77: val_mae improved from 0.13146 to 0.13048, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0454 - mae: 0.1860 - val_loss: 0.0211 - val_mae: 0.1305 - lr: 1.0000e-04\n",
            "Epoch 78/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0463 - mae: 0.1883\n",
            "Epoch 78: val_mae improved from 0.13048 to 0.12954, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0453 - mae: 0.1859 - val_loss: 0.0208 - val_mae: 0.1295 - lr: 1.0000e-04\n",
            "Epoch 79/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0462 - mae: 0.1886\n",
            "Epoch 79: val_mae improved from 0.12954 to 0.12862, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0452 - mae: 0.1858 - val_loss: 0.0206 - val_mae: 0.1286 - lr: 1.0000e-04\n",
            "Epoch 80/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0461 - mae: 0.1882\n",
            "Epoch 80: val_mae improved from 0.12862 to 0.12772, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 0.0451 - mae: 0.1857 - val_loss: 0.0203 - val_mae: 0.1277 - lr: 1.0000e-04\n",
            "Epoch 81/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0461 - mae: 0.1881\n",
            "Epoch 81: val_mae improved from 0.12772 to 0.12685, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 0.0450 - mae: 0.1856 - val_loss: 0.0201 - val_mae: 0.1269 - lr: 1.0000e-04\n",
            "Epoch 82/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0460 - mae: 0.1881\n",
            "Epoch 82: val_mae improved from 0.12685 to 0.12600, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 0.0449 - mae: 0.1856 - val_loss: 0.0199 - val_mae: 0.1260 - lr: 1.0000e-04\n",
            "Epoch 83/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0448 - mae: 0.1855\n",
            "Epoch 83: val_mae improved from 0.12600 to 0.12518, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0448 - mae: 0.1855 - val_loss: 0.0197 - val_mae: 0.1252 - lr: 1.0000e-04\n",
            "Epoch 84/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0447 - mae: 0.1854\n",
            "Epoch 84: val_mae improved from 0.12518 to 0.12438, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0447 - mae: 0.1854 - val_loss: 0.0195 - val_mae: 0.1244 - lr: 1.0000e-04\n",
            "Epoch 85/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0447 - mae: 0.1853\n",
            "Epoch 85: val_mae improved from 0.12438 to 0.12360, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 56ms/step - loss: 0.0447 - mae: 0.1853 - val_loss: 0.0193 - val_mae: 0.1236 - lr: 1.0000e-04\n",
            "Epoch 86/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0456 - mae: 0.1881\n",
            "Epoch 86: val_mae improved from 0.12360 to 0.12284, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 57ms/step - loss: 0.0446 - mae: 0.1852 - val_loss: 0.0191 - val_mae: 0.1228 - lr: 1.0000e-04\n",
            "Epoch 87/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0445 - mae: 0.1851\n",
            "Epoch 87: val_mae improved from 0.12284 to 0.12210, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 49ms/step - loss: 0.0445 - mae: 0.1851 - val_loss: 0.0189 - val_mae: 0.1221 - lr: 1.0000e-04\n",
            "Epoch 88/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0444 - mae: 0.1850\n",
            "Epoch 88: val_mae improved from 0.12210 to 0.12138, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 38ms/step - loss: 0.0444 - mae: 0.1850 - val_loss: 0.0187 - val_mae: 0.1214 - lr: 1.0000e-04\n",
            "Epoch 89/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0455 - mae: 0.1878\n",
            "Epoch 89: val_mae improved from 0.12138 to 0.12068, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0443 - mae: 0.1849 - val_loss: 0.0185 - val_mae: 0.1207 - lr: 1.0000e-04\n",
            "Epoch 90/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0454 - mae: 0.1877\n",
            "Epoch 90: val_mae improved from 0.12068 to 0.12000, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0443 - mae: 0.1849 - val_loss: 0.0184 - val_mae: 0.1200 - lr: 1.0000e-04\n",
            "Epoch 91/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0454 - mae: 0.1877\n",
            "Epoch 91: val_mae improved from 0.12000 to 0.11934, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 0.0442 - mae: 0.1848 - val_loss: 0.0182 - val_mae: 0.1193 - lr: 1.0000e-04\n",
            "Epoch 92/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0453 - mae: 0.1877\n",
            "Epoch 92: val_mae improved from 0.11934 to 0.11870, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0441 - mae: 0.1847 - val_loss: 0.0181 - val_mae: 0.1187 - lr: 1.0000e-04\n",
            "Epoch 93/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0441 - mae: 0.1846\n",
            "Epoch 93: val_mae improved from 0.11870 to 0.11807, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0441 - mae: 0.1846 - val_loss: 0.0179 - val_mae: 0.1181 - lr: 1.0000e-04\n",
            "Epoch 94/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0440 - mae: 0.1846\n",
            "Epoch 94: val_mae improved from 0.11807 to 0.11746, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 64ms/step - loss: 0.0440 - mae: 0.1846 - val_loss: 0.0178 - val_mae: 0.1175 - lr: 1.0000e-04\n",
            "Epoch 95/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0440 - mae: 0.1845\n",
            "Epoch 95: val_mae improved from 0.11746 to 0.11687, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 59ms/step - loss: 0.0440 - mae: 0.1845 - val_loss: 0.0176 - val_mae: 0.1169 - lr: 1.0000e-04\n",
            "Epoch 96/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0439 - mae: 0.1844\n",
            "Epoch 96: val_mae improved from 0.11687 to 0.11629, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0439 - mae: 0.1844 - val_loss: 0.0175 - val_mae: 0.1163 - lr: 1.0000e-04\n",
            "Epoch 97/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0438 - mae: 0.1843\n",
            "Epoch 97: val_mae improved from 0.11629 to 0.11573, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0438 - mae: 0.1843 - val_loss: 0.0173 - val_mae: 0.1157 - lr: 1.0000e-04\n",
            "Epoch 98/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0450 - mae: 0.1874\n",
            "Epoch 98: val_mae improved from 0.11573 to 0.11518, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0438 - mae: 0.1843 - val_loss: 0.0172 - val_mae: 0.1152 - lr: 1.0000e-04\n",
            "Epoch 99/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0449 - mae: 0.1874\n",
            "Epoch 99: val_mae improved from 0.11518 to 0.11465, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0437 - mae: 0.1842 - val_loss: 0.0171 - val_mae: 0.1147 - lr: 1.0000e-04\n",
            "Epoch 100/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0447 - mae: 0.1873\n",
            "Epoch 100: val_mae improved from 0.11465 to 0.11413, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0437 - mae: 0.1841 - val_loss: 0.0170 - val_mae: 0.1141 - lr: 1.0000e-04\n",
            "Epoch 101/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0448 - mae: 0.1873\n",
            "Epoch 101: val_mae improved from 0.11413 to 0.11363, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0436 - mae: 0.1840 - val_loss: 0.0169 - val_mae: 0.1136 - lr: 1.0000e-04\n",
            "Epoch 102/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0436 - mae: 0.1840\n",
            "Epoch 102: val_mae improved from 0.11363 to 0.11314, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0436 - mae: 0.1840 - val_loss: 0.0167 - val_mae: 0.1131 - lr: 1.0000e-04\n",
            "Epoch 103/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0446 - mae: 0.1871\n",
            "Epoch 103: val_mae improved from 0.11314 to 0.11266, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 39ms/step - loss: 0.0435 - mae: 0.1839 - val_loss: 0.0166 - val_mae: 0.1127 - lr: 1.0000e-04\n",
            "Epoch 104/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0445 - mae: 0.1870\n",
            "Epoch 104: val_mae improved from 0.11266 to 0.11220, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 60ms/step - loss: 0.0435 - mae: 0.1838 - val_loss: 0.0165 - val_mae: 0.1122 - lr: 1.0000e-04\n",
            "Epoch 105/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0445 - mae: 0.1870\n",
            "Epoch 105: val_mae improved from 0.11220 to 0.11175, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 52ms/step - loss: 0.0434 - mae: 0.1838 - val_loss: 0.0164 - val_mae: 0.1117 - lr: 1.0000e-04\n",
            "Epoch 106/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0444 - mae: 0.1869\n",
            "Epoch 106: val_mae improved from 0.11175 to 0.11131, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0434 - mae: 0.1837 - val_loss: 0.0163 - val_mae: 0.1113 - lr: 1.0000e-04\n",
            "Epoch 107/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0433 - mae: 0.1836\n",
            "Epoch 107: val_mae improved from 0.11131 to 0.11088, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 0.0433 - mae: 0.1836 - val_loss: 0.0162 - val_mae: 0.1109 - lr: 1.0000e-04\n",
            "Epoch 108/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0433 - mae: 0.1836\n",
            "Epoch 108: val_mae improved from 0.11088 to 0.11046, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0433 - mae: 0.1836 - val_loss: 0.0161 - val_mae: 0.1105 - lr: 1.0000e-04\n",
            "Epoch 109/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0445 - mae: 0.1869\n",
            "Epoch 109: val_mae improved from 0.11046 to 0.11006, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0432 - mae: 0.1835 - val_loss: 0.0160 - val_mae: 0.1101 - lr: 1.0000e-04\n",
            "Epoch 110/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0432 - mae: 0.1834\n",
            "Epoch 110: val_mae improved from 0.11006 to 0.10966, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0432 - mae: 0.1834 - val_loss: 0.0159 - val_mae: 0.1097 - lr: 1.0000e-04\n",
            "Epoch 111/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0444 - mae: 0.1868\n",
            "Epoch 111: val_mae improved from 0.10966 to 0.10928, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 0.0431 - mae: 0.1834 - val_loss: 0.0158 - val_mae: 0.1093 - lr: 1.0000e-04\n",
            "Epoch 112/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0443 - mae: 0.1868\n",
            "Epoch 112: val_mae improved from 0.10928 to 0.10891, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 0.0431 - mae: 0.1833 - val_loss: 0.0158 - val_mae: 0.1089 - lr: 1.0000e-04\n",
            "Epoch 113/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0441 - mae: 0.1865\n",
            "Epoch 113: val_mae improved from 0.10891 to 0.10854, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 60ms/step - loss: 0.0430 - mae: 0.1832 - val_loss: 0.0157 - val_mae: 0.1085 - lr: 1.0000e-04\n",
            "Epoch 114/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0441 - mae: 0.1865\n",
            "Epoch 114: val_mae improved from 0.10854 to 0.10819, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 52ms/step - loss: 0.0430 - mae: 0.1832 - val_loss: 0.0156 - val_mae: 0.1082 - lr: 1.0000e-04\n",
            "Epoch 115/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0442 - mae: 0.1867\n",
            "Epoch 115: val_mae improved from 0.10819 to 0.10785, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0429 - mae: 0.1831 - val_loss: 0.0155 - val_mae: 0.1078 - lr: 1.0000e-04\n",
            "Epoch 116/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0442 - mae: 0.1866\n",
            "Epoch 116: val_mae improved from 0.10785 to 0.10751, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0429 - mae: 0.1830 - val_loss: 0.0154 - val_mae: 0.1075 - lr: 1.0000e-04\n",
            "Epoch 117/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0439 - mae: 0.1863\n",
            "Epoch 117: val_mae improved from 0.10751 to 0.10718, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0429 - mae: 0.1830 - val_loss: 0.0154 - val_mae: 0.1072 - lr: 1.0000e-04\n",
            "Epoch 118/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0428 - mae: 0.1829\n",
            "Epoch 118: val_mae improved from 0.10718 to 0.10687, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0428 - mae: 0.1829 - val_loss: 0.0153 - val_mae: 0.1069 - lr: 1.0000e-04\n",
            "Epoch 119/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0441 - mae: 0.1865\n",
            "Epoch 119: val_mae improved from 0.10687 to 0.10656, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 0.0428 - mae: 0.1828 - val_loss: 0.0152 - val_mae: 0.1066 - lr: 1.0000e-04\n",
            "Epoch 120/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0427 - mae: 0.1828\n",
            "Epoch 120: val_mae improved from 0.10656 to 0.10625, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0427 - mae: 0.1828 - val_loss: 0.0152 - val_mae: 0.1063 - lr: 1.0000e-04\n",
            "Epoch 121/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0438 - mae: 0.1861\n",
            "Epoch 121: val_mae improved from 0.10625 to 0.10596, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 55ms/step - loss: 0.0427 - mae: 0.1827 - val_loss: 0.0151 - val_mae: 0.1060 - lr: 1.0000e-04\n",
            "Epoch 122/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0427 - mae: 0.1827\n",
            "Epoch 122: val_mae improved from 0.10596 to 0.10567, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 63ms/step - loss: 0.0427 - mae: 0.1827 - val_loss: 0.0150 - val_mae: 0.1057 - lr: 1.0000e-04\n",
            "Epoch 123/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0426 - mae: 0.1826\n",
            "Epoch 123: val_mae improved from 0.10567 to 0.10539, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 0.0426 - mae: 0.1826 - val_loss: 0.0150 - val_mae: 0.1054 - lr: 1.0000e-04\n",
            "Epoch 124/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0426 - mae: 0.1825\n",
            "Epoch 124: val_mae improved from 0.10539 to 0.10512, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0426 - mae: 0.1825 - val_loss: 0.0149 - val_mae: 0.1051 - lr: 1.0000e-04\n",
            "Epoch 125/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0426 - mae: 0.1825\n",
            "Epoch 125: val_mae improved from 0.10512 to 0.10486, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 0.0426 - mae: 0.1825 - val_loss: 0.0148 - val_mae: 0.1049 - lr: 1.0000e-04\n",
            "Epoch 126/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0436 - mae: 0.1858\n",
            "Epoch 126: val_mae improved from 0.10486 to 0.10460, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0425 - mae: 0.1824 - val_loss: 0.0148 - val_mae: 0.1046 - lr: 1.0000e-04\n",
            "Epoch 127/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0425 - mae: 0.1823\n",
            "Epoch 127: val_mae improved from 0.10460 to 0.10435, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0425 - mae: 0.1823 - val_loss: 0.0147 - val_mae: 0.1043 - lr: 1.0000e-04\n",
            "Epoch 128/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0424 - mae: 0.1823\n",
            "Epoch 128: val_mae improved from 0.10435 to 0.10410, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0424 - mae: 0.1823 - val_loss: 0.0147 - val_mae: 0.1041 - lr: 1.0000e-04\n",
            "Epoch 129/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0437 - mae: 0.1860\n",
            "Epoch 129: val_mae improved from 0.10410 to 0.10386, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0424 - mae: 0.1822 - val_loss: 0.0146 - val_mae: 0.1039 - lr: 1.0000e-04\n",
            "Epoch 130/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0424 - mae: 0.1822\n",
            "Epoch 130: val_mae improved from 0.10386 to 0.10363, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 54ms/step - loss: 0.0424 - mae: 0.1822 - val_loss: 0.0146 - val_mae: 0.1036 - lr: 1.0000e-04\n",
            "Epoch 131/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0423 - mae: 0.1821\n",
            "Epoch 131: val_mae improved from 0.10363 to 0.10340, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 64ms/step - loss: 0.0423 - mae: 0.1821 - val_loss: 0.0145 - val_mae: 0.1034 - lr: 1.0000e-04\n",
            "Epoch 132/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0436 - mae: 0.1858\n",
            "Epoch 132: val_mae improved from 0.10340 to 0.10318, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0423 - mae: 0.1820 - val_loss: 0.0145 - val_mae: 0.1032 - lr: 1.0000e-04\n",
            "Epoch 133/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0423 - mae: 0.1819\n",
            "Epoch 133: val_mae improved from 0.10318 to 0.10296, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0423 - mae: 0.1819 - val_loss: 0.0144 - val_mae: 0.1030 - lr: 1.0000e-04\n",
            "Epoch 134/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0435 - mae: 0.1857\n",
            "Epoch 134: val_mae improved from 0.10296 to 0.10275, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0422 - mae: 0.1819 - val_loss: 0.0144 - val_mae: 0.1028 - lr: 1.0000e-04\n",
            "Epoch 135/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0422 - mae: 0.1818\n",
            "Epoch 135: val_mae improved from 0.10275 to 0.10255, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0422 - mae: 0.1818 - val_loss: 0.0143 - val_mae: 0.1025 - lr: 1.0000e-04\n",
            "Epoch 136/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0421 - mae: 0.1817\n",
            "Epoch 136: val_mae improved from 0.10255 to 0.10235, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 0.0421 - mae: 0.1817 - val_loss: 0.0143 - val_mae: 0.1023 - lr: 1.0000e-04\n",
            "Epoch 137/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0421 - mae: 0.1817\n",
            "Epoch 137: val_mae improved from 0.10235 to 0.10215, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 0.0421 - mae: 0.1817 - val_loss: 0.0142 - val_mae: 0.1022 - lr: 1.0000e-04\n",
            "Epoch 138/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0421 - mae: 0.1816\n",
            "Epoch 138: val_mae improved from 0.10215 to 0.10196, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0421 - mae: 0.1816 - val_loss: 0.0142 - val_mae: 0.1020 - lr: 1.0000e-04\n",
            "Epoch 139/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0431 - mae: 0.1850\n",
            "Epoch 139: val_mae improved from 0.10196 to 0.10178, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 47ms/step - loss: 0.0420 - mae: 0.1815 - val_loss: 0.0142 - val_mae: 0.1018 - lr: 1.0000e-04\n",
            "Epoch 140/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0431 - mae: 0.1849\n",
            "Epoch 140: val_mae improved from 0.10178 to 0.10159, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 62ms/step - loss: 0.0420 - mae: 0.1815 - val_loss: 0.0141 - val_mae: 0.1016 - lr: 1.0000e-04\n",
            "Epoch 141/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0433 - mae: 0.1853\n",
            "Epoch 141: val_mae improved from 0.10159 to 0.10142, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0420 - mae: 0.1814 - val_loss: 0.0141 - val_mae: 0.1014 - lr: 1.0000e-04\n",
            "Epoch 142/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0419 - mae: 0.1813\n",
            "Epoch 142: val_mae improved from 0.10142 to 0.10125, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 0.0419 - mae: 0.1813 - val_loss: 0.0140 - val_mae: 0.1012 - lr: 1.0000e-04\n",
            "Epoch 143/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0432 - mae: 0.1852\n",
            "Epoch 143: val_mae improved from 0.10125 to 0.10108, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 0.0419 - mae: 0.1813 - val_loss: 0.0140 - val_mae: 0.1011 - lr: 1.0000e-04\n",
            "Epoch 144/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0432 - mae: 0.1851\n",
            "Epoch 144: val_mae improved from 0.10108 to 0.10091, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0419 - mae: 0.1812 - val_loss: 0.0140 - val_mae: 0.1009 - lr: 1.0000e-04\n",
            "Epoch 145/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0432 - mae: 0.1850\n",
            "Epoch 145: val_mae improved from 0.10091 to 0.10075, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0418 - mae: 0.1811 - val_loss: 0.0139 - val_mae: 0.1008 - lr: 1.0000e-04\n",
            "Epoch 146/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0429 - mae: 0.1845\n",
            "Epoch 146: val_mae improved from 0.10075 to 0.10059, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 44ms/step - loss: 0.0418 - mae: 0.1810 - val_loss: 0.0139 - val_mae: 0.1006 - lr: 1.0000e-04\n",
            "Epoch 147/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0417 - mae: 0.1810\n",
            "Epoch 147: val_mae improved from 0.10059 to 0.10044, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 3s 73ms/step - loss: 0.0417 - mae: 0.1810 - val_loss: 0.0139 - val_mae: 0.1004 - lr: 1.0000e-04\n",
            "Epoch 148/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0417 - mae: 0.1809\n",
            "Epoch 148: val_mae improved from 0.10044 to 0.10029, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 3s 98ms/step - loss: 0.0417 - mae: 0.1809 - val_loss: 0.0138 - val_mae: 0.1003 - lr: 1.0000e-04\n",
            "Epoch 149/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0417 - mae: 0.1809\n",
            "Epoch 149: val_mae improved from 0.10029 to 0.10015, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 45ms/step - loss: 0.0417 - mae: 0.1809 - val_loss: 0.0138 - val_mae: 0.1001 - lr: 1.0000e-04\n",
            "Epoch 150/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0416 - mae: 0.1808\n",
            "Epoch 150: val_mae improved from 0.10015 to 0.10001, saving model to model/my_checkpoint.ckpt\n",
            "35/35 [==============================] - 2s 56ms/step - loss: 0.0416 - mae: 0.1808 - val_loss: 0.0138 - val_mae: 0.1000 - lr: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NOn1_xWg0NGr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}