{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Judy-Choi/Tensorflow_Certificate/blob/main/16_power.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# val_mae = 0.05"
      ],
      "metadata": {
        "id": "utOh1w7G9VAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow \n",
        "!pip install tensorflow==2.10.0"
      ],
      "metadata": {
        "id": "vhzmdMmipB8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46406d32-d9f3-4740-f278-a8ceb67f12b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.11.0\n",
            "Uninstalling tensorflow-2.11.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.9/dist-packages/tensorflow-2.11.0.dist-info/*\n",
            "    /usr/local/lib/python3.9/dist-packages/tensorflow/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.7.0\n",
            "  Downloading tensorflow-2.7.0-cp39-cp39-manylinux2010_x86_64.whl (489.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.7/489.7 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (1.51.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (1.4.0)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.1/463.1 KB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (3.3.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (1.22.4)\n",
            "Collecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (0.38.4)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (15.0.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (4.5.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (0.31.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (1.15.0)\n",
            "Collecting keras-preprocessing>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flatbuffers<3.0,>=1.12\n",
            "  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (2.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (3.19.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (2.11.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.25.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.16.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.2.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (1.26.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.6->tensorflow==2.7.0) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, flatbuffers, keras-preprocessing, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.11.0\n",
            "    Uninstalling tensorflow-estimator-2.11.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.11.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.11.0\n",
            "    Uninstalling keras-2.11.0:\n",
            "      Successfully uninstalled keras-2.11.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 23.3.3\n",
            "    Uninstalling flatbuffers-23.3.3:\n",
            "      Successfully uninstalled flatbuffers-23.3.3\n",
            "Successfully installed flatbuffers-2.0.7 keras-2.7.0 keras-preprocessing-1.1.2 tensorflow-2.7.0 tensorflow-estimator-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 정답"
      ],
      "metadata": {
        "id": "ZbaldESR9Zb9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAilDFS2KzwN"
      },
      "source": [
        "# ABOUT THE DATASET\n",
        "#\n",
        "# Original Source:\n",
        "# https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption\n",
        "#\n",
        "# The original 'Individual House Hold Electric Power Consumption Dataset'\n",
        "# has Measurements of electric power consumption in one household with\n",
        "# a one-minute sampling rate over a period of almost 4 years.\n",
        "#\n",
        "# Different electrical quantities and some sub-metering values are available.\n",
        "#\n",
        "# For the purpose of the examination we have provided a subset containing\n",
        "# the data for the first 60 days in the dataset. We have also cleaned the\n",
        "# dataset beforehand to remove missing values. The dataset is provided as a\n",
        "# csv file in the project.\n",
        "#\n",
        "# The dataset has a total of 7 features ordered by time.\n",
        "# ==============================================================================\n",
        "#\n",
        "# INSTRUCTIONS\n",
        "#\n",
        "# Complete the code in following functions:\n",
        "# 1. windowed_dataset()\n",
        "# 2. solution_model()\n",
        "#\n",
        "# The model input and output shapes must match the following\n",
        "# specifications.\n",
        "#\n",
        "# 1. Model input_shape must be (BATCH_SIZE, N_PAST = 24, N_FEATURES = 7),\n",
        "#    since the testing infrastructure expects a window of past N_PAST = 24\n",
        "#    observations of the 7 features to predict the next 24 observations of\n",
        "#    the same features.\n",
        "#\n",
        "# 2. Model output_shape must be (BATCH_SIZE, N_FUTURE = 24, N_FEATURES = 7)\n",
        "#\n",
        "# 3. DON'T change the values of the following constants\n",
        "#    N_PAST, N_FUTURE, SHIFT in the windowed_dataset()\n",
        "#    BATCH_SIZE in solution_model() (See code for additional note on\n",
        "#    BATCH_SIZE).\n",
        "# 4. Code for normalizing the data is provided - DON't change it.\n",
        "#    Changing the normalizing code will affect your score.\n",
        "#\n",
        "# HINT: Your neural network must have a validation MAE of approximately 0.055 or\n",
        "# less on the normalized validation dataset for top marks.\n",
        "#\n",
        "# WARNING: Do not use lambda layers in your model, they are not supported\n",
        "# on the grading infrastructure.\n",
        "#\n",
        "# WARNING: If you are using the GRU layer, it is advised not to use the\n",
        "# 'recurrent_dropout' argument (you can alternatively set it to 0),\n",
        "# since it has not been implemented in the cuDNN kernel and may\n",
        "# result in much longer training times."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KPbpBIlJ_I4U",
        "outputId": "43fc1a58-a5b2-4da3-e06f-0cd776d4680d"
      },
      "source": [
        "import urllib\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Bidirectional, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "\n",
        "def download_and_extract_data():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/household_power.zip'\n",
        "    urllib.request.urlretrieve(url, 'household_power.zip')\n",
        "    with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "        \n",
        "def normalize_series(data, min, max):\n",
        "    data = data - min\n",
        "    data = data / max\n",
        "    return data\n",
        "\n",
        "def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(size=(n_past + n_future), shift = shift, drop_remainder = True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    ds = ds.shuffle(len(series))\n",
        "    ds = ds.map(\n",
        "        lambda w: (w[:n_past], w[n_past:])\n",
        "    )\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    download_and_extract_data()\n",
        "    \n",
        "    df = pd.read_csv('household_power_consumption.csv', sep=',', infer_datetime_format=True, index_col='datetime', header=0)\n",
        "    \n",
        "    N_FEATURES = len(df.columns)\n",
        "    \n",
        "    data = df.values\n",
        "    split_time = int(len(data) * 0.5)\n",
        "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        "    \n",
        "    x_train = data[:split_time]\n",
        "    x_valid = data[split_time:]\n",
        "    \n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    BATCH_SIZE = 32\n",
        "    N_PAST = 24\n",
        "    N_FUTURE = 24\n",
        "    SHIFT = 1\n",
        "\n",
        "    train_set = windowed_dataset(series=x_train, \n",
        "                                 batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, \n",
        "                                 n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    \n",
        "    valid_set = windowed_dataset(series=x_valid, \n",
        "                                 batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, \n",
        "                                 n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    \n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        Conv1D(filters=64, \n",
        "               kernel_size=5,\n",
        "               padding=\"causal\",\n",
        "               activation=\"relu\",\n",
        "               input_shape=[N_PAST, 7],\n",
        "               ),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)), \n",
        "        Dense(64, activation=\"relu\"),\n",
        "        Dropout(0.5),     \n",
        "        Dense(32, activation=\"relu\"),   \n",
        "        Dense(N_FEATURES)\n",
        "    ])\n",
        "    \n",
        "    checkpoint_path='model/my_checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path,\n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True,\n",
        "                             monitor='val_mae',\n",
        "                             verbose=1,\n",
        "                             )\n",
        "    early_stopping = EarlyStopping(monitor='val_mae', patience=10)\n",
        "\n",
        "    model.compile(loss='huber',\n",
        "              optimizer='adam', \n",
        "              metrics=[\"mae\"]\n",
        "              )\n",
        "\n",
        "    model.fit(train_set, \n",
        "          validation_data=(valid_set), \n",
        "          epochs=100, \n",
        "          callbacks=[checkpoint, early_stopping], \n",
        "          )\n",
        "    model.load_weights(checkpoint_path)\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"power-1.h5\")\n",
        "## don't copy following codes in real test\n",
        "    from google.colab import files\n",
        "    files.download('power-1.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "   1347/Unknown - 38s 13ms/step - loss: 0.0077 - mae: 0.0646\n",
            "Epoch 00001: val_mae improved from inf to 0.06423, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 60s 29ms/step - loss: 0.0077 - mae: 0.0646 - val_loss: 0.0073 - val_mae: 0.0642\n",
            "Epoch 2/100\n",
            "1347/1349 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0553\n",
            "Epoch 00002: val_mae improved from 0.06423 to 0.05133, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 41s 25ms/step - loss: 0.0065 - mae: 0.0553 - val_loss: 0.0060 - val_mae: 0.0513\n",
            "Epoch 3/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0062 - mae: 0.0530\n",
            "Epoch 00003: val_mae did not improve from 0.05133\n",
            "1349/1349 [==============================] - 40s 25ms/step - loss: 0.0062 - mae: 0.0530 - val_loss: 0.0061 - val_mae: 0.0538\n",
            "Epoch 4/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0060 - mae: 0.0515\n",
            "Epoch 00004: val_mae improved from 0.05133 to 0.04939, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 40s 24ms/step - loss: 0.0060 - mae: 0.0515 - val_loss: 0.0059 - val_mae: 0.0494\n",
            "Epoch 5/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0062 - mae: 0.0525\n",
            "Epoch 00005: val_mae did not improve from 0.04939\n",
            "1349/1349 [==============================] - 40s 24ms/step - loss: 0.0062 - mae: 0.0525 - val_loss: 0.0062 - val_mae: 0.0545\n",
            "Epoch 6/100\n",
            "1345/1349 [============================>.] - ETA: 0s - loss: 0.0060 - mae: 0.0514\n",
            "Epoch 00006: val_mae did not improve from 0.04939\n",
            "1349/1349 [==============================] - 41s 25ms/step - loss: 0.0060 - mae: 0.0514 - val_loss: 0.0060 - val_mae: 0.0531\n",
            "Epoch 7/100\n",
            "1347/1349 [============================>.] - ETA: 0s - loss: 0.0058 - mae: 0.0499\n",
            "Epoch 00007: val_mae did not improve from 0.04939\n",
            "1349/1349 [==============================] - 40s 25ms/step - loss: 0.0058 - mae: 0.0499 - val_loss: 0.0058 - val_mae: 0.0495\n",
            "Epoch 8/100\n",
            "1345/1349 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0489\n",
            "Epoch 00008: val_mae did not improve from 0.04939\n",
            "1349/1349 [==============================] - 39s 24ms/step - loss: 0.0056 - mae: 0.0489 - val_loss: 0.0058 - val_mae: 0.0500\n",
            "Epoch 9/100\n",
            "1346/1349 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0484\n",
            "Epoch 00009: val_mae did not improve from 0.04939\n",
            "1349/1349 [==============================] - 41s 25ms/step - loss: 0.0055 - mae: 0.0484 - val_loss: 0.0058 - val_mae: 0.0516\n",
            "Epoch 10/100\n",
            "1346/1349 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0481\n",
            "Epoch 00010: val_mae did not improve from 0.04939\n",
            "1349/1349 [==============================] - 40s 24ms/step - loss: 0.0055 - mae: 0.0481 - val_loss: 0.0060 - val_mae: 0.0547\n",
            "Epoch 11/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0477\n",
            "Epoch 00011: val_mae did not improve from 0.04939\n",
            "1349/1349 [==============================] - 40s 25ms/step - loss: 0.0054 - mae: 0.0477 - val_loss: 0.0059 - val_mae: 0.0501\n",
            "Epoch 12/100\n",
            "1346/1349 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0469\n",
            "Epoch 00012: val_mae did not improve from 0.04939\n",
            "1349/1349 [==============================] - 40s 25ms/step - loss: 0.0053 - mae: 0.0469 - val_loss: 0.0059 - val_mae: 0.0508\n",
            "Epoch 13/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0052 - mae: 0.0464\n",
            "Epoch 00013: val_mae did not improve from 0.04939\n",
            "1349/1349 [==============================] - 38s 24ms/step - loss: 0.0052 - mae: 0.0464 - val_loss: 0.0061 - val_mae: 0.0532\n",
            "Epoch 14/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0051 - mae: 0.0460\n",
            "Epoch 00014: val_mae did not improve from 0.04939\n",
            "1349/1349 [==============================] - 40s 24ms/step - loss: 0.0051 - mae: 0.0460 - val_loss: 0.0061 - val_mae: 0.0519\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8919be5e-4137-4ed4-a25f-dc7ba2689843\", \"power-1.h5\", 796080)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT"
      ],
      "metadata": {
        "id": "FmAhQR2t9fO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q) 다음 코드의 성능을 개선해 줘.\n",
        "사용하는 데이터는 float 형의 시계열 데이터이고\n",
        "다음 조건을 적용해 줘.\n",
        "Use dropout\n",
        "Use batch normalization\n",
        "Use checkpoint with save best only, and save weight only and verbose = 1\n",
        "Use early stopping with patience = 10\n",
        "Use reduce learning rate on plateau\n",
        "set verbose = 1\n",
        "Set epochs = 100\n",
        "do not use data augmentation\n",
        "Use other methods to make val loss value lower."
      ],
      "metadata": {
        "id": "ICBeYvd19yjZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plOYGUKtGTEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50815c47-d7ac-4521-d57f-dec73cefbd86"
      },
      "source": [
        "import urllib\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Bidirectional, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "def download_and_extract_data():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/household_power.zip'\n",
        "    urllib.request.urlretrieve(url, 'household_power.zip')\n",
        "    with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "        \n",
        "def normalize_series(data, min, max):\n",
        "    data = data - min\n",
        "    data = data / max\n",
        "    return data\n",
        "\n",
        "def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(size=(n_past + n_future), shift = shift, drop_remainder = True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    ds = ds.shuffle(len(series))\n",
        "    ds = ds.map(\n",
        "        lambda w: (w[:n_past], w[n_past:])\n",
        "    )\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    download_and_extract_data()\n",
        "    \n",
        "    df = pd.read_csv('household_power_consumption.csv', sep=',', infer_datetime_format=True, index_col='datetime', header=0)\n",
        "    \n",
        "    N_FEATURES = len(df.columns)\n",
        "    \n",
        "    data = df.values\n",
        "    split_time = int(len(data) * 0.5)\n",
        "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        "    \n",
        "    x_train = data[:split_time]\n",
        "    x_valid = data[split_time:]\n",
        "    \n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    BATCH_SIZE = 32\n",
        "    N_PAST = 24\n",
        "    N_FUTURE = 24\n",
        "    SHIFT = 1\n",
        "\n",
        "    train_set = windowed_dataset(series=x_train, \n",
        "                                 batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, \n",
        "                                 n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    \n",
        "    valid_set = windowed_dataset(series=x_valid, \n",
        "                                 batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, \n",
        "                                 n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    \n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        Conv1D(filters=64,\n",
        "            kernel_size=5,\n",
        "            padding=\"causal\",\n",
        "            activation=\"relu\",\n",
        "            input_shape=[N_PAST, 7],\n",
        "            ),\n",
        "        BatchNormalization(),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        BatchNormalization(),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        BatchNormalization(),\n",
        "        Dense(64, activation=\"relu\"),\n",
        "        Dropout(0.5),\n",
        "        Dense(32, activation=\"relu\"),\n",
        "        Dense(N_FEATURES)\n",
        "    ])\n",
        "\n",
        "    checkpoint_path = 'model/my_checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path,\n",
        "                                save_weights_only=True,\n",
        "                                save_best_only=True,\n",
        "                                monitor='val_mae',\n",
        "                                verbose=1,\n",
        "                                )\n",
        "    early_stopping = EarlyStopping(monitor='val_mae', patience=10)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
        "\n",
        "    model.compile(loss='huber',\n",
        "                optimizer=optimizer,\n",
        "                metrics=[\"mae\"]\n",
        "                )\n",
        "\n",
        "    model.fit(train_set,\n",
        "            validation_data=valid_set,\n",
        "            epochs=100,\n",
        "            callbacks=[checkpoint, early_stopping, reduce_lr],\n",
        "            verbose=1,\n",
        "            )\n",
        "    \n",
        "    model.load_weights(checkpoint_path)\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"power-1.h5\")\n",
        "## don't copy following codes in real test\n",
        "    # from google.colab import files\n",
        "    # files.download('power-1.h5')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "   1349/Unknown - 49s 17ms/step - loss: 0.0120 - mae: 0.0883\n",
            "Epoch 1: val_mae improved from inf to 0.06085, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 65s 29ms/step - loss: 0.0120 - mae: 0.0883 - val_loss: 0.0071 - val_mae: 0.0609 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0071 - mae: 0.0633\n",
            "Epoch 2: val_mae improved from 0.06085 to 0.05718, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 42s 26ms/step - loss: 0.0071 - mae: 0.0633 - val_loss: 0.0064 - val_mae: 0.0572 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "1347/1349 [============================>.] - ETA: 0s - loss: 0.0066 - mae: 0.0578\n",
            "Epoch 3: val_mae improved from 0.05718 to 0.05406, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 42s 26ms/step - loss: 0.0066 - mae: 0.0578 - val_loss: 0.0063 - val_mae: 0.0541 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0063 - mae: 0.0553\n",
            "Epoch 4: val_mae did not improve from 0.05406\n",
            "1349/1349 [==============================] - 41s 26ms/step - loss: 0.0063 - mae: 0.0553 - val_loss: 0.0062 - val_mae: 0.0544 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0061 - mae: 0.0537\n",
            "Epoch 5: val_mae improved from 0.05406 to 0.05207, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 43s 27ms/step - loss: 0.0061 - mae: 0.0537 - val_loss: 0.0062 - val_mae: 0.0521 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0060 - mae: 0.0527\n",
            "Epoch 6: val_mae did not improve from 0.05207\n",
            "1349/1349 [==============================] - 48s 30ms/step - loss: 0.0060 - mae: 0.0527 - val_loss: 0.0061 - val_mae: 0.0551 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0058 - mae: 0.0517\n",
            "Epoch 7: val_mae improved from 0.05207 to 0.04977, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 42s 26ms/step - loss: 0.0058 - mae: 0.0517 - val_loss: 0.0060 - val_mae: 0.0498 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0057 - mae: 0.0510\n",
            "Epoch 8: val_mae did not improve from 0.04977\n",
            "1349/1349 [==============================] - 47s 30ms/step - loss: 0.0057 - mae: 0.0510 - val_loss: 0.0062 - val_mae: 0.0539 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "1346/1349 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0501\n",
            "Epoch 9: val_mae did not improve from 0.04977\n",
            "1349/1349 [==============================] - 41s 26ms/step - loss: 0.0056 - mae: 0.0501 - val_loss: 0.0061 - val_mae: 0.0534 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "1346/1349 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0494\n",
            "Epoch 10: val_mae did not improve from 0.04977\n",
            "1349/1349 [==============================] - 47s 30ms/step - loss: 0.0055 - mae: 0.0494 - val_loss: 0.0066 - val_mae: 0.0560 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0054 - mae: 0.0488\n",
            "Epoch 11: val_mae did not improve from 0.04977\n",
            "1349/1349 [==============================] - 47s 31ms/step - loss: 0.0054 - mae: 0.0488 - val_loss: 0.0064 - val_mae: 0.0543 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0480\n",
            "Epoch 12: val_mae did not improve from 0.04977\n",
            "\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "1349/1349 [==============================] - 41s 26ms/step - loss: 0.0052 - mae: 0.0480 - val_loss: 0.0063 - val_mae: 0.0515 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "1346/1349 [============================>.] - ETA: 0s - loss: 0.0049 - mae: 0.0457\n",
            "Epoch 13: val_mae did not improve from 0.04977\n",
            "1349/1349 [==============================] - 48s 31ms/step - loss: 0.0049 - mae: 0.0457 - val_loss: 0.0064 - val_mae: 0.0512 - lr: 1.0000e-04\n",
            "Epoch 14/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0048 - mae: 0.0452\n",
            "Epoch 14: val_mae did not improve from 0.04977\n",
            "1349/1349 [==============================] - 41s 26ms/step - loss: 0.0048 - mae: 0.0452 - val_loss: 0.0065 - val_mae: 0.0518 - lr: 1.0000e-04\n",
            "Epoch 15/100\n",
            "1346/1349 [============================>.] - ETA: 0s - loss: 0.0048 - mae: 0.0450\n",
            "Epoch 15: val_mae did not improve from 0.04977\n",
            "1349/1349 [==============================] - 41s 26ms/step - loss: 0.0048 - mae: 0.0450 - val_loss: 0.0065 - val_mae: 0.0516 - lr: 1.0000e-04\n",
            "Epoch 16/100\n",
            "1346/1349 [============================>.] - ETA: 0s - loss: 0.0047 - mae: 0.0447\n",
            "Epoch 16: val_mae did not improve from 0.04977\n",
            "1349/1349 [==============================] - 42s 26ms/step - loss: 0.0047 - mae: 0.0447 - val_loss: 0.0065 - val_mae: 0.0516 - lr: 1.0000e-04\n",
            "Epoch 17/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0047 - mae: 0.0445\n",
            "Epoch 17: val_mae did not improve from 0.04977\n",
            "\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "1349/1349 [==============================] - 47s 31ms/step - loss: 0.0047 - mae: 0.0445 - val_loss: 0.0065 - val_mae: 0.0518 - lr: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 직접 코딩\n",
        "\n",
        "과제 순서를 바꿔서, ChatGPT 가 짜 준 코드를 직접 개선해 본다\n",
        "\n",
        "...\n",
        "\n",
        "개선이 되지 않아 ChatGPT 에 추가 질문을 해 봄\n",
        "\n",
        "Q) 시계열 데이터를 예측하는 문제에서 다음 모델 구조를 개선해 줘\n",
        "\n",
        "A) 이 모델은 Conv1D 레이어를 사용하여 입력 시계열 데이터를 처리하고, 그 다음에 두 개의 Bidirectional LSTM 레이어를 쌓아서 시계열 데이터를 학습하고 예측합니다. 그리고 뒤이어 Dense 레이어들을 통해 출력을 수행합니다.\n",
        "\n",
        "모델 구조 자체는 꽤 괜찮은 편이나 몇 가지 개선 가능한 점이 있습니다.\n",
        "\n",
        "BatchNormalization 추가하기\n",
        "배치 정규화는 각 레이어의 입력값을 평균과 분산으로 정규화하는 방법입니다. 이를 추가하면 학습이 안정화되고 더 빠르게 수렴하게 됩니다. 예측 결과도 개선될 가능성이 있습니다.\n",
        "\n",
        "Residual Connection 추가하기\n",
        "Residual Connection은 출력값과 입력값을 더하는 방식으로 네트워크를 구성하는 방법입니다. 이를 추가하면 학습이 더 안정화되고 더 깊은 모델을 학습할 수 있게 됩니다.\n",
        "\n",
        "다른 활성화 함수 적용해보기\n",
        "현재 모델에서는 ReLU 활성화 함수를 사용하고 있습니다. 이 외에도 다른 활성화 함수를 적용하여 성능 개선을 시도할 수 있습니다. 예를 들어, tanh, sigmoid 등의 함수를 사용해보는 것도 좋은 방법입니다.\n",
        "\n",
        "다양한 하이퍼파라미터 조합 시도하기\n",
        "학습률, 배치 크기, 에포크 수 등 다양한 하이퍼파라미터를 조합하여 실험해보는 것도 모델 개선에 도움이 됩니다.\n",
        "\n",
        "다른 모델 구조 시도하기\n",
        "컨볼루션 신경망(Convolutional Neural Network, CNN)이나 Transformer 등의 다른 모델 구조를 시도해보는 것도 좋은 방법입니다. 다양한 모델을 실험하여 최적의 모델을 찾아내는 것이 중요합니다."
      ],
      "metadata": {
        "id": "thFTWyyKS-NU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Bidirectional, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "def download_and_extract_data():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/household_power.zip'\n",
        "    urllib.request.urlretrieve(url, 'household_power.zip')\n",
        "    with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "        \n",
        "def normalize_series(data, min, max):\n",
        "    data = data - min\n",
        "    data = data / max\n",
        "    return data\n",
        "\n",
        "def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(size=(n_past + n_future), shift = shift, drop_remainder = True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    ds = ds.shuffle(len(series))\n",
        "    ds = ds.map(\n",
        "        lambda w: (w[:n_past], w[n_past:])\n",
        "    )\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    download_and_extract_data()\n",
        "    \n",
        "    df = pd.read_csv('household_power_consumption.csv', sep=',', infer_datetime_format=True, index_col='datetime', header=0)\n",
        "    \n",
        "    N_FEATURES = len(df.columns)\n",
        "    \n",
        "    data = df.values\n",
        "    split_time = int(len(data) * 0.5)\n",
        "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        "    \n",
        "    x_train = data[:split_time]\n",
        "    x_valid = data[split_time:]\n",
        "    \n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    BATCH_SIZE = 32\n",
        "    N_PAST = 24\n",
        "    N_FUTURE = 24\n",
        "    SHIFT = 1\n",
        "\n",
        "    train_set = windowed_dataset(series=x_train, \n",
        "                                 batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, \n",
        "                                 n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    \n",
        "    valid_set = windowed_dataset(series=x_valid, \n",
        "                                 batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, \n",
        "                                 n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        Conv1D(filters=64,\n",
        "            kernel_size=5,\n",
        "            padding=\"causal\",\n",
        "            activation=\"relu\",\n",
        "            input_shape=[N_PAST, 7],\n",
        "            ),\n",
        "        # BatchNormalization(),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        # BatchNormalization(),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        # BatchNormalization(),\n",
        "        Dense(64, activation=\"relu\"),\n",
        "        Dropout(0.5),\n",
        "        Dense(32, activation=\"relu\"),\n",
        "        Dense(N_FEATURES)\n",
        "    ])\n",
        "\n",
        "    checkpoint_path = 'model/my_checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path,\n",
        "                                save_weights_only=True,\n",
        "                                save_best_only=True,\n",
        "                                monitor='val_mae',\n",
        "                                verbose=1,\n",
        "                                )\n",
        "    early_stopping = EarlyStopping(monitor='val_mae', patience=10)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_mae', factor=0.1, patience=5, verbose=1)\n",
        "\n",
        "    # optimizer SGD 로 교체 시 수렴되기까지 더 많은 epoch.\n",
        "    # optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
        "    optimizer = tf.keras.optimizers.SGD(lr=0.001)\n",
        "\n",
        "    model.compile(loss='huber',\n",
        "                optimizer=optimizer,\n",
        "                metrics=[\"mae\"]\n",
        "                )\n",
        "\n",
        "    model.fit(train_set,\n",
        "            validation_data=valid_set,\n",
        "            epochs=100,\n",
        "            callbacks=[checkpoint, early_stopping, reduce_lr],\n",
        "            verbose=1,\n",
        "            )\n",
        "    \n",
        "    model.load_weights(checkpoint_path)\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"power-1.h5\")\n",
        "## don't copy following codes in real test\n",
        "    # from google.colab import files\n",
        "    # files.download('power-1.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ahKvyCTY_czB",
        "outputId": "6c0308d7-4f8f-4d66-f394-468664908ec8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "   1348/Unknown - 31s 13ms/step - loss: 0.0185 - mae: 0.1206\n",
            "Epoch 1: val_mae improved from inf to 0.10722, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 53s 29ms/step - loss: 0.0185 - mae: 0.1206 - val_loss: 0.0142 - val_mae: 0.1072 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0143 - mae: 0.1118\n",
            "Epoch 2: val_mae improved from 0.10722 to 0.10004, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 36s 23ms/step - loss: 0.0143 - mae: 0.1118 - val_loss: 0.0122 - val_mae: 0.1000 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "1347/1349 [============================>.] - ETA: 0s - loss: 0.0132 - mae: 0.1065\n",
            "Epoch 3: val_mae improved from 0.10004 to 0.09454, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 43s 28ms/step - loss: 0.0132 - mae: 0.1065 - val_loss: 0.0113 - val_mae: 0.0945 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.1024\n",
            "Epoch 4: val_mae improved from 0.09454 to 0.09044, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 37s 23ms/step - loss: 0.0126 - mae: 0.1024 - val_loss: 0.0109 - val_mae: 0.0904 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "1347/1349 [============================>.] - ETA: 0s - loss: 0.0122 - mae: 0.0993\n",
            "Epoch 5: val_mae improved from 0.09044 to 0.08756, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 36s 23ms/step - loss: 0.0122 - mae: 0.0993 - val_loss: 0.0106 - val_mae: 0.0876 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0119 - mae: 0.0970\n",
            "Epoch 6: val_mae improved from 0.08756 to 0.08558, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 36s 23ms/step - loss: 0.0119 - mae: 0.0970 - val_loss: 0.0104 - val_mae: 0.0856 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0117 - mae: 0.0952\n",
            "Epoch 7: val_mae improved from 0.08558 to 0.08418, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 36s 22ms/step - loss: 0.0117 - mae: 0.0952 - val_loss: 0.0102 - val_mae: 0.0842 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0115 - mae: 0.0938\n",
            "Epoch 8: val_mae improved from 0.08418 to 0.08318, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 43s 28ms/step - loss: 0.0115 - mae: 0.0938 - val_loss: 0.0100 - val_mae: 0.0832 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "1346/1349 [============================>.] - ETA: 0s - loss: 0.0114 - mae: 0.0927\n",
            "Epoch 9: val_mae improved from 0.08318 to 0.08222, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 43s 28ms/step - loss: 0.0114 - mae: 0.0927 - val_loss: 0.0099 - val_mae: 0.0822 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "1347/1349 [============================>.] - ETA: 0s - loss: 0.0112 - mae: 0.0916\n",
            "Epoch 10: val_mae improved from 0.08222 to 0.08140, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 36s 22ms/step - loss: 0.0112 - mae: 0.0916 - val_loss: 0.0098 - val_mae: 0.0814 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0111 - mae: 0.0908\n",
            "Epoch 11: val_mae improved from 0.08140 to 0.08078, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 37s 23ms/step - loss: 0.0111 - mae: 0.0908 - val_loss: 0.0097 - val_mae: 0.0808 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0110 - mae: 0.0899\n",
            "Epoch 12: val_mae improved from 0.08078 to 0.07997, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 43s 27ms/step - loss: 0.0110 - mae: 0.0899 - val_loss: 0.0096 - val_mae: 0.0800 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "1345/1349 [============================>.] - ETA: 0s - loss: 0.0109 - mae: 0.0892\n",
            "Epoch 13: val_mae improved from 0.07997 to 0.07973, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 37s 23ms/step - loss: 0.0109 - mae: 0.0892 - val_loss: 0.0095 - val_mae: 0.0797 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "1345/1349 [============================>.] - ETA: 0s - loss: 0.0108 - mae: 0.0886\n",
            "Epoch 14: val_mae improved from 0.07973 to 0.07912, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 36s 22ms/step - loss: 0.0108 - mae: 0.0885 - val_loss: 0.0095 - val_mae: 0.0791 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "1346/1349 [============================>.] - ETA: 0s - loss: 0.0107 - mae: 0.0880\n",
            "Epoch 15: val_mae improved from 0.07912 to 0.07880, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 36s 22ms/step - loss: 0.0107 - mae: 0.0880 - val_loss: 0.0094 - val_mae: 0.0788 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0107 - mae: 0.0874\n",
            "Epoch 16: val_mae improved from 0.07880 to 0.07830, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 43s 27ms/step - loss: 0.0107 - mae: 0.0874 - val_loss: 0.0093 - val_mae: 0.0783 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0106 - mae: 0.0869\n",
            "Epoch 17: val_mae improved from 0.07830 to 0.07793, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 43s 28ms/step - loss: 0.0106 - mae: 0.0869 - val_loss: 0.0093 - val_mae: 0.0779 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "1345/1349 [============================>.] - ETA: 0s - loss: 0.0105 - mae: 0.0863\n",
            "Epoch 18: val_mae improved from 0.07793 to 0.07749, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 37s 23ms/step - loss: 0.0105 - mae: 0.0863 - val_loss: 0.0092 - val_mae: 0.0775 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0104 - mae: 0.0858\n",
            "Epoch 19: val_mae improved from 0.07749 to 0.07700, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 37s 22ms/step - loss: 0.0104 - mae: 0.0858 - val_loss: 0.0092 - val_mae: 0.0770 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "1347/1349 [============================>.] - ETA: 0s - loss: 0.0104 - mae: 0.0853\n",
            "Epoch 20: val_mae improved from 0.07700 to 0.07673, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 38s 23ms/step - loss: 0.0104 - mae: 0.0853 - val_loss: 0.0091 - val_mae: 0.0767 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "1346/1349 [============================>.] - ETA: 0s - loss: 0.0103 - mae: 0.0849\n",
            "Epoch 21: val_mae improved from 0.07673 to 0.07624, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 37s 23ms/step - loss: 0.0103 - mae: 0.0849 - val_loss: 0.0090 - val_mae: 0.0762 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "1348/1349 [============================>.] - ETA: 0s - loss: 0.0102 - mae: 0.0844\n",
            "Epoch 22: val_mae improved from 0.07624 to 0.07600, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 37s 23ms/step - loss: 0.0102 - mae: 0.0844 - val_loss: 0.0090 - val_mae: 0.0760 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "1346/1349 [============================>.] - ETA: 0s - loss: 0.0102 - mae: 0.0840\n",
            "Epoch 23: val_mae improved from 0.07600 to 0.07560, saving model to model/my_checkpoint.ckpt\n",
            "1349/1349 [==============================] - 43s 27ms/step - loss: 0.0102 - mae: 0.0840 - val_loss: 0.0089 - val_mae: 0.0756 - lr: 0.0100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7cd1fb402d95>\u001b[0m in \u001b[0;36m<cell line: 144>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolution_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"power-1.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;31m## don't copy following codes in real test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-7cd1fb402d95>\u001b[0m in \u001b[0;36msolution_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m                 )\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     model.fit(train_set,\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1671\u001b[0m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1673\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1674\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N-6Sy_CiUuAA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}